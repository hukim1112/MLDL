{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZwAV0nGn6vPC"
   },
   "source": [
    "## 1. 다운로드 및 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DusCwxVS6vPD"
   },
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "o2qxoh5JrYsr"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "try:\n",
    "  # Colab only\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80Y3rNyq6vPO"
   },
   "source": [
    "Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "60B2Xx056vPP"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "keras = tf.keras\n",
    "import tensorflow_datasets as tfds\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0ACWCVA6vPT"
   },
   "source": [
    "Check out the details (features, statistics, etc.) of the dataset to be downloaded here: https://www.tensorflow.org/datasets/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "QlZTnDsn6vPU",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abstract_reasoning',\n",
       " 'accentdb',\n",
       " 'aeslc',\n",
       " 'aflw2k3d',\n",
       " 'ag_news_subset',\n",
       " 'ai2_arc',\n",
       " 'ai2_arc_with_ir',\n",
       " 'amazon_us_reviews',\n",
       " 'anli',\n",
       " 'arc',\n",
       " 'bair_robot_pushing_small',\n",
       " 'bccd',\n",
       " 'beans',\n",
       " 'big_patent',\n",
       " 'bigearthnet',\n",
       " 'billsum',\n",
       " 'binarized_mnist',\n",
       " 'binary_alpha_digits',\n",
       " 'blimp',\n",
       " 'bool_q',\n",
       " 'c4',\n",
       " 'caltech101',\n",
       " 'caltech_birds2010',\n",
       " 'caltech_birds2011',\n",
       " 'cars196',\n",
       " 'cassava',\n",
       " 'cats_vs_dogs',\n",
       " 'celeb_a',\n",
       " 'celeb_a_hq',\n",
       " 'cfq',\n",
       " 'cherry_blossoms',\n",
       " 'chexpert',\n",
       " 'cifar10',\n",
       " 'cifar100',\n",
       " 'cifar10_1',\n",
       " 'cifar10_corrupted',\n",
       " 'citrus_leaves',\n",
       " 'cityscapes',\n",
       " 'civil_comments',\n",
       " 'clevr',\n",
       " 'clic',\n",
       " 'clinc_oos',\n",
       " 'cmaterdb',\n",
       " 'cnn_dailymail',\n",
       " 'coco',\n",
       " 'coco_captions',\n",
       " 'coil100',\n",
       " 'colorectal_histology',\n",
       " 'colorectal_histology_large',\n",
       " 'common_voice',\n",
       " 'coqa',\n",
       " 'cos_e',\n",
       " 'cosmos_qa',\n",
       " 'covid19sum',\n",
       " 'crema_d',\n",
       " 'curated_breast_imaging_ddsm',\n",
       " 'cycle_gan',\n",
       " 'dart',\n",
       " 'davis',\n",
       " 'deep_weeds',\n",
       " 'definite_pronoun_resolution',\n",
       " 'dementiabank',\n",
       " 'diabetic_retinopathy_detection',\n",
       " 'div2k',\n",
       " 'dmlab',\n",
       " 'downsampled_imagenet',\n",
       " 'drop',\n",
       " 'dsprites',\n",
       " 'dtd',\n",
       " 'duke_ultrasound',\n",
       " 'e2e_cleaned',\n",
       " 'efron_morris75',\n",
       " 'emnist',\n",
       " 'eraser_multi_rc',\n",
       " 'esnli',\n",
       " 'eurosat',\n",
       " 'fashion_mnist',\n",
       " 'flic',\n",
       " 'flores',\n",
       " 'food101',\n",
       " 'forest_fires',\n",
       " 'fuss',\n",
       " 'gap',\n",
       " 'geirhos_conflict_stimuli',\n",
       " 'genomics_ood',\n",
       " 'german_credit_numeric',\n",
       " 'gigaword',\n",
       " 'glue',\n",
       " 'goemotions',\n",
       " 'gpt3',\n",
       " 'gref',\n",
       " 'groove',\n",
       " 'gtzan',\n",
       " 'gtzan_music_speech',\n",
       " 'hellaswag',\n",
       " 'higgs',\n",
       " 'horses_or_humans',\n",
       " 'howell',\n",
       " 'i_naturalist2017',\n",
       " 'imagenet2012',\n",
       " 'imagenet2012_corrupted',\n",
       " 'imagenet2012_real',\n",
       " 'imagenet2012_subset',\n",
       " 'imagenet_a',\n",
       " 'imagenet_r',\n",
       " 'imagenet_resized',\n",
       " 'imagenet_v2',\n",
       " 'imagenette',\n",
       " 'imagewang',\n",
       " 'imdb_reviews',\n",
       " 'irc_disentanglement',\n",
       " 'iris',\n",
       " 'kitti',\n",
       " 'kmnist',\n",
       " 'lambada',\n",
       " 'lfw',\n",
       " 'librispeech',\n",
       " 'librispeech_lm',\n",
       " 'libritts',\n",
       " 'ljspeech',\n",
       " 'lm1b',\n",
       " 'lost_and_found',\n",
       " 'lsun',\n",
       " 'lvis',\n",
       " 'malaria',\n",
       " 'math_dataset',\n",
       " 'mctaco',\n",
       " 'mlqa',\n",
       " 'mnist',\n",
       " 'mnist_corrupted',\n",
       " 'movie_lens',\n",
       " 'movie_rationales',\n",
       " 'movielens',\n",
       " 'moving_mnist',\n",
       " 'multi_news',\n",
       " 'multi_nli',\n",
       " 'multi_nli_mismatch',\n",
       " 'natural_questions',\n",
       " 'natural_questions_open',\n",
       " 'newsroom',\n",
       " 'nsynth',\n",
       " 'nyu_depth_v2',\n",
       " 'omniglot',\n",
       " 'open_images_challenge2019_detection',\n",
       " 'open_images_v4',\n",
       " 'openbookqa',\n",
       " 'opinion_abstracts',\n",
       " 'opinosis',\n",
       " 'opus',\n",
       " 'oxford_flowers102',\n",
       " 'oxford_iiit_pet',\n",
       " 'para_crawl',\n",
       " 'patch_camelyon',\n",
       " 'paws_wiki',\n",
       " 'paws_x_wiki',\n",
       " 'pet_finder',\n",
       " 'pg19',\n",
       " 'piqa',\n",
       " 'places365_small',\n",
       " 'plant_leaves',\n",
       " 'plant_village',\n",
       " 'plantae_k',\n",
       " 'qa4mre',\n",
       " 'qasc',\n",
       " 'quac',\n",
       " 'quickdraw_bitmap',\n",
       " 'race',\n",
       " 'radon',\n",
       " 'reddit',\n",
       " 'reddit_disentanglement',\n",
       " 'reddit_tifu',\n",
       " 'resisc45',\n",
       " 'robonet',\n",
       " 'rock_paper_scissors',\n",
       " 'rock_you',\n",
       " 's3o4d',\n",
       " 'salient_span_wikipedia',\n",
       " 'samsum',\n",
       " 'savee',\n",
       " 'scan',\n",
       " 'scene_parse150',\n",
       " 'scicite',\n",
       " 'scientific_papers',\n",
       " 'sentiment140',\n",
       " 'shapes3d',\n",
       " 'siscore',\n",
       " 'smallnorb',\n",
       " 'snli',\n",
       " 'so2sat',\n",
       " 'speech_commands',\n",
       " 'spoken_digit',\n",
       " 'squad',\n",
       " 'stanford_dogs',\n",
       " 'stanford_online_products',\n",
       " 'star_cfq',\n",
       " 'starcraft_video',\n",
       " 'stl10',\n",
       " 'story_cloze',\n",
       " 'sun397',\n",
       " 'super_glue',\n",
       " 'svhn_cropped',\n",
       " 'tao',\n",
       " 'ted_hrlr_translate',\n",
       " 'ted_multi_translate',\n",
       " 'tedlium',\n",
       " 'tf_flowers',\n",
       " 'the300w_lp',\n",
       " 'tiny_shakespeare',\n",
       " 'titanic',\n",
       " 'trec',\n",
       " 'trivia_qa',\n",
       " 'tydi_qa',\n",
       " 'uc_merced',\n",
       " 'ucf101',\n",
       " 'vctk',\n",
       " 'vgg_face2',\n",
       " 'visual_domain_decathlon',\n",
       " 'voc',\n",
       " 'voxceleb',\n",
       " 'voxforge',\n",
       " 'waymo_open_dataset',\n",
       " 'web_nlg',\n",
       " 'web_questions',\n",
       " 'wider_face',\n",
       " 'wiki40b',\n",
       " 'wiki_bio',\n",
       " 'wiki_table_questions',\n",
       " 'wiki_table_text',\n",
       " 'wikihow',\n",
       " 'wikipedia',\n",
       " 'wikipedia_toxicity_subtypes',\n",
       " 'wine_quality',\n",
       " 'winogrande',\n",
       " 'wmt13_translate',\n",
       " 'wmt14_translate',\n",
       " 'wmt15_translate',\n",
       " 'wmt16_translate',\n",
       " 'wmt17_translate',\n",
       " 'wmt18_translate',\n",
       " 'wmt19_translate',\n",
       " 'wmt_t2t_translate',\n",
       " 'wmt_translate',\n",
       " 'wordnet',\n",
       " 'wsc273',\n",
       " 'xnli',\n",
       " 'xquad',\n",
       " 'xsum',\n",
       " 'xtreme_pawsx',\n",
       " 'xtreme_xnli',\n",
       " 'yelp_polarity_reviews',\n",
       " 'yes_no',\n",
       " 'youtube_vis',\n",
       " 'huggingface:acronym_identification',\n",
       " 'huggingface:ade_corpus_v2',\n",
       " 'huggingface:adversarial_qa',\n",
       " 'huggingface:aeslc',\n",
       " 'huggingface:afrikaans_ner_corpus',\n",
       " 'huggingface:ag_news',\n",
       " 'huggingface:ai2_arc',\n",
       " 'huggingface:air_dialogue',\n",
       " 'huggingface:ajgt_twitter_ar',\n",
       " 'huggingface:allegro_reviews',\n",
       " 'huggingface:allocine',\n",
       " 'huggingface:alt',\n",
       " 'huggingface:amazon_polarity',\n",
       " 'huggingface:amazon_reviews_multi',\n",
       " 'huggingface:amazon_us_reviews',\n",
       " 'huggingface:ambig_qa',\n",
       " 'huggingface:amttl',\n",
       " 'huggingface:anli',\n",
       " 'huggingface:app_reviews',\n",
       " 'huggingface:aqua_rat',\n",
       " 'huggingface:aquamuse',\n",
       " 'huggingface:ar_cov19',\n",
       " 'huggingface:ar_res_reviews',\n",
       " 'huggingface:ar_sarcasm',\n",
       " 'huggingface:arabic_billion_words',\n",
       " 'huggingface:arabic_pos_dialect',\n",
       " 'huggingface:arabic_speech_corpus',\n",
       " 'huggingface:arcd',\n",
       " 'huggingface:arsentd_lev',\n",
       " 'huggingface:art',\n",
       " 'huggingface:arxiv_dataset',\n",
       " 'huggingface:aslg_pc12',\n",
       " 'huggingface:asnq',\n",
       " 'huggingface:asset',\n",
       " 'huggingface:assin',\n",
       " 'huggingface:assin2',\n",
       " 'huggingface:atomic',\n",
       " 'huggingface:autshumato',\n",
       " 'huggingface:bbc_hindi_nli',\n",
       " 'huggingface:bc2gm_corpus',\n",
       " 'huggingface:best2009',\n",
       " 'huggingface:bianet',\n",
       " 'huggingface:bible_para',\n",
       " 'huggingface:big_patent',\n",
       " 'huggingface:billsum',\n",
       " 'huggingface:bing_coronavirus_query_set',\n",
       " 'huggingface:biomrc',\n",
       " 'huggingface:blended_skill_talk',\n",
       " 'huggingface:blimp',\n",
       " 'huggingface:blog_authorship_corpus',\n",
       " 'huggingface:bn_hate_speech',\n",
       " 'huggingface:bookcorpus',\n",
       " 'huggingface:bookcorpusopen',\n",
       " 'huggingface:boolq',\n",
       " 'huggingface:bprec',\n",
       " 'huggingface:break_data',\n",
       " 'huggingface:brwac',\n",
       " 'huggingface:bsd_ja_en',\n",
       " 'huggingface:bswac',\n",
       " 'huggingface:c3',\n",
       " 'huggingface:c4',\n",
       " 'huggingface:cail2018',\n",
       " 'huggingface:caner',\n",
       " 'huggingface:capes',\n",
       " 'huggingface:catalonia_independence',\n",
       " 'huggingface:cawac',\n",
       " 'huggingface:cc100',\n",
       " 'huggingface:cc_news',\n",
       " 'huggingface:cdsc',\n",
       " 'huggingface:cdt',\n",
       " 'huggingface:cfq',\n",
       " 'huggingface:chr_en',\n",
       " 'huggingface:cifar10',\n",
       " 'huggingface:cifar100',\n",
       " 'huggingface:circa',\n",
       " 'huggingface:civil_comments',\n",
       " 'huggingface:clickbait_news_bg',\n",
       " 'huggingface:climate_fever',\n",
       " 'huggingface:clinc_oos',\n",
       " 'huggingface:clue',\n",
       " 'huggingface:cmrc2018',\n",
       " 'huggingface:cnn_dailymail',\n",
       " 'huggingface:coached_conv_pref',\n",
       " 'huggingface:coarse_discourse',\n",
       " 'huggingface:codah',\n",
       " 'huggingface:code_search_net',\n",
       " 'huggingface:com_qa',\n",
       " 'huggingface:common_gen',\n",
       " 'huggingface:commonsense_qa',\n",
       " 'huggingface:compguesswhat',\n",
       " 'huggingface:conceptnet5',\n",
       " 'huggingface:conll2000',\n",
       " 'huggingface:conll2002',\n",
       " 'huggingface:conll2003',\n",
       " 'huggingface:conv_ai',\n",
       " 'huggingface:conv_ai_2',\n",
       " 'huggingface:conv_ai_3',\n",
       " 'huggingface:coqa',\n",
       " 'huggingface:cord19',\n",
       " 'huggingface:cornell_movie_dialog',\n",
       " 'huggingface:cos_e',\n",
       " 'huggingface:cosmos_qa',\n",
       " 'huggingface:counter',\n",
       " 'huggingface:covid_qa_castorini',\n",
       " 'huggingface:covid_qa_deepset',\n",
       " 'huggingface:covid_qa_ucsd',\n",
       " 'huggingface:covid_tweets_japanese',\n",
       " 'huggingface:craigslist_bargains',\n",
       " 'huggingface:crawl_domain',\n",
       " 'huggingface:crd3',\n",
       " 'huggingface:crime_and_punish',\n",
       " 'huggingface:crows_pairs',\n",
       " 'huggingface:cs_restaurants',\n",
       " 'huggingface:curiosity_dialogs',\n",
       " 'huggingface:daily_dialog',\n",
       " 'huggingface:dane',\n",
       " 'huggingface:danish_political_comments',\n",
       " 'huggingface:dart',\n",
       " 'huggingface:datacommons_factcheck',\n",
       " 'huggingface:dbpedia_14',\n",
       " 'huggingface:dbrd',\n",
       " 'huggingface:deal_or_no_dialog',\n",
       " 'huggingface:definite_pronoun_resolution',\n",
       " 'huggingface:dengue_filipino',\n",
       " 'huggingface:dialog_re',\n",
       " 'huggingface:diplomacy_detection',\n",
       " 'huggingface:disaster_response_messages',\n",
       " 'huggingface:discofuse',\n",
       " 'huggingface:discovery',\n",
       " 'huggingface:doc2dial',\n",
       " 'huggingface:docred',\n",
       " 'huggingface:doqa',\n",
       " 'huggingface:dream',\n",
       " 'huggingface:drop',\n",
       " 'huggingface:duorc',\n",
       " 'huggingface:dutch_social',\n",
       " 'huggingface:dyk',\n",
       " 'huggingface:e2e_nlg',\n",
       " 'huggingface:e2e_nlg_cleaned',\n",
       " 'huggingface:ecb',\n",
       " 'huggingface:ehealth_kd',\n",
       " 'huggingface:eitb_parcc',\n",
       " 'huggingface:eli5',\n",
       " 'huggingface:emea',\n",
       " 'huggingface:emo',\n",
       " 'huggingface:emotion',\n",
       " 'huggingface:emotone_ar',\n",
       " 'huggingface:empathetic_dialogues',\n",
       " 'huggingface:enriched_web_nlg',\n",
       " 'huggingface:eraser_multi_rc',\n",
       " 'huggingface:esnli',\n",
       " 'huggingface:eth_py150_open',\n",
       " 'huggingface:ethos',\n",
       " 'huggingface:euronews',\n",
       " 'huggingface:europa_eac_tm',\n",
       " 'huggingface:europa_ecdc_tm',\n",
       " 'huggingface:event2Mind',\n",
       " 'huggingface:evidence_infer_treatment',\n",
       " 'huggingface:exams',\n",
       " 'huggingface:factckbr',\n",
       " 'huggingface:fake_news_english',\n",
       " 'huggingface:fake_news_filipino',\n",
       " 'huggingface:farsi_news',\n",
       " 'huggingface:fever',\n",
       " 'huggingface:finer',\n",
       " 'huggingface:flores',\n",
       " 'huggingface:flue',\n",
       " 'huggingface:fquad',\n",
       " 'huggingface:freebase_qa',\n",
       " 'huggingface:gap',\n",
       " 'huggingface:gem',\n",
       " 'huggingface:generated_reviews_enth',\n",
       " 'huggingface:generics_kb',\n",
       " 'huggingface:german_legal_entity_recognition',\n",
       " 'huggingface:germaner',\n",
       " 'huggingface:germeval_14',\n",
       " 'huggingface:giga_fren',\n",
       " 'huggingface:gigaword',\n",
       " 'huggingface:glucose',\n",
       " 'huggingface:glue',\n",
       " 'huggingface:gnad10',\n",
       " 'huggingface:go_emotions',\n",
       " 'huggingface:google_wellformed_query',\n",
       " 'huggingface:grail_qa',\n",
       " 'huggingface:great_code',\n",
       " 'huggingface:guardian_authorship',\n",
       " 'huggingface:gutenberg_time',\n",
       " 'huggingface:hans',\n",
       " 'huggingface:hansards',\n",
       " 'huggingface:hard',\n",
       " 'huggingface:harem',\n",
       " 'huggingface:has_part',\n",
       " 'huggingface:hate_offensive',\n",
       " 'huggingface:hate_speech18',\n",
       " 'huggingface:hate_speech_filipino',\n",
       " 'huggingface:hate_speech_offensive',\n",
       " 'huggingface:hate_speech_pl',\n",
       " 'huggingface:hate_speech_portuguese',\n",
       " 'huggingface:hatexplain',\n",
       " 'huggingface:hausa_voa_ner',\n",
       " 'huggingface:hausa_voa_topics',\n",
       " 'huggingface:hda_nli_hindi',\n",
       " 'huggingface:head_qa',\n",
       " 'huggingface:health_fact',\n",
       " 'huggingface:hebrew_projectbenyehuda',\n",
       " 'huggingface:hebrew_sentiment',\n",
       " 'huggingface:hebrew_this_world',\n",
       " 'huggingface:hellaswag',\n",
       " 'huggingface:hind_encorp',\n",
       " 'huggingface:hindi_discourse',\n",
       " 'huggingface:hippocorpus',\n",
       " 'huggingface:hkcancor',\n",
       " 'huggingface:hope_edi',\n",
       " 'huggingface:hotpot_qa',\n",
       " 'huggingface:hover',\n",
       " 'huggingface:hrenwac_para',\n",
       " 'huggingface:hrwac',\n",
       " 'huggingface:humicroedit',\n",
       " 'huggingface:hybrid_qa',\n",
       " 'huggingface:hyperpartisan_news_detection',\n",
       " 'huggingface:id_clickbait',\n",
       " 'huggingface:id_liputan6',\n",
       " 'huggingface:id_nergrit_corpus',\n",
       " 'huggingface:id_newspapers_2018',\n",
       " 'huggingface:id_panl_bppt',\n",
       " 'huggingface:id_puisi',\n",
       " 'huggingface:igbo_english_machine_translation',\n",
       " 'huggingface:igbo_monolingual',\n",
       " 'huggingface:igbo_ner',\n",
       " 'huggingface:ilist',\n",
       " 'huggingface:imdb',\n",
       " 'huggingface:imdb_urdu_reviews',\n",
       " 'huggingface:imppres',\n",
       " 'huggingface:indic_glue',\n",
       " 'huggingface:indonlu',\n",
       " 'huggingface:inquisitive_qg',\n",
       " 'huggingface:interpress_news_category_tr',\n",
       " 'huggingface:irc_disentangle',\n",
       " 'huggingface:isixhosa_ner_corpus',\n",
       " 'huggingface:isizulu_ner_corpus',\n",
       " 'huggingface:iwslt2017',\n",
       " 'huggingface:jeopardy',\n",
       " 'huggingface:jfleg',\n",
       " 'huggingface:jigsaw_toxicity_pred',\n",
       " 'huggingface:jnlpba',\n",
       " 'huggingface:journalists_questions',\n",
       " 'huggingface:kannada_news',\n",
       " 'huggingface:kd_conv',\n",
       " 'huggingface:kde4',\n",
       " 'huggingface:kelm',\n",
       " 'huggingface:kilt_tasks',\n",
       " 'huggingface:kilt_wikipedia',\n",
       " 'huggingface:kinnews_kirnews',\n",
       " 'huggingface:kor_3i4k',\n",
       " 'huggingface:kor_hate',\n",
       " 'huggingface:kor_ner',\n",
       " 'huggingface:kor_nli',\n",
       " 'huggingface:kor_nlu',\n",
       " 'huggingface:kor_qpair',\n",
       " 'huggingface:kor_sae',\n",
       " 'huggingface:kor_sarcasm',\n",
       " 'huggingface:labr',\n",
       " 'huggingface:lama',\n",
       " 'huggingface:lambada',\n",
       " 'huggingface:large_spanish_corpus',\n",
       " 'huggingface:lc_quad',\n",
       " 'huggingface:lener_br',\n",
       " 'huggingface:liar',\n",
       " 'huggingface:librispeech_asr',\n",
       " 'huggingface:librispeech_lm',\n",
       " 'huggingface:limit',\n",
       " 'huggingface:lince',\n",
       " 'huggingface:linnaeus',\n",
       " 'huggingface:liveqa',\n",
       " 'huggingface:lj_speech',\n",
       " 'huggingface:lm1b',\n",
       " 'huggingface:lst20',\n",
       " 'huggingface:mac_morpho',\n",
       " 'huggingface:makhzan',\n",
       " 'huggingface:math_dataset',\n",
       " 'huggingface:math_qa',\n",
       " 'huggingface:matinf',\n",
       " 'huggingface:mc_taco',\n",
       " 'huggingface:md_gender_bias',\n",
       " 'huggingface:med_hop',\n",
       " 'huggingface:medal',\n",
       " 'huggingface:medical_dialog',\n",
       " 'huggingface:medical_questions_pairs',\n",
       " 'huggingface:menyo20k_mt',\n",
       " 'huggingface:meta_woz',\n",
       " 'huggingface:metooma',\n",
       " 'huggingface:metrec',\n",
       " 'huggingface:mkb',\n",
       " 'huggingface:mkqa',\n",
       " 'huggingface:mlqa',\n",
       " 'huggingface:mlsum',\n",
       " 'huggingface:mnist',\n",
       " 'huggingface:mocha',\n",
       " 'huggingface:movie_rationales',\n",
       " 'huggingface:mrqa',\n",
       " 'huggingface:ms_marco',\n",
       " 'huggingface:ms_terms',\n",
       " 'huggingface:msr_genomics_kbcomp',\n",
       " 'huggingface:msr_sqa',\n",
       " 'huggingface:msr_text_compression',\n",
       " 'huggingface:msr_zhen_translation_parity',\n",
       " 'huggingface:msra_ner',\n",
       " 'huggingface:mt_eng_vietnamese',\n",
       " 'huggingface:muchocine',\n",
       " 'huggingface:multi_booked',\n",
       " 'huggingface:multi_news',\n",
       " 'huggingface:multi_nli',\n",
       " 'huggingface:multi_nli_mismatch',\n",
       " 'huggingface:multi_para_crawl',\n",
       " 'huggingface:multi_re_qa',\n",
       " 'huggingface:multi_woz_v22',\n",
       " 'huggingface:multi_x_science_sum',\n",
       " 'huggingface:mutual_friends',\n",
       " 'huggingface:mwsc',\n",
       " 'huggingface:myanmar_news',\n",
       " 'huggingface:narrativeqa',\n",
       " 'huggingface:narrativeqa_manual',\n",
       " 'huggingface:natural_questions',\n",
       " 'huggingface:ncbi_disease',\n",
       " 'huggingface:nchlt',\n",
       " 'huggingface:ncslgr',\n",
       " 'huggingface:nell',\n",
       " 'huggingface:neural_code_search',\n",
       " 'huggingface:news_commentary',\n",
       " 'huggingface:newsgroup',\n",
       " 'huggingface:newsph',\n",
       " 'huggingface:newsph_nli',\n",
       " 'huggingface:newsqa',\n",
       " 'huggingface:newsroom',\n",
       " 'huggingface:nkjp-ner',\n",
       " 'huggingface:nli_tr',\n",
       " 'huggingface:norwegian_ner',\n",
       " 'huggingface:nq_open',\n",
       " 'huggingface:nsmc',\n",
       " 'huggingface:numer_sense',\n",
       " 'huggingface:numeric_fused_head',\n",
       " 'huggingface:oclar',\n",
       " 'huggingface:offcombr',\n",
       " 'huggingface:offenseval2020_tr',\n",
       " 'huggingface:offenseval_dravidian',\n",
       " 'huggingface:ofis_publik',\n",
       " 'huggingface:ohsumed',\n",
       " 'huggingface:ollie',\n",
       " 'huggingface:omp',\n",
       " 'huggingface:onestop_english',\n",
       " 'huggingface:open_subtitles',\n",
       " 'huggingface:openbookqa',\n",
       " 'huggingface:openwebtext',\n",
       " 'huggingface:opinosis',\n",
       " 'huggingface:opus100',\n",
       " 'huggingface:opus_books',\n",
       " 'huggingface:opus_dgt',\n",
       " 'huggingface:opus_dogc',\n",
       " 'huggingface:opus_elhuyar',\n",
       " 'huggingface:opus_euconst',\n",
       " 'huggingface:opus_finlex',\n",
       " 'huggingface:opus_fiskmo',\n",
       " 'huggingface:opus_gnome',\n",
       " 'huggingface:opus_infopankki',\n",
       " 'huggingface:opus_memat',\n",
       " 'huggingface:opus_montenegrinsubs',\n",
       " 'huggingface:opus_openoffice',\n",
       " 'huggingface:opus_paracrawl',\n",
       " 'huggingface:opus_rf',\n",
       " 'huggingface:opus_tedtalks',\n",
       " 'huggingface:opus_ubuntu',\n",
       " 'huggingface:opus_wikipedia',\n",
       " 'huggingface:opus_xhosanavy',\n",
       " 'huggingface:orange_sum',\n",
       " 'huggingface:oscar',\n",
       " 'huggingface:para_crawl',\n",
       " 'huggingface:para_pat',\n",
       " 'huggingface:paws',\n",
       " 'huggingface:paws-x',\n",
       " 'huggingface:pec',\n",
       " 'huggingface:peer_read',\n",
       " 'huggingface:peoples_daily_ner',\n",
       " 'huggingface:per_sent',\n",
       " 'huggingface:persian_ner',\n",
       " 'huggingface:pg19',\n",
       " 'huggingface:php',\n",
       " 'huggingface:piaf',\n",
       " 'huggingface:pib',\n",
       " 'huggingface:piqa',\n",
       " 'huggingface:pn_summary',\n",
       " 'huggingface:poem_sentiment',\n",
       " 'huggingface:polemo2',\n",
       " 'huggingface:poleval2019_cyberbullying',\n",
       " 'huggingface:poleval2019_mt',\n",
       " 'huggingface:polsum',\n",
       " 'huggingface:polyglot_ner',\n",
       " 'huggingface:prachathai67k',\n",
       " 'huggingface:pragmeval',\n",
       " 'huggingface:proto_qa',\n",
       " 'huggingface:psc',\n",
       " 'huggingface:ptb_text_only',\n",
       " 'huggingface:pubmed',\n",
       " 'huggingface:pubmed_qa',\n",
       " 'huggingface:py_ast',\n",
       " 'huggingface:qa4mre',\n",
       " 'huggingface:qa_srl',\n",
       " 'huggingface:qa_zre',\n",
       " 'huggingface:qangaroo',\n",
       " 'huggingface:qanta',\n",
       " 'huggingface:qasc',\n",
       " 'huggingface:qed',\n",
       " 'huggingface:qed_amara',\n",
       " 'huggingface:quac',\n",
       " 'huggingface:quail',\n",
       " 'huggingface:quarel',\n",
       " 'huggingface:quartz',\n",
       " 'huggingface:quora',\n",
       " 'huggingface:quoref',\n",
       " 'huggingface:race',\n",
       " 'huggingface:re_dial',\n",
       " 'huggingface:reasoning_bg',\n",
       " 'huggingface:recipe_nlg',\n",
       " 'huggingface:reclor',\n",
       " 'huggingface:reddit',\n",
       " 'huggingface:reddit_tifu',\n",
       " 'huggingface:refresd',\n",
       " 'huggingface:reuters21578',\n",
       " 'huggingface:roman_urdu',\n",
       " 'huggingface:ronec',\n",
       " 'huggingface:ropes',\n",
       " 'huggingface:rotten_tomatoes',\n",
       " 'huggingface:s2orc',\n",
       " 'huggingface:samsum',\n",
       " 'huggingface:sanskrit_classic',\n",
       " 'huggingface:saudinewsnet',\n",
       " 'huggingface:scan',\n",
       " 'huggingface:scb_mt_enth_2020',\n",
       " 'huggingface:schema_guided_dstc8',\n",
       " 'huggingface:scicite',\n",
       " 'huggingface:scielo',\n",
       " 'huggingface:scientific_papers',\n",
       " 'huggingface:scifact',\n",
       " 'huggingface:sciq',\n",
       " 'huggingface:scitail',\n",
       " 'huggingface:scitldr',\n",
       " 'huggingface:search_qa',\n",
       " 'huggingface:selqa',\n",
       " 'huggingface:sem_eval_2010_task_8',\n",
       " 'huggingface:sem_eval_2014_task_1',\n",
       " 'huggingface:sem_eval_2020_task_11',\n",
       " 'huggingface:sent_comp',\n",
       " 'huggingface:senti_lex',\n",
       " 'huggingface:senti_ws',\n",
       " 'huggingface:sentiment140',\n",
       " 'huggingface:sepedi_ner',\n",
       " 'huggingface:sesotho_ner_corpus',\n",
       " 'huggingface:setimes',\n",
       " 'huggingface:setswana_ner_corpus',\n",
       " 'huggingface:sharc',\n",
       " 'huggingface:sharc_modified',\n",
       " 'huggingface:sick',\n",
       " 'huggingface:silicone',\n",
       " 'huggingface:simple_questions_v2',\n",
       " 'huggingface:siswati_ner_corpus',\n",
       " 'huggingface:smartdata',\n",
       " 'huggingface:sms_spam',\n",
       " 'huggingface:snips_built_in_intents',\n",
       " 'huggingface:snli',\n",
       " 'huggingface:snow_simplified_japanese_corpus',\n",
       " 'huggingface:so_stacksample',\n",
       " 'huggingface:social_bias_frames',\n",
       " 'huggingface:social_i_qa',\n",
       " 'huggingface:sofc_materials_articles',\n",
       " 'huggingface:sogou_news',\n",
       " 'huggingface:spanish_billion_words',\n",
       " 'huggingface:spc',\n",
       " 'huggingface:species_800',\n",
       " 'huggingface:spider',\n",
       " 'huggingface:squad',\n",
       " 'huggingface:squad_adversarial',\n",
       " 'huggingface:squad_es',\n",
       " 'huggingface:squad_it',\n",
       " 'huggingface:squad_kor_v1',\n",
       " 'huggingface:squad_kor_v2',\n",
       " 'huggingface:squad_v1_pt',\n",
       " 'huggingface:squad_v2',\n",
       " 'huggingface:squadshifts',\n",
       " 'huggingface:srwac',\n",
       " 'huggingface:stereoset',\n",
       " 'huggingface:stsb_mt_sv',\n",
       " 'huggingface:style_change_detection',\n",
       " 'huggingface:super_glue',\n",
       " 'huggingface:swag',\n",
       " 'huggingface:swahili',\n",
       " 'huggingface:swahili_news',\n",
       " 'huggingface:swda',\n",
       " 'huggingface:swedish_ner_corpus',\n",
       " 'huggingface:swedish_reviews',\n",
       " 'huggingface:tab_fact',\n",
       " 'huggingface:tamilmixsentiment',\n",
       " 'huggingface:tanzil',\n",
       " 'huggingface:tapaco',\n",
       " 'huggingface:tashkeela',\n",
       " 'huggingface:taskmaster1',\n",
       " 'huggingface:taskmaster2',\n",
       " 'huggingface:taskmaster3',\n",
       " 'huggingface:tatoeba',\n",
       " 'huggingface:ted_hrlr',\n",
       " 'huggingface:ted_iwlst2013',\n",
       " 'huggingface:ted_multi',\n",
       " 'huggingface:ted_talks_iwslt',\n",
       " 'huggingface:telugu_books',\n",
       " 'huggingface:telugu_news',\n",
       " 'huggingface:tep_en_fa_para',\n",
       " 'huggingface:thai_toxicity_tweet',\n",
       " 'huggingface:thainer',\n",
       " 'huggingface:thaiqa_squad',\n",
       " 'huggingface:thaisum',\n",
       " 'huggingface:tilde_model',\n",
       " 'huggingface:times_of_india_news_headlines',\n",
       " 'huggingface:tiny_shakespeare',\n",
       " 'huggingface:tlc',\n",
       " 'huggingface:tmu_gfm_dataset',\n",
       " 'huggingface:totto',\n",
       " 'huggingface:trec',\n",
       " 'huggingface:trivia_qa',\n",
       " 'huggingface:tsac',\n",
       " 'huggingface:ttc4900',\n",
       " 'huggingface:tunizi',\n",
       " 'huggingface:tuple_ie',\n",
       " 'huggingface:turk',\n",
       " 'huggingface:turkish_movie_sentiment',\n",
       " 'huggingface:turkish_ner',\n",
       " 'huggingface:turkish_product_reviews',\n",
       " 'huggingface:turkish_shrinked_ner',\n",
       " 'huggingface:turku_ner_corpus',\n",
       " 'huggingface:tweet_eval',\n",
       " 'huggingface:tweet_qa',\n",
       " 'huggingface:tweets_ar_en_parallel',\n",
       " 'huggingface:tweets_hate_speech_detection',\n",
       " 'huggingface:twi_text_c3',\n",
       " 'huggingface:twi_wordsim353',\n",
       " 'huggingface:tydiqa',\n",
       " 'huggingface:ubuntu_dialogs_corpus',\n",
       " 'huggingface:udhr',\n",
       " 'huggingface:um005',\n",
       " 'huggingface:un_ga',\n",
       " 'huggingface:un_multi',\n",
       " 'huggingface:un_pc',\n",
       " 'huggingface:universal_dependencies',\n",
       " 'huggingface:universal_morphologies',\n",
       " 'huggingface:urdu_fake_news',\n",
       " 'huggingface:urdu_sentiment_corpus',\n",
       " 'huggingface:web_nlg',\n",
       " 'huggingface:web_of_science',\n",
       " 'huggingface:web_questions',\n",
       " 'huggingface:weibo_ner',\n",
       " 'huggingface:wi_locness',\n",
       " 'huggingface:wiki40b',\n",
       " 'huggingface:wiki_asp',\n",
       " 'huggingface:wiki_atomic_edits',\n",
       " 'huggingface:wiki_auto',\n",
       " 'huggingface:wiki_bio',\n",
       " 'huggingface:wiki_dpr',\n",
       " 'huggingface:wiki_hop',\n",
       " 'huggingface:wiki_lingua',\n",
       " 'huggingface:wiki_movies',\n",
       " 'huggingface:wiki_qa',\n",
       " 'huggingface:wiki_qa_ar',\n",
       " 'huggingface:wiki_snippets',\n",
       " 'huggingface:wiki_source',\n",
       " 'huggingface:wiki_split',\n",
       " 'huggingface:wiki_summary',\n",
       " 'huggingface:wikiann',\n",
       " 'huggingface:wikicorpus',\n",
       " 'huggingface:wikihow',\n",
       " 'huggingface:wikipedia',\n",
       " 'huggingface:wikisql',\n",
       " 'huggingface:wikitext',\n",
       " 'huggingface:wikitext_tl39',\n",
       " 'huggingface:wili_2018',\n",
       " 'huggingface:wino_bias',\n",
       " 'huggingface:winograd_wsc',\n",
       " 'huggingface:winogrande',\n",
       " 'huggingface:wiqa',\n",
       " 'huggingface:wisesight1000',\n",
       " 'huggingface:wisesight_sentiment',\n",
       " 'huggingface:wmt14',\n",
       " 'huggingface:wmt15',\n",
       " 'huggingface:wmt16',\n",
       " 'huggingface:wmt17',\n",
       " 'huggingface:wmt18',\n",
       " 'huggingface:wmt19',\n",
       " 'huggingface:wmt20_mlqe_task1',\n",
       " 'huggingface:wmt20_mlqe_task2',\n",
       " 'huggingface:wmt20_mlqe_task3',\n",
       " 'huggingface:wmt_t2t',\n",
       " 'huggingface:wnut_17',\n",
       " 'huggingface:wongnai_reviews',\n",
       " 'huggingface:woz_dialogue',\n",
       " 'huggingface:wrbsc',\n",
       " 'huggingface:x_stance',\n",
       " 'huggingface:xcopa',\n",
       " 'huggingface:xed_en_fi',\n",
       " 'huggingface:xglue',\n",
       " 'huggingface:xnli',\n",
       " 'huggingface:xor_tydi_qa',\n",
       " 'huggingface:xquad',\n",
       " 'huggingface:xquad_r',\n",
       " 'huggingface:xsum',\n",
       " 'huggingface:xsum_factuality',\n",
       " 'huggingface:xtreme',\n",
       " 'huggingface:yahoo_answers_qa',\n",
       " 'huggingface:yahoo_answers_topics',\n",
       " 'huggingface:yelp_polarity',\n",
       " 'huggingface:yelp_review_full',\n",
       " 'huggingface:yoruba_bbc_topics',\n",
       " 'huggingface:yoruba_gv_ner',\n",
       " 'huggingface:yoruba_text_c3',\n",
       " 'huggingface:yoruba_wordsim353',\n",
       " 'huggingface:youtube_caption_corrections',\n",
       " 'huggingface:zest']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List the available datasets\n",
    "tfds.list_builders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2B2dasf6vPY"
   },
   "source": [
    "### 플라워 데이터셋 다운로드 및 로드하기 `tfds.load()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_UJfaBYH6vPZ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Since \"tf_flowers\" doesn't define standard splits, \n",
    "# use the subsplit feature to divide it into (train, validation, test)\n",
    "# with 80%, 10%, 10% of the data respectively.\n",
    "\n",
    "(raw_train, raw_validation, raw_test), metadata = tfds.load(name=\"tf_flowers\", \n",
    "                                                            with_info=True,\n",
    "                                                            split=('train[:80%]', 'train[80%:90%]', 'train[90%:]'),\n",
    "# specifying batch_size=-1 will load full dataset in the memory\n",
    "#                                                             batch_size=-1,\n",
    "# as_supervised: `bool`, if `True`, the returned `tf.data.Dataset`\n",
    "# will have a 2-tuple structure `(input, label)`                                                            \n",
    "                                                            as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bhPoKnhz6vPc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset shapes: ((None, None, 3), ()), types: (tf.uint8, tf.int64)>\n",
      "<PrefetchDataset shapes: ((None, None, 3), ()), types: (tf.uint8, tf.int64)>\n",
      "<PrefetchDataset shapes: ((None, None, 3), ()), types: (tf.uint8, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "print(raw_train)\n",
    "print(raw_validation)\n",
    "print(raw_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rjhvggeF6vPg"
   },
   "source": [
    "Set the training input image dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "L7NLOcLr6vPh"
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 128\n",
    "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BLqAwKYF6vPk"
   },
   "source": [
    "Create a function which will format the examples in the training dataset as per out needs. The arguments of the `format_example()` depends on the the parameters passed to `tfds.load()`. If `as_supervised=True` then (image, labels) tuple pair will be downloaded else a single dictionary with keys 'image' and 'label' will be passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "GmVsKsz06vPl",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Format example called!\n",
      "Format example called!\n",
      "Format example called!\n"
     ]
    }
   ],
   "source": [
    "# 기본적인 데이터 전처리 수행하기\n",
    "\n",
    "def format_example(image, label):\n",
    "    print(\"Format example called!\")\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = image / 255.0\n",
    "    # Resize the image if required\n",
    "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "    return image, label\n",
    "\n",
    "train = raw_train.map(format_example)\n",
    "validation = raw_validation.map(format_example)\n",
    "test = raw_test.map(format_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sS8YmYJ9MTV8"
   },
   "source": [
    "### 효율적으로 데이터 증폭시키기\n",
    "Read more [here](https://stackoverflow.com/questions/55141076/how-to-apply-data-augmentation-in-tensorflow-2-0-after-tfds-load) to know how the map function applies data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "PJ3tonJLMUOx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augment data called!\n"
     ]
    }
   ],
   "source": [
    "def augment_data(image, label):\n",
    "  print(\"Augment data called!\")\n",
    "  image = tf.image.random_flip_left_right(image)\n",
    "  image = tf.image.random_contrast(image, lower=0.0, upper=1.0)\n",
    "  # Add more augmentation of your choice\n",
    "  return image, label\n",
    "\n",
    "train = train.map(augment_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1vbOSs76vPo"
   },
   "source": [
    "Here we'll shuffle the dataset so that we don't have any ordering bias of examples, and create batches of size 32. Setting a shuffle buffer size as large as the dataset ensures that the data is completely shuffled. `.prefetch()` lets the dataset fetch batches, in the background while the model is training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BNGB5P36vPp"
   },
   "source": [
    "Without prefetch, the CPU and the GPU/TPU sit idle much of the time\n",
    "\n",
    "![alt text](https://www.tensorflow.org/images/datasets_without_pipelining.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qy7j1-5g6vPq"
   },
   "source": [
    "With prefetch, idle time diminishes significantly\n",
    "\n",
    "![alt text](https://www.tensorflow.org/images/datasets_with_pipelining.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hWJEyvNy6vPq"
   },
   "outputs": [],
   "source": [
    "SHUFFLE_BUFFER_SIZE = 1024\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train = train.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "validation = validation.batch(BATCH_SIZE)\n",
    "test = test.batch(BATCH_SIZE)\n",
    "# (Optional) prefetch will enable the input pipeline to asynchronously fetch batches while\n",
    "# your model is training.\n",
    "train = train.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "print(train)\n",
    "print(validation)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xRzjagmH6vPu",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kjpRZl8q6vPz"
   },
   "source": [
    "<b> \n",
    "1. TFDS Reference: https://www.tensorflow.org/datasets/overview\n",
    "2. For importing data from custom URL or disk location refer, Loading images using tf.data: https://www.tensorflow.org/alpha/tutorials/load_data/images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9gVRK_8U6vP0"
   },
   "source": [
    "### 데이터 시각화하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XVLYHdIQ6vP1"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t7KAUbUe6vP5"
   },
   "outputs": [],
   "source": [
    "# Get the function which converts label indices to string\n",
    "get_label_name = metadata.features['label'].int2str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPER0sD36vQA"
   },
   "source": [
    "Here we fetch the datatset batch by batch and convert it to numpy array just before passing it to plotting function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3IRrZBVh6vQC",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12)) \n",
    "for batch in train.take(1):\n",
    "    for i in range(9):\n",
    "        image, label = batch[0][i], batch[1][i]\n",
    "        plt.subplot(3,3,i+1)\n",
    "        plt.imshow(image.numpy())\n",
    "        plt.title(get_label_name(label.numpy()))\n",
    "        plt.grid(False)\n",
    "print(image.shape)\n",
    "print(np.min(image))\n",
    "print(np.max(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNX3KiVR6vQe"
   },
   "source": [
    "## 2.  `tf.keras`로 모델 빌드하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TTzNlpqy6vQh"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "caG4zs_p6vQt"
   },
   "outputs": [],
   "source": [
    "# Creating a simple CNN model in keras using functional API\n",
    "def create_model():\n",
    "    img_inputs = keras.Input(shape=IMG_SHAPE)\n",
    "    conv_1 = keras.layers.Conv2D(32, (3, 3), activation='relu')(img_inputs)\n",
    "    maxpool_1 = keras.layers.MaxPooling2D((2, 2))(conv_1)\n",
    "    conv_2 = keras.layers.Conv2D(64, (3, 3), activation='relu')(maxpool_1)\n",
    "    maxpool_2 = keras.layers.MaxPooling2D((2, 2))(conv_2)\n",
    "    conv_3 = keras.layers.Conv2D(64, (3, 3), activation='relu')(maxpool_2)\n",
    "    flatten = keras.layers.Flatten()(conv_3)\n",
    "    dense_1 = keras.layers.Dense(64, activation='relu')(flatten)\n",
    "    output = keras.layers.Dense(metadata.features['label'].num_classes, activation='softmax')(dense_1)\n",
    "\n",
    "    model = keras.Model(inputs=img_inputs, outputs=output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eG5P3tMa6vQ6"
   },
   "source": [
    "### Visualizing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dskLiSfU6vQ9"
   },
   "source": [
    "summary를 통해 model의 정보를 요약하고, plot_model 메서드로 모델의 구조를 시각화할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5tX4bXDi6vQ_",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "simple_model = create_model()\n",
    "simple_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oGaposiK6vRI",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keras.utils.plot_model(simple_model, 'flower_model_with_shape_info.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fz4KI8Ok6vRR"
   },
   "source": [
    "### Setting training parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bYrbdJzA6vRU"
   },
   "source": [
    "We specifiy the directory where tensorboard can save its logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kCGT66xT6vRW"
   },
   "outputs": [],
   "source": [
    "import datetime, os\n",
    "\n",
    "log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "os.makedirs(log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-x6971M6vRf"
   },
   "source": [
    "We also get values of the number of examples in train, val and test sets.\n",
    "* `steps_per_epoch`: It is the number of batches to train the model in one epoch. Its calculated by dividing the number of training examples by the size of each batch.\n",
    "* `validation_steps`: It is the same as `steps_per_epoch` but applies to validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oHQ6rrO76vRh"
   },
   "outputs": [],
   "source": [
    "num_train, num_val, num_test = (\n",
    "  metadata.splits['train'].num_examples * weight/10 for weight in [8,1,1]\n",
    ")\n",
    "\n",
    "steps_per_epoch = round(num_train)//BATCH_SIZE\n",
    "validation_steps = round(num_val)//BATCH_SIZE\n",
    "\n",
    "print('Number of examples in the train set:', num_train)\n",
    "print('Number of examples in the validation set:', num_val)\n",
    "print('Number of examples in the test set:', num_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVnTtebb6vRp"
   },
   "source": [
    "### Compile and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uf8oDR5r6vRt"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def train_model(model):\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Creating Keras callbacks \n",
    "    tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        'training_checkpoints/weights.{epoch:02d}-{val_loss:.2f}.h5')\n",
    "    os.makedirs('training_checkpoints/', exist_ok=True)\n",
    "    early_stopping_checkpoint = keras.callbacks.EarlyStopping(patience=5)\n",
    "\n",
    "    history = model.fit(train.repeat(),\n",
    "              epochs=50, \n",
    "              steps_per_epoch=steps_per_epoch,\n",
    "              validation_data=validation.repeat(),\n",
    "              validation_steps=validation_steps,\n",
    "              callbacks=[tensorboard_callback,\n",
    "                         model_checkpoint_callback,\n",
    "                         early_stopping_checkpoint])\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5fV1mJKu6vSZ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = train_model(simple_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UI734wAd6vSe"
   },
   "source": [
    "Plotting the training and validation metrics returned by the `train_model()` routine. We use matplotlib to plot the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lqA2fU0h6vSf"
   },
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8yo5qpbd6vSj"
   },
   "source": [
    "These graphs give us insights into how well our model has trained. It is necessary to ensure that both the training and validation accuracies increase and losses decrease.\n",
    "* If the <b>training accuracy is high but the validation accuracy is low</b>, then it's a typical case of overfitting. You may have to increase your training dataset by performing data augmentation or downloading more images from the internet. You can also try out other model architectures which include regularisation techniques like Dropout and BatchNormalisation.\n",
    "* If on the other hand your <b>training accuracy and validation accuracy both are higher</b> but, your <b>validation accuracy is slightly higher</b> then maybe your validation dataset comprises of ideal images of the given classes. Sometimes using techniques like dropout and BatchNorm add randomness in training, making training more difficult. To a lesser extent, it is also because training metrics report the average for an epoch, while validation metrics are evaluated after the epoch, so validation metrics see a model that has trained slightly longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_Fcu-qO6vSl"
   },
   "source": [
    "## 3. Using pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wowahn6n6vSm"
   },
   "source": [
    "pre-trained model 사용의 개념은 구글, 아마존 그리고 MS와 같은 기업에서 개발된 모델을 사용하는 것입니다. 그런 모델들은 1000개의 카테고리로 구성된 140만개의 이미지인 ImageNet 데이터셋을 학습했습니다. 이런 모델은 1000개의 오브젝트에 대한 기본적인 특징을 이미 학습했기 때문에 강력한 특징추출 능력을 갖고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ERUa4sdy6vSq"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# Create the base model from the pre-trained model MobileNet V2\n",
    "base_model = keras.applications.InceptionV3(input_shape=IMG_SHAPE,\n",
    "# We cannot use the top classification layer of the pre-trained model as it contains 1000 classes.\n",
    "# It also restricts our input dimensions to that which this model is trained on (default: 299x299)\n",
    "                                               include_top=False, \n",
    "                                               weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rj9EUgCd6vSx",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_model.trainable = False\n",
    "# Let's take a look at the base model architecture\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q9zsi75k6vS5"
   },
   "outputs": [],
   "source": [
    "keras.utils.plot_model(base_model, 'inception_model_with_shape_info.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwC3m4na6vTA"
   },
   "source": [
    "우리가 가져온 네트워크는 128x128x3 사진을 영상특징들로 변환하는 특징추출기(feature extractor)입니다. include_top=False 옵션을 통해, 우리 데이터를 위한 분류기를 추가해 학습할 수 있습니다. \n",
    "\n",
    "NOTE: It's important to freeze the convolutional based before we compile and train the model. By freezing (or setting `layer.trainable = False`), we prevent the weights in these layers from being updated during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7JSNJUM6vTB"
   },
   "source": [
    "### Adding classification head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wPIvxz2t6vTC"
   },
   "source": [
    "예측을 생성하기 위해 특징들의 spatial average를 계산하기 위해 `tf.keras.layers.GlobalAveragePooling2d` 레이어를 사용합니다. GAP 레이어는 특징맵을 벡터로 변환해주고, 그 뒤 FCN을 연결해 분류합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "mWnkpDhc6vTD"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-fa0cda4e1fa1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0minception_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0minception_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-fa0cda4e1fa1>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     model = keras.Sequential([\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGlobalAveragePooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "def build_model():\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        base_model,\n",
    "        keras.layers.GlobalAveragePooling2D(),\n",
    "        keras.layers.Dense(metadata.features['label'].num_classes, \n",
    "                           activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "inception_model = build_model()\n",
    "inception_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94oVhvUy6vTG"
   },
   "source": [
    "### Train the InceptionV3 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f3SV15r-6vTH"
   },
   "outputs": [],
   "source": [
    "# Evaluating model before training (Optional)\n",
    "loss0, accuracy0 = inception_model.evaluate(validation.repeat(), steps = validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TlQ_GjtX6vTJ"
   },
   "outputs": [],
   "source": [
    "log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "os.makedirs(log_dir)\n",
    "\n",
    "# Creating Keras callbacks \n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    'training_checkpoints/weights.{epoch:02d}-{val_loss:.2f}.hdf5', period=5)\n",
    "os.makedirs('training_checkpoints/', exist_ok=True)\n",
    "early_stopping_checkpoint = keras.callbacks.EarlyStopping(patience=5)\n",
    "\n",
    "\n",
    "\n",
    "history = inception_model.fit(train.repeat(),\n",
    "                              epochs=5,\n",
    "                              steps_per_epoch = steps_per_epoch,\n",
    "                              validation_data=validation.repeat(), \n",
    "                              validation_steps=validation_steps,\n",
    "                              callbacks=[tensorboard_callback,\n",
    "                              model_checkpoint_callback,\n",
    "                              early_stopping_checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvktN4hJ6vTM"
   },
   "source": [
    "After training the model for 5 epochs we were able to get ~70% accuracy. We plot the learning curves of the training and validation accuracy / loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VU0mxkhT6vTN"
   },
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "57usKx2F6vTV"
   },
   "outputs": [],
   "source": [
    "# Save keras model\n",
    "inception_model.save('inception_v3_128_tf_flowes.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mrKUIUo56vTZ"
   },
   "outputs": [],
   "source": [
    "loaded_model = keras.models.load_model('inception_v3_128_tf_flowes.h5')\n",
    "loaded_model.evaluate(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qvel3O0P6vTe"
   },
   "source": [
    "### Fine tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M_2YyN6I6vTg"
   },
   "outputs": [],
   "source": [
    "# Un-freeze the top layers of the model\n",
    "base_model.trainable = True\n",
    "# Let's take a look to see how many layers are in the base model\n",
    "print(\"Number of layers in the base model: \", len(base_model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q5yd_hRw6vTj"
   },
   "outputs": [],
   "source": [
    "# Fine tune from this layer onwards\n",
    "fine_tune_at = 249\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable =  False\n",
    "    \n",
    "# Compile the model using a much-lower training rate.\n",
    "inception_model.compile(optimizer = tf.keras.optimizers.RMSprop(lr=0.0001),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "inception_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0zpcHPy06vTq"
   },
   "source": [
    "If you trained to convergence earlier, this will get you a few percent more accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L-vYJVUa6vTr"
   },
   "outputs": [],
   "source": [
    "history_fine = inception_model.fit(train.repeat(), \n",
    "                                  steps_per_epoch = steps_per_epoch,\n",
    "                                  epochs=10, \n",
    "                                  initial_epoch = 5,\n",
    "                                  validation_data=validation.repeat(), \n",
    "                                  validation_steps=validation_steps,\n",
    "                                  callbacks=[tensorboard_callback,\n",
    "                                  model_checkpoint_callback,\n",
    "                                  early_stopping_checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_5rJLeZ6vTw"
   },
   "source": [
    "Note: If the training dataset is fairly small, and is similar to the original datasets that Inception V3 was trained on, so fine-tuning may result in overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SWxEUsSP6vTx"
   },
   "outputs": [],
   "source": [
    "acc += history_fine.history['accuracy']\n",
    "val_acc += history_fine.history['val_accuracy']\n",
    "\n",
    "loss += history_fine.history['loss']\n",
    "val_loss += history_fine.history['val_loss']\n",
    "\n",
    "initial_epochs=5\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.plot([initial_epochs-1,initial_epochs-1], \n",
    "          plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.plot([initial_epochs-1,initial_epochs-1], \n",
    "         plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03dYOfNS6vT1"
   },
   "source": [
    "\n",
    "In summary here is what we covered on how to do transfer learning using a pre-trained model to improve accuracy: \n",
    "* Using a pre-trained model for <b>feature extraction</b> - when working with a small dataset, it is common to leverage the features learned by a model trained on a larger dataset in the same domain. This is done by instantiating the pre-trained model and adding a fully connected classifier on top. The pre-trained model is \"frozen\" and only the weights of the classifier are updated during training. In this case, the convolutional base extracts all the features associated with each image and we train a classifier that determines, given these set of features to which class it belongs. \n",
    "* <b>Fine-tuning</b> a pre-trained model - to further improve performance, one might want to repurpose the top-level layers of the pre-trained models to the new dataset via fine-tuning. In this case, we tune our weights such that we learn highly specified and high level features specific to our dataset. This only make sense when the training dataset is large and very similar to the orginial dataset that the pre-trained model was trained on.\n",
    "\n",
    "References: \n",
    "1. https://www.tensorflow.org/alpha/tutorials/images/intro_to_cnns\n",
    "2. https://www.tensorflow.org/alpha/tutorials/images/transfer_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KC2TiLGn6vUW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Practical_intro_to_TF2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
