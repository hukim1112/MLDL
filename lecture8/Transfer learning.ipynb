{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZwAV0nGn6vPC"
   },
   "source": [
    "## 1. 다운로드 및 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DusCwxVS6vPD"
   },
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o2qxoh5JrYsr"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "try:\n",
    "  # Colab only\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80Y3rNyq6vPO"
   },
   "source": [
    "Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "60B2Xx056vPP"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "keras = tf.keras\n",
    "import tensorflow_datasets as tfds\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0ACWCVA6vPT"
   },
   "source": [
    "Check out the details (features, statistics, etc.) of the dataset to be downloaded here: https://www.tensorflow.org/datasets/datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2B2dasf6vPY"
   },
   "source": [
    "### 플라워 데이터셋 다운로드 및 로드하기 `tfds.load()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_UJfaBYH6vPZ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Since \"tf_flowers\" doesn't define standard splits, \n",
    "# use the subsplit feature to divide it into (train, validation, test)\n",
    "# with 80%, 10%, 10% of the data respectively.\n",
    "\n",
    "(raw_train, raw_validation, raw_test), metadata = tfds.load(name=\"tf_flowers\", \n",
    "                                                            with_info=True,\n",
    "                                                            split=('train[:80%]', 'train[80%:90%]', 'train[90%:]'),\n",
    "# specifying batch_size=-1 will load full dataset in the memory\n",
    "#                                                             batch_size=-1,\n",
    "# as_supervised: `bool`, if `True`, the returned `tf.data.Dataset`\n",
    "# will have a 2-tuple structure `(input, label)`                                                            \n",
    "                                                            as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bhPoKnhz6vPc"
   },
   "outputs": [],
   "source": [
    "print(raw_train)\n",
    "print(raw_validation)\n",
    "print(raw_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BLqAwKYF6vPk"
   },
   "source": [
    "데이터 파이프라인을 통한 전처리 함수를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GmVsKsz06vPl",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 기본적인 데이터 전처리 수행하기\n",
    "\n",
    "IMG_SIZE = 128\n",
    "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
    "\n",
    "def format_example(image, label):\n",
    "    print(\"Format example called!\")\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = image / 255.0\n",
    "    # Resize the image if required\n",
    "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "    return image, label\n",
    "\n",
    "train = raw_train.map(format_example)\n",
    "validation = raw_validation.map(format_example)\n",
    "test = raw_test.map(format_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sS8YmYJ9MTV8"
   },
   "source": [
    "### 효율적으로 데이터 증폭시키기\n",
    "Read more [here](https://stackoverflow.com/questions/55141076/how-to-apply-data-augmentation-in-tensorflow-2-0-after-tfds-load) to know how the map function applies data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PJ3tonJLMUOx"
   },
   "outputs": [],
   "source": [
    "def augment(image,label):\n",
    "  # Add 6 pixels of padding\n",
    "  image = tf.image.resize_with_crop_or_pad(image, IMG_SIZE + 6, IMG_SIZE + 6) \n",
    "   # Random crop back to the original size\n",
    "  image = tf.image.random_crop(image, size=[IMG_SIZE, IMG_SIZE, 3])\n",
    "  image = tf.image.random_brightness(image, max_delta=0.5) # Random brightness\n",
    "  image = tf.clip_by_value(image, 0, 1)\n",
    "  return image, label\n",
    "\n",
    "train = train.map(augment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1vbOSs76vPo"
   },
   "source": [
    "또한 우리는 데이터셋을 섞어 샘플 순서에 따른 편향을 없앱니다. 또한 32 크기의 미니배치를 생성하고, 데이터셋이 prefetch 될 수 있도록 설정합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BNGB5P36vPp"
   },
   "source": [
    "Without prefetch, the CPU and the GPU/TPU sit idle much of the time\n",
    "\n",
    "![alt text](https://www.tensorflow.org/images/datasets_without_pipelining.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qy7j1-5g6vPq"
   },
   "source": [
    "With prefetch, idle time diminishes significantly\n",
    "\n",
    "![alt text](https://www.tensorflow.org/images/datasets_with_pipelining.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hWJEyvNy6vPq"
   },
   "outputs": [],
   "source": [
    "SHUFFLE_BUFFER_SIZE = 1024\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train = train.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "validation = validation.batch(BATCH_SIZE)\n",
    "test = test.batch(BATCH_SIZE)\n",
    "# (Optional) prefetch will enable the input pipeline to asynchronously fetch batches while\n",
    "# your model is training.\n",
    "train = train.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "print(train)\n",
    "print(validation)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9gVRK_8U6vP0"
   },
   "source": [
    "### 데이터 시각화하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XVLYHdIQ6vP1"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t7KAUbUe6vP5"
   },
   "outputs": [],
   "source": [
    "# Get the function which converts label indices to string\n",
    "get_label_name = metadata.features['label'].int2str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPER0sD36vQA"
   },
   "source": [
    "Here we fetch the datatset batch by batch and convert it to numpy array just before passing it to plotting function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3IRrZBVh6vQC",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12)) \n",
    "for batch in train.take(1):\n",
    "    for i in range(9):\n",
    "        image, label = batch[0][i], batch[1][i]\n",
    "        plt.subplot(3,3,i+1)\n",
    "        plt.imshow(image.numpy())\n",
    "        plt.title(get_label_name(label.numpy()))\n",
    "        plt.grid(False)\n",
    "print(image.shape)\n",
    "print(np.min(image))\n",
    "print(np.max(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNX3KiVR6vQe"
   },
   "source": [
    "## 2.  `tf.keras`로 모델 빌드하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TTzNlpqy6vQh"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "caG4zs_p6vQt"
   },
   "outputs": [],
   "source": [
    "# Creating a simple CNN model in keras using functional API\n",
    "def create_model():\n",
    "    img_inputs = keras.Input(shape=IMG_SHAPE)\n",
    "    conv_1 = keras.layers.Conv2D(32, (3, 3), activation='relu')(img_inputs)\n",
    "    maxpool_1 = keras.layers.MaxPooling2D((2, 2))(conv_1)\n",
    "    conv_2 = keras.layers.Conv2D(64, (3, 3), activation='relu')(maxpool_1)\n",
    "    maxpool_2 = keras.layers.MaxPooling2D((2, 2))(conv_2)\n",
    "    conv_3 = keras.layers.Conv2D(64, (3, 3), activation='relu')(maxpool_2)\n",
    "    flatten = keras.layers.Flatten()(conv_3)\n",
    "    dense_1 = keras.layers.Dense(64, activation='relu')(flatten)\n",
    "    output = keras.layers.Dense(metadata.features['label'].num_classes, activation='softmax')(dense_1)\n",
    "\n",
    "    model = keras.Model(inputs=img_inputs, outputs=output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eG5P3tMa6vQ6"
   },
   "source": [
    "### Visualizing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dskLiSfU6vQ9"
   },
   "source": [
    "summary를 통해 model의 정보를 요약하고, plot_model 메서드로 모델의 구조를 시각화할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5tX4bXDi6vQ_",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "simple_model = create_model()\n",
    "simple_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oGaposiK6vRI",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keras.utils.plot_model(simple_model, 'flower_model_with_shape_info.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fz4KI8Ok6vRR"
   },
   "source": [
    "### Setting training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oHQ6rrO76vRh"
   },
   "outputs": [],
   "source": [
    "num_train, num_val, num_test = (\n",
    "  metadata.splits['train'].num_examples * weight/10 for weight in [8,1,1]\n",
    ")\n",
    "\n",
    "steps_per_epoch = round(num_train)//BATCH_SIZE\n",
    "validation_steps = round(num_val)//BATCH_SIZE\n",
    "\n",
    "print('Number of examples in the train set:', num_train)\n",
    "print('Number of examples in the validation set:', num_val)\n",
    "print('Number of examples in the test set:', num_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVnTtebb6vRp"
   },
   "source": [
    "### Compile and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uf8oDR5r6vRt"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def train_model(model):\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Creating Keras callbacks \n",
    "    os.makedirs('training_checkpoints/', exist_ok=True)\n",
    "    model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        'training_checkpoints/weights.{epoch:02d}-{val_loss:.2f}.h5')\n",
    "    early_stopping_checkpoint = keras.callbacks.EarlyStopping(patience=5)\n",
    "\n",
    "    history = model.fit(train,\n",
    "              epochs=50, \n",
    "              validation_data=validation,\n",
    "              callbacks=[model_checkpoint_callback,\n",
    "                         early_stopping_checkpoint])\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5fV1mJKu6vSZ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = train_model(simple_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UI734wAd6vSe"
   },
   "source": [
    "Plotting the training and validation metrics returned by the `train_model()` routine. We use matplotlib to plot the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lqA2fU0h6vSf"
   },
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8yo5qpbd6vSj"
   },
   "source": [
    "그래프는 모델 학습이 어떻게 이루어졌는지 알려줍니다. 학습과 검증데이터에 대해 정확도는 상승하고, 비용은 감시했는지 확인해야합니다.\n",
    "\n",
    "만약 학습 정확도는 높지만 검증 정확도가 낮다면, 전형적인 과대적합의 케이스입니다. 이런 경우 학습데이터를 더 늘리거나 데이터증폭이 필요할 수 있습니다. 또한 Batchnorm이나 Dropout과 같은 정규화 기능을 갖는 모델을 시도해볼 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_Fcu-qO6vSl"
   },
   "source": [
    "## 3. Using pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wowahn6n6vSm"
   },
   "source": [
    "pre-trained model 사용의 개념은 구글, 아마존 그리고 MS와 같은 기업에서 개발된 모델을 사용하는 것입니다. 그런 모델들은 1000개의 카테고리로 구성된 140만개의 이미지인 ImageNet 데이터셋을 학습했습니다. 이런 모델은 1000개의 오브젝트에 대한 기본적인 특징을 이미 학습했기 때문에 강력한 특징추출 능력을 갖고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ERUa4sdy6vSq"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# Create the base model from the pre-trained model MobileNet V2\n",
    "base_model = keras.applications.InceptionV3(input_shape=IMG_SHAPE,\n",
    "# We cannot use the top classification layer of the pre-trained model as it contains 1000 classes.\n",
    "# It also restricts our input dimensions to that which this model is trained on (default: 299x299)\n",
    "                                               include_top=False, \n",
    "                                               weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rj9EUgCd6vSx",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_model.trainable = False\n",
    "# Let's take a look at the base model architecture\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q9zsi75k6vS5"
   },
   "outputs": [],
   "source": [
    "keras.utils.plot_model(base_model, 'inception_model_with_shape_info.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwC3m4na6vTA"
   },
   "source": [
    "우리가 가져온 네트워크는 128x128x3 사진을 영상특징들로 변환하는 특징추출기(feature extractor)입니다. include_top=False 옵션을 통해, 우리 데이터를 위한 분류기를 추가해 학습할 수 있습니다. \n",
    "\n",
    "NOTE: It's important to freeze the convolutional based before we compile and train the model. By freezing (or setting `layer.trainable = False`), we prevent the weights in these layers from being updated during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7JSNJUM6vTB"
   },
   "source": [
    "### Adding classification head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wPIvxz2t6vTC"
   },
   "source": [
    "예측을 생성하기 위해 특징들의 spatial average를 계산하기 위해 `tf.keras.layers.GlobalAveragePooling2d` 레이어를 사용합니다. GAP 레이어는 특징맵을 벡터로 변환해주고, 그 뒤 FCN을 연결해 분류합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mWnkpDhc6vTD"
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        base_model,\n",
    "        keras.layers.GlobalAveragePooling2D(),\n",
    "        keras.layers.Dense(metadata.features['label'].num_classes, \n",
    "                           activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "inception_model = build_model()\n",
    "inception_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94oVhvUy6vTG"
   },
   "source": [
    "### Train the InceptionV3 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f3SV15r-6vTH"
   },
   "outputs": [],
   "source": [
    "# Evaluating model before training (Optional)\n",
    "loss0, accuracy0 = inception_model.evaluate(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TlQ_GjtX6vTJ"
   },
   "outputs": [],
   "source": [
    "# Creating Keras callbacks \n",
    "os.makedirs('training_checkpoints/', exist_ok=True)\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        'training_checkpoints/weights.{epoch:02d}-{val_loss:.2f}.h5')\n",
    "early_stopping_checkpoint = keras.callbacks.EarlyStopping(patience=5)\n",
    "\n",
    "history = inception_model.fit(train,\n",
    "                              epochs=5,\n",
    "                              validation_data=validation, \n",
    "                              callbacks=[model_checkpoint_callback,\n",
    "                              early_stopping_checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvktN4hJ6vTM"
   },
   "source": [
    "After training the model for 5 epochs we were able to get ~70% accuracy. We plot the learning curves of the training and validation accuracy / loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VU0mxkhT6vTN"
   },
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mrKUIUo56vTZ"
   },
   "outputs": [],
   "source": [
    "inception_model.evaluate(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qvel3O0P6vTe"
   },
   "source": [
    "### Fine tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M_2YyN6I6vTg"
   },
   "outputs": [],
   "source": [
    "# Un-freeze the top layers of the model\n",
    "base_model.trainable = True\n",
    "# Let's take a look to see how many layers are in the base model\n",
    "print(\"Number of layers in the base model: \", len(base_model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q5yd_hRw6vTj"
   },
   "outputs": [],
   "source": [
    "# Fine tune from this layer onwards\n",
    "fine_tune_at = 249\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable =  False\n",
    "    \n",
    "# Compile the model using a much-lower training rate.\n",
    "inception_model.compile(optimizer = tf.keras.optimizers.RMSprop(lr=0.0001),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "inception_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0zpcHPy06vTq"
   },
   "source": [
    "If you trained to convergence earlier, this will get you a few percent more accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L-vYJVUa6vTr"
   },
   "outputs": [],
   "source": [
    "history_fine = inception_model.fit(train, \n",
    "                                  epochs=10, \n",
    "                                  initial_epoch = 5,\n",
    "                                  validation_data=validation, \n",
    "                                  callbacks=[model_checkpoint_callback,\n",
    "                                  early_stopping_checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_5rJLeZ6vTw"
   },
   "source": [
    "Note: If the training dataset is fairly small, and is similar to the original datasets that Inception V3 was trained on, so fine-tuning may result in overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SWxEUsSP6vTx"
   },
   "outputs": [],
   "source": [
    "acc += history_fine.history['accuracy']\n",
    "val_acc += history_fine.history['val_accuracy']\n",
    "\n",
    "loss += history_fine.history['loss']\n",
    "val_loss += history_fine.history['val_loss']\n",
    "\n",
    "initial_epochs=5\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.plot([initial_epochs-1,initial_epochs-1], \n",
    "          plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.plot([initial_epochs-1,initial_epochs-1], \n",
    "         plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception_model.evaluate(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03dYOfNS6vT1"
   },
   "source": [
    "\n",
    "In summary here is what we covered on how to do transfer learning using a pre-trained model to improve accuracy: \n",
    "* Using a pre-trained model for <b>feature extraction</b> - when working with a small dataset, it is common to leverage the features learned by a model trained on a larger dataset in the same domain. This is done by instantiating the pre-trained model and adding a fully connected classifier on top. The pre-trained model is \"frozen\" and only the weights of the classifier are updated during training. In this case, the convolutional base extracts all the features associated with each image and we train a classifier that determines, given these set of features to which class it belongs. \n",
    "* <b>Fine-tuning</b> a pre-trained model - to further improve performance, one might want to repurpose the top-level layers of the pre-trained models to the new dataset via fine-tuning. In this case, we tune our weights such that we learn highly specified and high level features specific to our dataset. This only make sense when the training dataset is large and very similar to the orginial dataset that the pre-trained model was trained on.\n",
    "\n",
    "References: \n",
    "1. https://www.tensorflow.org/alpha/tutorials/images/intro_to_cnns\n",
    "2. https://www.tensorflow.org/alpha/tutorials/images/transfer_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KC2TiLGn6vUW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Practical_intro_to_TF2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
