{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Preprocessing_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hukim1112/MLDL/blob/master/lecture9/Text_Preprocessing_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5KLiswFjbwl"
      },
      "source": [
        "# **06) 정수 인코딩(Integer Encoding)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_2g4byUl5y7"
      },
      "source": [
        "컴퓨터는 텍스트보다는 숫자를 더 잘 처리 하기 때문에 자연어 처리에서는 텍스트를 숫자로 바꾸는 여러가지 기법들 존재\n",
        "\n",
        "=> 이러한 기법들을 본격적으로 적용하기 위한 첫 단계 : 각 단어를 고유한 정수에 맵핑(mapping)시키는 전처리 작업\n",
        "\n",
        "예를 들어 갖고 있는 텍스트에 단어가 5,000개가 있다면, 5,000개의 단어들 각각에 1번부터 5,000번까지 단어와 맵핑되는 고유한 정수, 다른 표현으로는 인덱스를 부여(예를 들어, book은 150번, dog는 171번, love는 192번, books는 212번과 같이 숫자 부여)\n",
        "\n",
        "인덱스를 부여하는 방법은 여러 가지가 있을 수 있는데 랜덤으로 부여하기도 하지만, 보통은 단어에 대한 빈도수를 기준으로 정렬한 뒤에 부여"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KaLYUSWnOD5"
      },
      "source": [
        "> ## **1. 정수 인코딩(Integer Encoding)**\n",
        "\n",
        "단어에 정수를 부여하는 방법 중 하나\n",
        "\n",
        "==> 단어를 빈도수 순으로 정렬한 단어 집합(vocabulary)를 만들고, 빈도수가 높은 순서대로 차례로 낮은 숫자부터 정수를 부여하는 방법\n",
        "\n",
        "==> 자연어 처리 작업 이전에 단어가 텍스트일 때만 할 수 있는 최대한의 전처리를 끝내고 난 후 텍스트를 숫자로 변환해야 함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3YknB3nncG7"
      },
      "source": [
        "---\n",
        "### **1.1) dictionary 사용하기**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBoOaqV4nyUa"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCOh7-BuoErh"
      },
      "source": [
        "text = \"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! \\\n",
        "The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. \\\n",
        "But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0-LtQzrpQBj",
        "outputId": "52e41362-943f-4d9a-c003-6235c7e6b362"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FusRQSfpoKcg",
        "outputId": "0ce63cdf-84f9-4b93-c1d0-635519df174c"
      },
      "source": [
        "#############################################################\n",
        "### 문장 토큰화 - 토큰의 단위를 문장으로 하여 토큰화 수행 ###\n",
        "#############################################################\n",
        "\n",
        "text = sent_tokenize(text)\n",
        "print(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['A barber is a person.', 'a barber is good person.', 'a barber is huge person.', 'he Knew A Secret!', 'The Secret He Kept is huge secret.', 'Huge secret.', 'His barber kept his word.', 'a barber kept his word.', 'His barber kept his secret.', 'But keeping and keeping such a huge secret to himself was driving the barber crazy.', 'the barber went up a huge mountain.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvyRetr6pWjx",
        "outputId": "c10f852b-967d-434e-8378-38241bacc91c"
      },
      "source": [
        "##########################\n",
        "### 정제와 단어 토큰화 ###\n",
        "##########################\n",
        "\n",
        "vocab = {} ## 파이썬의 dictionary 자료형\n",
        "sentences = []\n",
        "stop_words = set(stopwords.words('english')) ## NLTK에서 정의한 불용어 패키지(I, my, me, over, 조사, 접미사 등 큰 의미가 없는 단어)\n",
        "\n",
        "for i in text:\n",
        "    ### 단어 토큰화 ###\n",
        "    sentence = word_tokenize(i)\n",
        "    # print(sentence)\n",
        "    result = []\n",
        "\n",
        "    ### 정제 작업 ### -> 불용어 / 빈도수가 낮은 단어 / 길이가 짧은 단어 제거\n",
        "    for word in sentence: \n",
        "        word = word.lower() ## 소문자화하여 단어의 개수 줄이기 (동일한 단어가 대문자로 표기되어 다른 단어로 카운트 되는 경우 방지)\n",
        "        if word not in stop_words: ## 불용어 제거\n",
        "            if len(word) > 2: ## 단어 길이가 2 이하인 경우 추가로 제거\n",
        "                result.append(word)\n",
        "                if word not in vocab:\n",
        "                    vocab[word] = 0 \n",
        "                vocab[word] += 1 ## vocab 사전에는 중복을 제거한 단어의 빈도수 저장\n",
        "\n",
        "    sentences.append(result) \n",
        "print(sentences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rY7Gp_EtpynA",
        "outputId": "e70afdc2-c08c-434c-c990-6c5b56cc6d7e"
      },
      "source": [
        "########################################\n",
        "### 빈도수가 저장된 vocab dictionary ###\n",
        "########################################\n",
        "\n",
        "print(vocab) ## 단어를 키(key)로, 단어에 대한 빈도수가 값(value)으로 저장됨"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 8, 'person': 3, 'good': 1, 'huge': 5, 'knew': 1, 'secret': 6, 'kept': 4, 'word': 2, 'keeping': 2, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9GVctDpqnig",
        "outputId": "b50c5ded-78bb-447f-bf09-205fa4af8372"
      },
      "source": [
        "print(vocab[\"barber\"]) ## 'barber'라는 단어의 빈도수 출력 (vocab에 단어를 입력하면 빈도수를 리턴)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjA6qZk_qsFg",
        "outputId": "e9cb4cb7-53e1-4cac-99b6-b03fbbe54315"
      },
      "source": [
        "##########################\n",
        "### 빈도수 순으로 정렬 ### -> 빈도수가 높을수록 낮은 정수 인덱스를 부여하기 위해\n",
        "##########################\n",
        "\n",
        "vocab_sorted = sorted(vocab.items(), key = lambda x:x[1], reverse = True)\n",
        "print(vocab_sorted)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3), ('word', 2), ('keeping', 2), ('good', 1), ('knew', 1), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yyr7VLyWp2OI",
        "outputId": "536b1259-772e-43ba-b1ce-cd20d2354e15"
      },
      "source": [
        "######################################\n",
        "### 빈도수에 따른 정수 인덱스 부여 ###\n",
        "######################################\n",
        "\n",
        "word_to_index = {}\n",
        "i=0\n",
        "for (word, frequency) in vocab_sorted :\n",
        "    if frequency > 1 : # 빈도수 적은 단어는 제외 (등장 빈도가 낮은 단어는 자연어 처리에서 의미를 가지지 않을 가능성이 높기 때문)\n",
        "        i=i+1\n",
        "        word_to_index[word] = i ## 높은 빈도수를 가진 단어일수록 낮은 정수 인덱스를 부여\n",
        "print(word_to_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yv_da5QDq0xs",
        "outputId": "5c691a66-6507-4360-c948-22389588c826"
      },
      "source": [
        "###################################\n",
        "### 빈도수 상위 5개 단어만 저장 ###\n",
        "###################################\n",
        "\n",
        "vocab_size = 5\n",
        "words_frequency = [w for w,c in word_to_index.items() if c >= vocab_size + 1] # 인덱스가 5 초과인 단어 제거\n",
        "for w in words_frequency:\n",
        "    del word_to_index[w] # 해당 단어에 대한 인덱스 정보를 삭제\n",
        "print(word_to_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12Tlwtl8mLMp",
        "outputId": "3b81b2be-1442-416e-b3ab-abd214c86aa4"
      },
      "source": [
        "######################################\n",
        "### word_to_index에 없는 단어 처리 ###\n",
        "######################################\n",
        "\n",
        "print(text[0], \"=>\", sentences[0], \"=>\", \"[1,5]\")\n",
        "print(text[1], \"=>\", sentences[1], \"=>\", \"[1,?,5]\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A barber is a person. => ['barber', 'person'] => [1,5]\n",
            "a barber is good person. => ['barber', 'good', 'person'] => [1,?,5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dy5qZ086q5PZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddf35756-37f0-4785-9c8e-b0a1f5e15a7b"
      },
      "source": [
        "################################\n",
        "### Out-Of-Vocabulary 인코딩 ###\n",
        "################################\n",
        "\n",
        "## good과 같이 단어 집합에 존재하지 않는 단어들을 Out-Of-Vocabulary(단어 집합에 없는 단어)의 약자로 'OOV'라고 함\n",
        "## word_to_index에 'OOV'란 단어를 새롭게 추가하고, 단어 집합에 없는 단어들은 'OOV'의 인덱스로 인코딩\n",
        "word_to_index['OOV'] = len(word_to_index) + 1\n",
        "word_to_index"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'OOV': 6, 'barber': 1, 'huge': 3, 'kept': 4, 'person': 5, 'secret': 2}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnbZmRFOq9-w",
        "outputId": "f2ab09ec-c0c0-47e5-e11f-ba097b5fc3c0"
      },
      "source": [
        "#########################################################\n",
        "### sentences에 있는 단어를 정수로 변환 = 정수 인코딩 ###\n",
        "#########################################################\n",
        "\n",
        "encoded = []\n",
        "for s in sentences: ## word_to_index를 사용하여 sentences의 모든 단어들을 맵핑되는 정수로 인코딩\n",
        "    temp = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            temp.append(word_to_index[w])\n",
        "        except KeyError:\n",
        "            temp.append(word_to_index['OOV'])\n",
        "    encoded.append(temp)\n",
        "print(encoded)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 5], [1, 6, 5], [1, 3, 5], [6, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [6, 6, 3, 2, 6, 1, 6], [1, 6, 3, 6]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeCZqsBsrCCR"
      },
      "source": [
        "---\n",
        "### **1.2) Counter 사용하기**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_ysIls2rLR6"
      },
      "source": [
        "from collections import Counter\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41mmZLR2rPBI",
        "outputId": "abc0413d-30f1-4aa7-9099-a72d8cabf6c5"
      },
      "source": [
        "print(sentences) ## 단어 토큰화 결과 저장 -> 단어 집합(vocabulary)를 만들기 위해 [,] 제거 필요 -> 하나의 리스트로 만들기"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hS2rmht2rUnZ",
        "outputId": "6b83c56d-0b82-4b4e-c47d-3a8f7c35ef40"
      },
      "source": [
        "########################################\n",
        "### sentences를 하나의 리스트로 변환 ###\n",
        "########################################\n",
        "\n",
        "words = np.hstack(sentences) ## 배열 결합 시 수평으로 결합\n",
        "print(words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['barber' 'person' 'barber' 'good' 'person' 'barber' 'huge' 'person'\n",
            " 'knew' 'secret' 'secret' 'kept' 'huge' 'secret' 'huge' 'secret' 'barber'\n",
            " 'kept' 'word' 'barber' 'kept' 'word' 'barber' 'kept' 'secret' 'keeping'\n",
            " 'keeping' 'huge' 'secret' 'driving' 'barber' 'crazy' 'barber' 'went'\n",
            " 'huge' 'mountain']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxAGDqYarY7w",
        "outputId": "694038a6-bee6-4c89-c0fe-cb6b0448f8c3"
      },
      "source": [
        "#################################################################\n",
        "### 파이썬의 Counter() 모듈로 중복 제거 및 단어의 빈도수 기록 ###\n",
        "#################################################################\n",
        "\n",
        "vocab = Counter(words)\n",
        "print(vocab) ## dictonary와 똑같이 단어를 키(key)로, 단어에 대한 빈도수가 값(value)으로 저장 + 빈도수 순으로 정렬"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'barber': 8, 'secret': 6, 'huge': 5, 'kept': 4, 'person': 3, 'word': 2, 'keeping': 2, 'good': 1, 'knew': 1, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHwmPtcArcwh",
        "outputId": "8754e4af-4aa1-4e83-d828-8b90adcfb0e7"
      },
      "source": [
        "print(vocab[\"barber\"])  ## 'barber'라는 단어의 빈도수 출력 (vocab에 단어를 입력하면 빈도수를 리턴)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkmoiELUrgY7",
        "outputId": "c46e2288-ed7f-4ca0-f946-371768d798e3"
      },
      "source": [
        "###################################################\n",
        "### most_common()로 빈도수 상위 5개 단어만 저장 ###\n",
        "###################################################\n",
        "\n",
        "vocab_size = 5\n",
        "vocab = vocab.most_common(vocab_size) # most_common() -> 상위 빈도수를 가진 주어진 수의 단어만을 리턴\n",
        "vocab"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-M_wIRuqrjXx",
        "outputId": "ce7e31cd-3e9b-41f6-a6fc-36d66f790bdc"
      },
      "source": [
        "######################################\n",
        "### 빈도수에 따른 정수 인덱스 부여 ###\n",
        "######################################\n",
        "\n",
        "word_to_index = {}\n",
        "i = 0\n",
        "for (word, frequency) in vocab : ## 높은 빈도수를 가진 단어일수록 낮은 정수 인덱스를 부여\n",
        "    i = i+1\n",
        "    word_to_index[word] = i\n",
        "print(word_to_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gixMdGVCrlIp"
      },
      "source": [
        "---\n",
        "### **1.3) NLTK의 FreqDist 사용하기**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adoedo85rsfB"
      },
      "source": [
        "NLTK에서는 빈도수 계산 도구인 FreqDist()를 지원, 위에서 사용한 Counter()랑 같은 방법으로 사용 가능"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiEySgWCrnU5"
      },
      "source": [
        "from nltk import FreqDist\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlVghTNTrna1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccbc7a19-7974-4ec2-ff05-2a2867d5d656"
      },
      "source": [
        "################################################################\n",
        "### NLTK의 FreqDist() 모듈로 중복 제거 및 단어의 빈도수 기록 ###\n",
        "################################################################\n",
        "\n",
        "vocab = FreqDist(np.hstack(sentences)) ## np.hstack으로 문장 구분을 제거하여 입력으로 사용\n",
        "print(vocab)  ## dictonary, Counter()와 똑같이 단어를 키(key)로, 단어에 대한 빈도수가 값(value)으로 저장 + 빈도수 순으로 정렬"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<FreqDist with 13 samples and 36 outcomes>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EloIinPr31c",
        "outputId": "2876d98d-7aef-4e16-89ce-5e6c4b1921e1"
      },
      "source": [
        "print(vocab[\"barber\"])  ## 'barber'라는 단어의 빈도수 출력 (vocab에 단어를 입력하면 빈도수를 리턴)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzFgR7tBr7j4",
        "outputId": "f3a1cf68-545b-4240-eb85-55e056032ec4"
      },
      "source": [
        "###################################################\n",
        "### most_common()로 빈도수 상위 5개 단어만 저장 ###\n",
        "###################################################\n",
        "\n",
        "vocab_size = 5\n",
        "vocab = vocab.most_common(vocab_size) # 등장 빈도수가 높은 상위 5개의 단어만 저장\n",
        "vocab"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RbUF2fEsAVA",
        "outputId": "8e5d83d7-4766-4a33-d2ad-f6f94de0dc61"
      },
      "source": [
        "####################################################\n",
        "### enumerate()로 빈도수에 따른 정수 인덱스 부여 ###\n",
        "####################################################\n",
        "\n",
        "word_to_index = {word[0] : index + 1 for index, word in enumerate(vocab)}\n",
        "print(word_to_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5ZGtZ0LsC5p"
      },
      "source": [
        "---\n",
        "### **1.4) enumerate 이해하기**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoRvtgxHsIPx"
      },
      "source": [
        "enumerate()는 순서가 있는 자료형(list, set, tuple, dictionary, string)을 입력으로 받아 인덱스를 순차적으로 함께 리턴한다는 특징이 있음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7eKbjVcsLEp",
        "outputId": "91bd7f78-2ee6-44cb-ca70-a3a122c556cf"
      },
      "source": [
        "test=['a', 'b', 'c', 'd', 'e']\n",
        "for index, value in enumerate(test): # 입력의 순서대로 0부터 인덱스를 부여함\n",
        "  print(\"value : {}, index: {}\".format(value, index))\n",
        "\n",
        "## 리스트의 모든 토큰에 대해서 인덱스가 순차적으로 증가되며 부여된 것을 보여줌"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "value : a, index: 0\n",
            "value : b, index: 1\n",
            "value : c, index: 2\n",
            "value : d, index: 3\n",
            "value : e, index: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhokVDMgsOux"
      },
      "source": [
        "\n",
        "> ## **2. 케라스(Keras)의 텍스트 전처리**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V25Yo9L1sV_b"
      },
      "source": [
        "케라스(Keras)는 기본적인 전처리를 위한 도구들을 제공\n",
        "\n",
        "때로는 정수 인코딩을 위해서 케라스의 전처리 도구인 토크나이저를 사용하기도 함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXuQVInusYHU"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4L7iBRpPsRXZ"
      },
      "source": [
        "## 단어 토큰화까지 수행된 앞서 사용한 텍스트 데이터와 동일한 데이터 사용\n",
        "sentences=[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'],\n",
        "           ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'],\n",
        "           ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1sOCr_osYDA"
      },
      "source": [
        "##############################################\n",
        "### fit_on_texts()로 정수 인코딩 작업 수행 ###\n",
        "##############################################\n",
        "## fit_on_texts() : 입력한 텍스트로부터 단어 빈도수가 높은 순으로 낮은 정수 인덱스를 부여\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences) ## fit_on_texts()안에 코퍼스를 입력하여 빈도수를 기준으로 단어 집합 생성"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwpG7oy7sqZT",
        "outputId": "c809a4a7-213d-4876-98c7-25ce34b7b952"
      },
      "source": [
        "##################################################\n",
        "### fit_on_texts 사용 후 빈도수 및 인덱스 확인 ###\n",
        "##################################################\n",
        "\n",
        "## word_counts로 각 단어가 카운트를 수행하였을 때 몇 개였는지 개수 확인 가능\n",
        "print(\"< word_counts >\")\n",
        "print(tokenizer.word_counts)\n",
        "\n",
        "## word_index로 각 단어에 인덱스가 어떻게 부여되는지 확인 가능\n",
        "print(\"\\n< word_index >\")\n",
        "print(tokenizer.word_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "< word_counts >\n",
            "OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])\n",
            "\n",
            "< word_index >\n",
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "io_A_s__ssks",
        "outputId": "680bc4f1-8a18-4268-b3ee-108b1a4b2cbd"
      },
      "source": [
        "##########################################\n",
        "### texts_to_sequences()로 정수 인코딩 ###\n",
        "##########################################\n",
        "## texts_to_sequences() : 입력으로 들어온 코퍼스에 대해서 각 단어를 이미 정해진 인덱스로 변환\n",
        "\n",
        "print(tokenizer.texts_to_sequences(sentences)) ## --> 상위 빈도수에 상관없이 모든 단어에 인덱스 부여"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnkJbEE5swnk"
      },
      "source": [
        "###################################\n",
        "### 빈도수 상위 5개 단어만 저장 ###\n",
        "###################################\n",
        "## tokenizer = Tokenizer(num_words=숫자)와 같은 방법으로 빈도수가 높은 상위 몇 개의 단어만 사용하겠다고 지정 가능\n",
        "\n",
        "vocab_size = 5\n",
        "tokenizer = Tokenizer(num_words = vocab_size + 1) # 상위 5개 단어만 사용\n",
        "tokenizer.fit_on_texts(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCk_y5RDs46z",
        "outputId": "d9ad33d8-ca77-4e1e-d1d0-ad2a3889713b"
      },
      "source": [
        "##################################################\n",
        "### fit_on_texts 사용 후 빈도수 및 인덱스 확인 ### --> 상위 빈도수에 상관없이 13개의 단어 모두 출력\n",
        "##################################################\n",
        "\n",
        "## word_counts로 각 단어가 카운트를 수행하였을 때 몇 개였는지 개수 확인 가능\n",
        "print(\"< word_counts >\")\n",
        "print(tokenizer.word_counts)\n",
        "\n",
        "## word_index로 각 단어에 인덱스가 어떻게 부여되는지 확인 가능\n",
        "print(\"\\n< word_index >\")\n",
        "print(tokenizer.word_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "< word_counts >\n",
            "OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])\n",
            "\n",
            "< word_index >\n",
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPjrf99otA80",
        "outputId": "96963367-97ac-42f1-9ac9-05cccdc9d813"
      },
      "source": [
        "##########################################\n",
        "### texts_to_sequences()로 정수 인코딩 ### --> 상위 5개의 단어만 사용되어 1번 단어부터 5번 단어까지만 보존되고 나머지 단어들은 제거됨\n",
        "##########################################\n",
        "\n",
        "print(tokenizer.texts_to_sequences(sentences)) ## 케라스 토크나이저는 기본적으로 단어 집합에 없는 단어인 OOV에 대해서는 단어를 정수로 바꾸는 과정에서 아예 단어를 제거"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 5], [1, 5], [1, 3, 5], [2], [2, 4, 3, 2], [3, 2], [1, 4], [1, 4], [1, 4, 2], [3, 2, 1], [1, 3]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFLqhz97tFvY",
        "outputId": "2cb74dbb-4ccc-4705-82af-b617e624a97b"
      },
      "source": [
        "##############################################################################\n",
        "### word_counts와 word_index에서도 상위 빈도수 5개의 단어만 남기고 싶다면? ###\n",
        "##############################################################################\n",
        "\n",
        "tokenizer = Tokenizer() # num_words를 여기서는 지정하지 않은 상태\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "vocab_size = 5\n",
        "words_frequency = [w for w,c in tokenizer.word_index.items() if c >= vocab_size + 1] # 인덱스가 5 초과인 단어 제거\n",
        "for w in words_frequency:\n",
        "    del tokenizer.word_index[w] # 해당 단어에 대한 인덱스 정보를 삭제\n",
        "    del tokenizer.word_counts[w] # 해당 단어에 대한 카운트 정보를 삭제\n",
        "\n",
        "print(\"< word_index >\")\n",
        "print(tokenizer.word_index)\n",
        "print(\"\\n< word_counts >\")\n",
        "print(tokenizer.word_counts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "< word_index >\n",
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n",
            "\n",
            "< word_counts >\n",
            "OrderedDict([('barber', 8), ('person', 3), ('huge', 5), ('secret', 6), ('kept', 4)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ElpADCbtJH5"
      },
      "source": [
        "#####################################################\n",
        "### 단어 집합에 없는 단어들을 OOV로 보존하는 방법 ### -> Tokenizer의 인자 oov_token 사용\n",
        "#####################################################\n",
        "\n",
        "vocab_size = 5\n",
        "tokenizer = Tokenizer(num_words = vocab_size + 2, oov_token = 'OOV')\n",
        "# 빈도수 상위 5개 단어만 사용. 숫자 0과 OOV를 고려해서 단어 집합의 크기는 +2\n",
        "tokenizer.fit_on_texts(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQukhbTQtQGS",
        "outputId": "1a8f87b4-25d9-4463-b435-c3dc8a79ba66"
      },
      "source": [
        "## 케라스 토크나이저는 기본적으로 'OOV'의 인덱스를 1로 함\n",
        "print('단어 OOV의 인덱스 : {}'.format(tokenizer.word_index['OOV']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 OOV의 인덱스 : 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHLzL73utTE3",
        "outputId": "2508ff9b-f804-44eb-8467-eed2fcd70cd0"
      },
      "source": [
        "##########################################\n",
        "### texts_to_sequences()로 정수 인코딩 ### --> 상위 5개의 단어는 인덱스 2-6 부여, 나머지 단어들(OOV)은 인덱스 1을 부여\n",
        "##########################################\n",
        "\n",
        "print(tokenizer.texts_to_sequences(sentences))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2, 6], [2, 1, 6], [2, 4, 6], [1, 3], [3, 5, 4, 3], [4, 3], [2, 5, 1], [2, 5, 1], [2, 5, 3], [1, 1, 4, 3, 1, 2, 1], [2, 1, 4, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E76dkvRwjjYj"
      },
      "source": [
        "# **07) 패딩(Padding)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvp8HysFtaVP"
      },
      "source": [
        "자연어 처리를 하다보면 각 문장(또는 문서)은 서로 길이가 다를 수 있음\n",
        "\n",
        "그런데 기계는 길이가 전부 동일한 문서들에 대해서는 하나의 행렬로 보고, 한꺼번에 묶어서 처리할 수 있음\n",
        "\n",
        "다시 말해 병렬 연산을 위해서 여러 문장의 길이를 임의로 동일하게 맞춰주는 작업이 필요할 때가 있음"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uswsVnXntfWq"
      },
      "source": [
        "> ## **1. Numpy로 패딩하기**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cftfMB5Djij2"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkQuB6Bitq_y"
      },
      "source": [
        "## 단어 토큰화까지 수행된 앞서 사용한 텍스트 데이터와 동일한 데이터 사용\n",
        "sentences = [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'],\n",
        "             ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'],\n",
        "             ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8c0q73aCtrL6"
      },
      "source": [
        "##############################################\n",
        "### fit_on_texts()로 정수 인코딩 작업 수행 ###\n",
        "##############################################\n",
        "## fit_on_texts() : 입력한 텍스트로부터 단어 빈도수가 높은 순으로 낮은 정수 인덱스를 부여\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences) ## fit_on_texts()안에 코퍼스를 입력하여 빈도수를 기준으로 단어 집합 생성"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dneEgbQ4t6n7",
        "outputId": "31461c5c-1887-4846-d390-95590c589851"
      },
      "source": [
        "############################################################\n",
        "### 텍스트 시퀀스의 모든 단어들을 각 정수로 맵핑 후 출력 ###\n",
        "############################################################\n",
        "\n",
        "encoded = tokenizer.texts_to_sequences(sentences)\n",
        "print(encoded)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iS3O-nFQt7CL",
        "outputId": "fd39d74c-d2b0-40eb-8e93-9f8ca201a40f"
      },
      "source": [
        "## 모두 동일한 길이로 맞춰주기 위해서 이 중에서 가장 길이가 긴 문장의 길이 계산\n",
        "max_len = max(len(item) for item in encoded)\n",
        "print(max_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22kQwVt-uD-u",
        "outputId": "897bc291-0028-4b9f-aade-fa89dcba8a3d"
      },
      "source": [
        "#######################################################\n",
        "### 모든 문장의 길이를 가장 긴 문장의 길이에 맞추기 ###\n",
        "#######################################################\n",
        "\n",
        "for item in encoded: # 각 문장에 대해서\n",
        "    while len(item) < max_len:   # max_len보다 작으면\n",
        "        item.append(0)\n",
        "\n",
        "padded_np = np.array(encoded)\n",
        "padded_np"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  5,  0,  0,  0,  0,  0],\n",
              "       [ 1,  8,  5,  0,  0,  0,  0],\n",
              "       [ 1,  3,  5,  0,  0,  0,  0],\n",
              "       [ 9,  2,  0,  0,  0,  0,  0],\n",
              "       [ 2,  4,  3,  2,  0,  0,  0],\n",
              "       [ 3,  2,  0,  0,  0,  0,  0],\n",
              "       [ 1,  4,  6,  0,  0,  0,  0],\n",
              "       [ 1,  4,  6,  0,  0,  0,  0],\n",
              "       [ 1,  4,  2,  0,  0,  0,  0],\n",
              "       [ 7,  7,  3,  2, 10,  1, 11],\n",
              "       [ 1, 12,  3, 13,  0,  0,  0]])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_FjXCrRuFrm"
      },
      "source": [
        "기계는 이제 이들을 하나의 행렬로 보고, 병렬 처리 가능\n",
        "\n",
        "0번 단어는 사실 아무런 의미도 없는 단어이기 때문에 자연어 처리하는 과정에서 기계는 0번 단어를 무시함\n",
        "\n",
        "이와 같이 데이터에 특정 값을 채워서 데이터의 크기(shape)를 조정하는 것 : 패딩(padding)\n",
        "\n",
        "숫자 0을 사용하고 있다면 제로 패딩(zero padding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egKpIppTuJWW"
      },
      "source": [
        "> ## **2. 케라스 전처리 도구로 패딩하기**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3ILrmIzuNN7"
      },
      "source": [
        "케라스에서는 위와 같은 패딩을 위한 도구 pad_sequences() 제공"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Mo50DkruNT2"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_x1MpJWquNkN",
        "outputId": "e4312717-c648-4f90-a364-9b59cef2f2ff"
      },
      "source": [
        "############################################################\n",
        "### 텍스트 시퀀스의 모든 단어들을 각 정수로 맵핑 후 출력 ###\n",
        "############################################################\n",
        "\n",
        "encoded = tokenizer.texts_to_sequences(sentences)\n",
        "print(encoded)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRFXeHBsuVDa",
        "outputId": "e71ea3d5-02ba-4db5-f75a-0f348a87acc3"
      },
      "source": [
        "#######################################\n",
        "### 케라스의 pad_sequences()로 패딩 ###\n",
        "#######################################\n",
        "\n",
        "padded = pad_sequences(encoded) ## pad_sequences는 Numpy로 패딩했을 때와 달리 기본적으로 문서의 앞을 0으로 채움\n",
        "padded"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  0,  0,  0,  0,  1,  5],\n",
              "       [ 0,  0,  0,  0,  1,  8,  5],\n",
              "       [ 0,  0,  0,  0,  1,  3,  5],\n",
              "       [ 0,  0,  0,  0,  0,  9,  2],\n",
              "       [ 0,  0,  0,  2,  4,  3,  2],\n",
              "       [ 0,  0,  0,  0,  0,  3,  2],\n",
              "       [ 0,  0,  0,  0,  1,  4,  6],\n",
              "       [ 0,  0,  0,  0,  1,  4,  6],\n",
              "       [ 0,  0,  0,  0,  1,  4,  2],\n",
              "       [ 7,  7,  3,  2, 10,  1, 11],\n",
              "       [ 0,  0,  0,  1, 12,  3, 13]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaMG9O3TuYn5",
        "outputId": "5196c60f-0bf6-4e4e-eb97-5360db8c0836"
      },
      "source": [
        "padded = pad_sequences(encoded, padding = 'post') ## padding='post' 인자를 이용해 문서의 뒤를 0으로 채움\n",
        "padded"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  5,  0,  0,  0,  0,  0],\n",
              "       [ 1,  8,  5,  0,  0,  0,  0],\n",
              "       [ 1,  3,  5,  0,  0,  0,  0],\n",
              "       [ 9,  2,  0,  0,  0,  0,  0],\n",
              "       [ 2,  4,  3,  2,  0,  0,  0],\n",
              "       [ 3,  2,  0,  0,  0,  0,  0],\n",
              "       [ 1,  4,  6,  0,  0,  0,  0],\n",
              "       [ 1,  4,  6,  0,  0,  0,  0],\n",
              "       [ 1,  4,  2,  0,  0,  0,  0],\n",
              "       [ 7,  7,  3,  2, 10,  1, 11],\n",
              "       [ 1, 12,  3, 13,  0,  0,  0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_Gm_Z9vudUW"
      },
      "source": [
        "Numpy를 이용하여 패딩을 했을 때와 결과가 동일합니다. 실제로 결과가 동일한지 두 결과를 비교합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDm11IjYudZG",
        "outputId": "93aa4705-c04f-4047-9d7c-585d22702548"
      },
      "source": [
        "(padded == padded_np).all()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMgGDnveuhId"
      },
      "source": [
        "지금까지는 가장 긴 길이를 가진 문서의 길이를 기준으로 패딩을 한다고 가정하였지만, 실제로는 꼭 가장 긴 문서의 길이를 기준으로 해야하는 것은 아님\n",
        "\n",
        "가령, 모든 문서의 평균 길이가 20인데 문서 1개의 길이가 5,000이라고 해서 굳이 모든 문서의 길이를 5,000으로 패딩할 필요는 없을 수 있음\n",
        "\n",
        "이와 같은 경우에는 길이에 제한을 두고 패딩할 수 있음\n",
        "\n",
        "max_len의 인자로 정수를 주면, 해당 정수로 모든 문서의 길이를 동일하게 맞춤"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFaj60g2uhPY",
        "outputId": "253f9423-240e-4796-fc48-0a8273cdadb4"
      },
      "source": [
        "#######################################\n",
        "### 모든 문장의 길이를 5로 패딩하기 ###\n",
        "#######################################\n",
        "\n",
        "padded = pad_sequences(encoded, padding = 'post', maxlen = 5)\n",
        "# truncating= 'post')\n",
        "padded ## 길이가 5보다 짧은 문서들은 0으로 패딩되고, 기존에 5보다 길었다면 데이터가 손실됨"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  5,  0,  0,  0],\n",
              "       [ 1,  8,  5,  0,  0],\n",
              "       [ 1,  3,  5,  0,  0],\n",
              "       [ 9,  2,  0,  0,  0],\n",
              "       [ 2,  4,  3,  2,  0],\n",
              "       [ 3,  2,  0,  0,  0],\n",
              "       [ 1,  4,  6,  0,  0],\n",
              "       [ 1,  4,  6,  0,  0],\n",
              "       [ 1,  4,  2,  0,  0],\n",
              "       [ 3,  2, 10,  1, 11],\n",
              "       [ 1, 12,  3, 13,  0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0H8Noi-6ulrY",
        "outputId": "1248a431-6bb7-4d97-8ff7-fc2032d10af6"
      },
      "source": [
        "#####################################\n",
        "### 0이 아닌 다른 숫자로 패딩하기 ###\n",
        "#####################################\n",
        "\n",
        "last_value = len(tokenizer.word_index) + 1 ## 단어 집합의 크기보다 1 큰 숫자를 사용 (현재 사용된 정수들과 겹치지 않도록)\n",
        "\n",
        "padded = pad_sequences(encoded, padding = 'post', value = last_value) ## pad_sequences의 인자로 value를 사용하여 0이 아닌 다른 숫자로 패딩 가능\n",
        "padded"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  5, 14, 14, 14, 14, 14],\n",
              "       [ 1,  8,  5, 14, 14, 14, 14],\n",
              "       [ 1,  3,  5, 14, 14, 14, 14],\n",
              "       [ 9,  2, 14, 14, 14, 14, 14],\n",
              "       [ 2,  4,  3,  2, 14, 14, 14],\n",
              "       [ 3,  2, 14, 14, 14, 14, 14],\n",
              "       [ 1,  4,  6, 14, 14, 14, 14],\n",
              "       [ 1,  4,  6, 14, 14, 14, 14],\n",
              "       [ 1,  4,  2, 14, 14, 14, 14],\n",
              "       [ 7,  7,  3,  2, 10,  1, 11],\n",
              "       [ 1, 12,  3, 13, 14, 14, 14]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soquJzfRjpx0"
      },
      "source": [
        "# **3. 원-핫 인코딩(One-Hot Encoding)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vz2NzPUCutMb"
      },
      "source": [
        "원-핫 인코딩 : 단어 집합의 크기를 벡터의 차원으로 하고, 표현하고 싶은 단어의 인덱스에 1의 값을 부여하고, 다른 인덱스에는 0을 부여하는 단어의 벡터 표현 방식\n",
        "\n",
        "- 이렇게 표현된 벡터 -> 원-핫 벡터(One-Hot vector)\n",
        "\n",
        "- 머신 러닝, 딥 러닝 등에서 사용"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ay3PcVlwuw5D"
      },
      "source": [
        "> ## **1. 원-핫 인코딩(One-Hot Encoding)이란?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCSRxB_Tu0Jj"
      },
      "source": [
        "원-핫 인코딩을 두 가지 과정으로 정리\n",
        "\n",
        "(1) 각 단어에 고유한 인덱스를 부여 (정수 인코딩)\n",
        "\n",
        "(2) 표현하고 싶은 단어의 인덱스의 위치에 1을 부여, 다른 단어의 인덱스의 위치에는 0을 부여\n",
        "\n",
        "---\n",
        "\n",
        "*문장 : 나는 자연어 처리를 배운다*\n",
        "\n",
        "위 문장에 대해서 원-핫 인코딩을 진행하는 코드는 아래와 같습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nq0xScrJv6NM",
        "outputId": "8256ff10-f4cb-449a-87d8-d5c6b0eaed39"
      },
      "source": [
        "pip install konlpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 1.3 MB/s \n",
            "\u001b[?25hCollecting beautifulsoup4==4.6.0\n",
            "  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 69.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.5.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: JPype1, colorama, beautifulsoup4, konlpy\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZWsbWbtutxM",
        "outputId": "3eed3caf-7986-49b6-d0cb-eda612e99218"
      },
      "source": [
        "########################\n",
        "### 문장 토큰화 수행 ###\n",
        "########################\n",
        "## 코엔엘파이 Okt 형태소 분석기 사용\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "okt=Okt()\n",
        "token=okt.morphs(\"나는 자연어 처리를 배운다\")  \n",
        "print(token)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['나', '는', '자연어', '처리', '를', '배운다']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdZQzkcPvFzN",
        "outputId": "75b6979d-f751-4cad-a78b-05006b121f7a"
      },
      "source": [
        "###############################\n",
        "### 토큰에 고유 인덱스 부여 ###\n",
        "###############################\n",
        "## 본 예제의 문장은 짧아서 단어의 빈도수 고려 X\n",
        "\n",
        "word2index={}\n",
        "for voca in token:\n",
        "     if voca not in word2index.keys():\n",
        "       word2index[voca]=len(word2index)\n",
        "print(word2index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'나': 0, '는': 1, '자연어': 2, '처리': 3, '를': 4, '배운다': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwR55_ckvOd6"
      },
      "source": [
        "#################################\n",
        "### 원-핫 벡터 생성 함수 정의 ###\n",
        "#################################\n",
        "## 토큰을 입력하면 해당 토큰에 대한 원-핫 벡터 생성\n",
        "\n",
        "def one_hot_encoding(word, word2index):\n",
        "       one_hot_vector = [0]*(len(word2index))\n",
        "       index=word2index[word]\n",
        "       one_hot_vector[index]=1\n",
        "       return one_hot_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgdr72iavR4F",
        "outputId": "a49a87b1-be17-4328-b14c-d79ee89f9daa"
      },
      "source": [
        "############################################\n",
        "### 토큰 '자연어'에 대한 원-핫 벡터 반환 ###\n",
        "############################################\n",
        "\n",
        "one_hot_encoding(\"자연어\",word2index) ## 자연어는 단어 집합에서 인덱스가 2이므로, 자연어를 표현하는 원-핫 벡터는 인덱스 2의 값이 1이며, 나머지 값은 0인 벡터가 나옴"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 0, 1, 0, 0, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPWBjqFIvYk4"
      },
      "source": [
        "> ## **2. 케라스(Keras)를 이용한 원-핫 인코딩(One-Hot Encoding)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnVwxaEUvfBb"
      },
      "source": [
        "위에서는 원-핫 인코딩 이해를 위해 파이썬으로 코드를 작성하였지만, 케라스는 원-핫 인코딩을 수행하는 유용한 도구 to_categorical()를 지원함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38cgl_Z-6gVi"
      },
      "source": [
        "text=\"나랑 점심 먹으러 갈래 점심 메뉴는 햄버거 갈래 갈래 햄버거 최고야\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gC62Cq8ivehq",
        "outputId": "a5fcb717-c924-43bc-9c06-c28132b27422"
      },
      "source": [
        "##############################################\n",
        "### fit_on_texts()로 정수 인코딩 작업 수행 ###\n",
        "##############################################\n",
        "## fit_on_texts() : 입력한 텍스트로부터 단어 빈도수가 높은 순으로 낮은 정수 인덱스를 부여\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "t = Tokenizer()\n",
        "t.fit_on_texts([text])\n",
        "print(t.word_index) # 각 단어에 대한 인코딩 결과 출력."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'갈래': 1, '점심': 2, '햄버거': 3, '나랑': 4, '먹으러': 5, '메뉴는': 6, '최고야': 7}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bH_M9dESvg6e",
        "outputId": "2925a54a-63e1-4539-bca3-14a934a1d1db"
      },
      "source": [
        "############################################################\n",
        "### 텍스트 시퀀스의 모든 단어들을 각 정수로 맵핑 후 출력 ###\n",
        "############################################################\n",
        "## 생성된 단어 집합 내의 일부 단어들로만 구성된 서브 텍스트인 sub_text를 정수 시퀀스로 변환\n",
        "\n",
        "sub_text=\"점심 먹으러 갈래 메뉴는 햄버거 최고야\"\n",
        "encoded=t.texts_to_sequences([sub_text])[0]\n",
        "print(encoded)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 5, 1, 6, 3, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8qk7xQDvkSN",
        "outputId": "6dff6da7-cd45-4a9b-f3ef-f2ce7129f5d3"
      },
      "source": [
        "#####################################################\n",
        "### 케라스의 to_categorical()로 원-핫 인코딩 수행 ###\n",
        "#####################################################\n",
        "\n",
        "one_hot = to_categorical(encoded)\n",
        "print(one_hot)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kawzSoVtvplp"
      },
      "source": [
        "> ## **3. 원-핫 인코딩(One-Hot Encoding)의 한계**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0BZWuT4vuNy"
      },
      "source": [
        "#### **한계점**\n",
        "**- 단어의 개수가 늘어날수록 벡터를 저장하기 위해 필요한 공간이 계속 늘어남 (벡터의 차원이 계속 늘어남)**\n",
        "\n",
        "원 핫 벡터는 단어 집합의 크기 = 벡터의 차원 수\n",
        "\n",
        "가령, 단어가 1,000개인 코퍼스를 가지고 원 핫 벡터를 만들면, 모든 단어 각각은 모두 1,000개의 차원을 가진 벡터가 됨\n",
        "\n",
        "다시 말해 모든 단어 각각은 하나의 값만 1을 가지고, 999개의 값은 0의 값을 가지는 벡터가 되는데 이는 저장 공간 측면에서는 매우 비효율적인 표현 방법임\n",
        "\n",
        "**- 원-핫 벡터는 단어의 유사도를 표현하지 못함**\n",
        "예를 들어서 늑대, 호랑이, 강아지, 고양이라는 4개의 단어에 대해서 원-핫 인코딩을 해서 각각, [1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]이라는 원-핫 벡터를 부여받았다고 하면 이 때 원-핫 벡터로는 강아지와 늑대가 유사하고, 호랑이와 고양이가 유사하다는 것을 표현할 수가 없음\n",
        "\n",
        "좀 더 극단적으로는 강아지, 개, 냉장고라는 단어가 있을 때 강아지라는 단어가 개와 냉장고라는 단어 중 어떤 단어와 더 유사한지도 알 수 없음\n",
        "\n",
        "단어 간 유사성을 알 수 없다는 단점은 검색 시스템 등에서 심각한 문제\n",
        "\n",
        "#### **해결방안**\n",
        "이러한 단점을 해결하기 위해 단어의 잠재 의미를 반영하여 다차원 공간에 벡터화 하는 기법으로 크게 두 가지가 있음\n",
        "\n",
        "첫째는 카운트 기반의 벡터화 방법인 LSA, HAL 등이 있으며, 둘째는 예측 기반으로 벡터화하는 NNLM, RNNLM, Word2Vec, FastText 등이 있음\n",
        "\n",
        "카운트 기반과 예측 기반 두 가지 방법을 모두 사용하는 방법으로 GloVe라는 방법도 존재"
      ]
    }
  ]
}