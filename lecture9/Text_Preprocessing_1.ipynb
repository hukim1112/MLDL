{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hukim1112/MLDL/blob/master/lecture9/Text_Preprocessing_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QeybFyhTJXM"
      },
      "source": [
        "# 텍스트 전처리 (Text Preprocessing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3jMJyTGTTAs"
      },
      "source": [
        "# 01) 토큰화(Toknization)\n",
        "\n",
        "자연어 처리에서 크롤링 등 으로 얻어낸 코퍼스 데이터가필요에 맞게 전처리되지 않은 상태라면 해당 데이터를 사용하고자 하는 용도에 맞게 \n",
        "1. 토큰화 (tokenization)\n",
        "2. 정제 (cleaning)\n",
        "3. 정규화 (normalization)\n",
        "\n",
        "하는 일을 하게 된다.\n",
        "\n",
        "토큰화\n",
        ": 주어진 코퍼스(corpus)에서 토큰(token)이라 불리는 단위로 나누는 작업을 토큰화(tokenization)라고 부른다. 토큰의 단위는 상황에 따라 다르지만, 보통 의미있는 단위로 토큰을 정의한다.\n",
        "\n",
        "[wikidocs](https://wikidocs.net/21698)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8HcvPqaULAt"
      },
      "source": [
        "## 1. 단어 토큰화 (Word Toknization)\n",
        "토큰의 기준을 단어(word)로 하는 경우, 단어 토큰화(word tokenization)라고 합니다. 다만, 여기서 단어(word)는 단어 단위 외에도 단어구, 의미를 갖는 문자열로도 간주되기도 합니다.\n",
        "\n",
        "예를 들어, 아래의 입력으로부터 구두점(punctuation)과 같은 문자는 제외시키는 간단한 단어 토큰화 작업을 해봅시다.\n",
        " - 구두점 : 마침표(.), 쉼표(,), 물음표(?), 세미콜론(;), 느낌표(!) 등 과 같은 기호\n",
        "\n",
        "입력 : **Time is an illusion. Lunchtime double so!**\n",
        "\n",
        "출력 : \"Time\", \"is\", \"an\", \"illustion\", \"Lunchtime\", \"double\", \"so\"\n",
        "\n",
        "이 기초적인 예제에서는 구두점을 지운 뒤에 띄어쓰기(whitespace)를 기준으로 잘라냄으로 토큰화를 진행하였습니다.\n",
        "\n",
        "보통 토큰화 작업은 단순히 구두점이나 특수문자를 전부 제거하는 정제(cleaning) 작업을 수행하는 것만으로 해결되지 않습니다. 구두점이나 특수문자를 전부 제거하면 토큰이 의미를 잃어버리는 경우가 발생하기도 합니다. 심지어 띄어쓰기 단위로 자르면 사실상 단어 토큰이 구분되는 영어와 달리, 한국어는 띄어쓰기만으로는 단어 토큰을 구분하기 어렵습니다. 그 이유는 뒤에서 언급하도록 하겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmFBqMCcWav2"
      },
      "source": [
        "## 토큰화 중 생기는 선택의 순간\n",
        "토큰화를 하다보면, 토큰화의 기준을 생각해봐야 하는 경우가 발생한다. 이러한 선택은 해당 데이터를 가지고 어떤 용도로 사용할 것인지에 따라, 그 용도에 영향이 없는 기준으로 정하면 된다.\n",
        "\n",
        "예:\n",
        "**Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.**\n",
        "- Don't, Jone's를 토큰화 하는 방법?\n",
        "  - Don't\n",
        "  - Don t\n",
        "  - Dont\n",
        "  - Do n't\n",
        "  - Jone's\n",
        "  - Jone s\n",
        "  - Jone\n",
        "  - Jones\n",
        "\n",
        "원하는 결과가 나오도록 토큰화 도구를 직접 설계할 수도 있지만, 토큰화를 도와주는 모듈이 존재한다. \n",
        "\n",
        "NLTK는 영어 corpus를 토큰화 하기 위한 도구를 제공한다. 그 중 word_tokenize와 WordPunctTokenizer를 사용하여 (')를 어떻게 처리하는지 확인해보자\n",
        " - NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOzNAgWFZSEq",
        "outputId": "b4fad40c-0053-43d9-bb5a-0eac8df47bb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OC5NhctFdF37",
        "outputId": "78c2d74e-0c60-4ebf-bc4f-8ce76ce5b411"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize  \n",
        "print(word_tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\")) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eC9dsoFbZymE"
      },
      "source": [
        "word_tokenize는 Don't를 Do와 n't로 분리하였으며, 반면 Jone's는 Jone과 's로 분리한 것을 확인할 수 있습니다.\n",
        "\n",
        "\n",
        "- wordPunctTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7E1U8nPatFl",
        "outputId": "baa01e3b-b286-4265-8eac-9c3ee423d0be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import WordPunctTokenizer  \n",
        "print(WordPunctTokenizer().tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkWxcfUNaxl6"
      },
      "source": [
        "WordPunctTokenizer는 구두점을 별도로 분류하는 특징을 갖고 있기때문에, 앞서 확인했던 word_tokenize와는 달리 Don't를 Don과 '와 t로 분리하였으며, 이와 마찬가지로 Jone's를 Jone과 '와 s로 분리한 것을 확인할 수 있습니다.\n",
        "\n",
        " - 케라스 - text_to_word_sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mpjOfcAay02",
        "outputId": "81a58a75-b249-42c2-f22d-e597d74171ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', \"jone's\", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "print(text_to_word_sequence(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvO7v2z4a2NX"
      },
      "source": [
        "케라스의 text_to_word_sequence는 기본적으로 모든 알파벳을 소문자로 바꾸면서 마침표나 컴마, 느낌표 등의 구두점을 제거합니다. 하지만 don't나 jone's와 같은 경우 아포스트로피는 보존하는 것을 볼 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8Kry2ZdckUf"
      },
      "source": [
        "## 3. 토큰화에서 고려해야할 사항\n",
        "토큰화를 단순히 구두점을 제외하고 공백으로 잘라내는 작업이라고 간주할 수는 없다.\n",
        "\n",
        "1. 구두점이나 특수문자를 단순 제외해서는 안됨\n",
        "\n",
        "  코퍼스에 대한 정제 작업을 진행하다보면, 구두점도 하나의 토큰으로 분류하기도 한다. \n",
        "\n",
        "  - 마침표(.)의 경우 문장의 경계를 알 수 있음\n",
        "  - 단어 자체의 구두점 \n",
        "    - e.g. $45.55, 2021.09.13 1,000,000\n",
        "2. 줄임말과 단어 내에 띄어쓰기가 있는 경우\n",
        "  \n",
        "  - 토큰화 작업에서 종종 영어권의 (')는 압축된 단어를 펼치는 역할을 하기도 한다. e.g. we're - we are, I'm - I am\n",
        "  - New York, rock 'n' roll 하나의 단어이지만 중간에 띄어쓰기가 존재. 이런 단어를 하나의 단어로 인식할 수 있는 능력이 필요하다.\n",
        "\n",
        "3. 표준 토큰화 예제\n",
        "표준으로 쓰이고 있는 토큰화 방법 중 하나인 **Penn Treebank Tokenization의 규칙**에 대해 소개하고 결과를 보도록 하겠습니다.\n",
        "\n",
        "  규칙 1. 하이픈(-)으로 구성된 단어는 하나로 유지\n",
        "  \n",
        "  규칙 2. doesn't와 같이 (')로 줄여쓰는 단어는 분리\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9wlRfwQa4q9",
        "outputId": "0f3cf8c1-866b-46eb-c02b-a0d879f3307c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "tokenizer=TreebankWordTokenizer()\n",
        "text=\"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\"\n",
        "print(tokenizer.tokenize(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hi0OQ-ieh_w"
      },
      "source": [
        "각각 규칙1과 규칙2에 따라서 home-based는 하나의 토큰으로 취급하고 있으며, dosen't의 경우 does와 n't는 분리되었음을 볼 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGW2RLaCenBM"
      },
      "source": [
        "## 4. 문장 토큰화 (Sentence Tokenization) - 문장분류\n",
        "토큰의 단위가 문장(sentence)일 떄, 어떻게 토큰화를 수행해야할지 논의합니다.\n",
        "\n",
        "문장 단위로 분류하는 직관적인 방법은 ?, ., ! 로 문장을 구분하는 것이다. \n",
        "하지만 !와 ?는 꽤 명확한 구분자(boundary)의 역할을 하지만 마침표는 그렇지 않다.\n",
        "\n",
        "EX1) IP 192.168.56.31 서버에 들어가서 로그 파일 저장해서 abc123@gmail.com로 결과 좀 보내줘. 그러고나서 점심 먹으러 가자."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTLbdZhoegjt",
        "outputId": "340480b9-b147-47e8-90f7-905546bd9fd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to make sure no one was near.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "text=\"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near.\"\n",
        "print(sent_tokenize(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xIVhNa1fwwf"
      },
      "source": [
        "마침표가 여러번 등장하는 경우"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_O8meurftgi",
        "outputId": "1553b0cb-151e-4653-a0fc-1d545dc1b2fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "text=\"I am actively looking for Ph.D. students. and you are a Ph.D student.\"\n",
        "print(sent_tokenize(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXstvrFCf3Yt"
      },
      "source": [
        "NLTK는 단순히 마침표를 구분자로 하여 문장을 구분하지 않았기 때문에, Ph.D.를 문장 내의 단어로 인식하여 성공적으로 인식하는 것을 볼 수 있습니다.\n",
        "\n",
        "한국어 문장 토큰화 도구 또한 존재한다. \n",
        " - KSS(Korean Sentence Splitter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-TcR1Nmfzr8",
        "outputId": "2533f592-a292-445d-b281-db8ed19eaa22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kss\n",
            "  Downloading kss-3.3.1.1.tar.gz (42.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 42.4 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting emoji\n",
            "  Downloading emoji-1.6.1.tar.gz (170 kB)\n",
            "\u001b[K     |████████████████████████████████| 170 kB 50.7 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: kss, emoji\n",
            "  Building wheel for kss (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kss: filename=kss-3.3.1.1-py3-none-any.whl size=42449239 sha256=919e5fafa60d237a50de9b9207989baf0b9ebef7044a4ea8e32d12783eeecd2e\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/9d/1d/52871154eff5273abb86b96f4f984c1cd67c5bde64239b060a\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.6.1-py3-none-any.whl size=169314 sha256=7b9f1e8ded33d872698708123cb225aa393585ff5984d567f9dd0f3ff3e2c81f\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/5f/d3/03d313ddb3c2a1a427bb4690f1621eea60fe6f2a30cc95940f\n",
            "Successfully built kss emoji\n",
            "Installing collected packages: emoji, kss\n",
            "Successfully installed emoji-1.6.1 kss-3.3.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install kss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJfDuDekf_CP",
        "outputId": "f447fa05-befd-4483-9eb5-cf13ac1d043f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Korean Sentence Splitter]: Initializing Pynori...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['딥 러닝 자연어 처리가 재미있기는 합니다.', '그런데 문제는 영어보다 한국어로 할 때 너무 어려워요. 농담아니에요.', '이제 해보면 알걸요?']\n"
          ]
        }
      ],
      "source": [
        "import kss\n",
        "\n",
        "text='딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어려워요. 농담아니에요. 이제 해보면 알걸요?'\n",
        "print(kss.split_sentences(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qS1tSSlXgCld",
        "outputId": "bc6685cc-5c62-430c-85f1-e12c5f3598f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['오늘은 2021.09.13 이에요.', '날씨는 맑아요!']\n"
          ]
        }
      ],
      "source": [
        "text='오늘은 2021.09.13 이에요. 날씨는 맑아요!'\n",
        "print(kss.split_sentences(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHk6cQvAgPGA"
      },
      "source": [
        "## 5. 이진 분류기(Binary Classifier)\n",
        "\n",
        "문장 토큰화에서 예외를 발생시키는 마침표의 처리를 위해 입력에 따라 두개의 클래스로 분류하는 이진 분류기(binary classifier)를 사용하기도 합니다.\n",
        "\n",
        "1. 마침표(.)가 단어의 일부분일 경우. 즉, 마침표가 약어(abbreivation)로 쓰이는 경우\n",
        "2. 마침표(.)가 정말로 문장의 구분자(boundary)일 경우\n",
        "\n",
        "이진 분류기는 앞서 언급했듯이, 임의로 정한 여러가지 규칙을 코딩한 함수일 수도 있으며, 머신 러닝을 통해 이진 분류기를 구현하기도 합니다.\n",
        "\n",
        "마침표(.)가 어떤 클래스에 속하는지 결정을 위해서는 어떤 마침표가 주로 약어(abbreviation)으로 쓰이는 지 알아야합니다. \n",
        " - [약어 사전(abbreviation dictionary)](https://public.oed.com/how-to-use-the-oed/abbreviations/)\n",
        "\n",
        "\n",
        "이러한 문장 토큰화를 수행하는 오픈 소스로는 NLTK, OpenNLP, 스탠포드 CoreNLP, splitta, LingPipe 등이 있습니다.\n",
        " \n",
        "문장 토큰화 규칙을 짤 때, 발생할 수 있는 여러가지 예외사항을 다룬 [참고 자료](https://www.grammarly.com/blog/engineering/how-to-split-sentences/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OF9L3otZhYcO"
      },
      "source": [
        "## 6. 한국어 토큰화의 어려움\n",
        "영어는 New York과 같은 합성어나, he's와 같은 줄임말에 대한 예외처리만 한다면 띄어쓰기(white space)를 기준으로 하는 띄어쓰기 토큰화를 해도 단어 토큰화가 잘 작동한다.\n",
        " - 대부분의 경우에서 단어단위로 띄어쓰기가 이루어짐\n",
        "\n",
        "하지만 한국어는 띄어쓰기만으로 토큰화를 하기가 부족하다. 띄어쓰기의 단위를 '어절'이라고 하는데 어절토큰화가 단어토큰화와 같지 않기 때문에 지양된다.\n",
        "\n",
        "1. 한국어는 교착어이다.\n",
        "  - 교착어 : 조사, 어미 등을 붙여서 말을 만듦\n",
        "\n",
        "  그라는 단어 하나에도 '그가', '그에게', '그를', '그와', '그는'과 같이 다양한 조사가 '그'라는 글자 뒤에 띄어쓰기 없이 바로 붙게됩니다. \n",
        "\n",
        "  같은 단어임에도 다른 조사가 붙어 다른 단어로 인식되면 번거로워지는 경우가 많다. -> 한국어 NLP에서 조사를 분리해야 한다.\n",
        "\n",
        " =>  **형태소(morpheme, 뜻을 가진 가장 작은 말의 단위)** 토큰화를 수행하여야 한다.\n",
        "\n",
        " - 문장 : 에디가 딥러닝책을 읽었다\n",
        "\n",
        "    - 자립 형태소 : 에디, 딥러닝책\n",
        "    - 의존 형태소 : -가, -을, 읽-, -었, -다\n",
        "\n",
        "\n",
        "\n",
        "2. 한국어는 띄어쓰기가 영어보다 잘 지켜지지 않는다.\n",
        "\n",
        " EX1) 제가이렇게띄어쓰기를전혀하지않고글을썼다고하더라도글을이해할수있습니다.\n",
        "\n",
        " EX2) Tobeornottobethatisthequestion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5LmWJytjWru"
      },
      "source": [
        "## 7. 품사 태깅(Part-of-speech tagging)\n",
        "단어의 표기는 같지만, 품사에 따라 의미가 달라진다.\n",
        "\n",
        "- fly\n",
        "  - v : 날다\n",
        "  - n : 파리\n",
        "\n",
        "- 못\n",
        "  - 동작 동사(먹다, 달리다 ..) 를 할 수 없다는 의미\n",
        "  - 목재 따위를 고정하는 물건\n",
        "\n",
        "즉, 단어의 의미를 제대로 파악하기 위해 품사를 파악하는 것이 지표가 될 수 있다. 이에 따라 단어 토큰화 과정에서 각 단어의 품사를 구분하기도 하는데 이러한 작업을 품사 태깅(part-of-speech tagging)이라고 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0j9irGCtkHyA"
      },
      "source": [
        "## 8. NLTK와 KoNLPy를 이용한 영어, 한국어 토큰화 실습\n",
        " - NLTK \n",
        "   - 영어 코퍼스에 품사 태깅 - Penn Treebank POG Tags 기준 사용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIG_0886gL-1",
        "outputId": "282f5915-f682-4c82-f36d-fae2a267731e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "text=\"I am actively looking for Ph.D. students. and you are a Ph.D. student.\"\n",
        "print(word_tokenize(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IITfEnekV8f",
        "outputId": "81ff5e97-fb6c-4e88-fe0f-cac6352b06e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', 'PRP'),\n",
              " ('am', 'VBP'),\n",
              " ('actively', 'RB'),\n",
              " ('looking', 'VBG'),\n",
              " ('for', 'IN'),\n",
              " ('Ph.D.', 'NNP'),\n",
              " ('students', 'NNS'),\n",
              " ('.', '.'),\n",
              " ('and', 'CC'),\n",
              " ('you', 'PRP'),\n",
              " ('are', 'VBP'),\n",
              " ('a', 'DT'),\n",
              " ('Ph.D.', 'NNP'),\n",
              " ('student', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.tag import pos_tag\n",
        "x=word_tokenize(text)\n",
        "pos_tag(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9-u5KdGki-b"
      },
      "source": [
        "PRP는 인칭 대명사, VBP는 동사, RB는 부사, VBG는 현재부사, IN은 전치사, NNP는 고유 명사, NNS는 복수형 명사, CC는 접속사, DT는 관사를 의미"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QG901MTYkmRZ"
      },
      "source": [
        "- KoNLPy : 한국어 자연어 처리기\n",
        "  - 형태소 분석기 : Okt(Open Korea Text), 메캅(Mecab), 코모란(Komoran), 한나눔(Hannanum), 꼬꼬마(Kkma)\n",
        "  - 설치 [참고자료](https://mr-doosun.tistory.com/21)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57WFuhN-k3HZ",
        "outputId": "8a203f4f-b404-4c30-b7cc-72f69b33e47e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 49.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 61.7 MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 4.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.10.0.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.10.8)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: JPype1, colorama, beautifulsoup4, konlpy\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"
          ]
        }
      ],
      "source": [
        "!pip install konlpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_I4SFoIkXfy",
        "outputId": "e94147a1-f996-4356-fef9-9433ccd6b8f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']\n"
          ]
        }
      ],
      "source": [
        "from konlpy.tag import Okt  \n",
        "okt=Okt()  \n",
        "print(okt.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIM5HkJGk1eg",
        "outputId": "bd0a4b42-bb4b-45ad-fdf7-4213c13f833c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('열심히', 'Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa'), ('여행', 'Noun'), ('을', 'Josa'), ('가봐요', 'Verb')]\n"
          ]
        }
      ],
      "source": [
        "print(okt.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4i_-Keak2C-",
        "outputId": "370cead8-7854-4420-9776-fc835318a9f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['코딩', '당신', '연휴', '여행']\n"
          ]
        }
      ],
      "source": [
        "print(okt.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6bzzPyWlTJO"
      },
      "source": [
        "1. morphs : 형태소 추출\n",
        "2. pos : 품사 태깅(Part-of-speech tagging)\n",
        "3. nouns : 명사 추출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQLemlwElRIT",
        "outputId": "0b1579bf-5e09-4f37-9eb3-8ba81ab0dbaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가보', '아요']\n"
          ]
        }
      ],
      "source": [
        "from konlpy.tag import Kkma  \n",
        "kkma=Kkma()  \n",
        "print(kkma.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJvorCgjlYgD",
        "outputId": "89387222-408d-43ac-b36e-c785651c9510"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('열심히', 'MAG'), ('코딩', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('당신', 'NP'), (',', 'SP'), ('연휴', 'NNG'), ('에', 'JKM'), ('는', 'JX'), ('여행', 'NNG'), ('을', 'JKO'), ('가보', 'VV'), ('아요', 'EFN')]\n"
          ]
        }
      ],
      "source": [
        "print(kkma.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-d9C9wl0lZq6",
        "outputId": "2bbfb002-1785-4119-e499-408672b51139"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['코딩', '당신', '연휴', '여행']\n"
          ]
        }
      ],
      "source": [
        "print(kkma.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KP3H6XtTletV"
      },
      "source": [
        "각 형태소 분석기는 성능과 결과가 다르게 나오기 때문에, 형태소 분석기의 선택은 사용하고자 하는 필요 용도에 어떤 형태소 분석기가 가장 적절한지를 판단하고 사용하면 됩니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i31iXqqslg_g"
      },
      "source": [
        "한국어 형태소 분석기 성능 비교 : https://passerby14.tistory.com/3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "CbDUUk7BlakS"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRhkQ2Jel2uS"
      },
      "source": [
        "# 02) 정제(Cleaning) and 정규화(Normalization)\n",
        "코퍼스에서 용도에 맞게 토큰을 분류하는 작업을 토큰화(tokenization)라고 하며, 토큰화 작업 전, 후에는 텍스트 데이터를 용도에 맞게 정제(cleaning) 및 정규화(normalization)하는 일이 함께한다. \n",
        "\n",
        "목적\n",
        "- 정제(cleaning) : 가지고 있는 코퍼스로부터 노이즈를 제거한다.\n",
        "- 정규화(normalization) : 표현 방법이 다른 단어들을 통합시켜 같은 단어로 만들어준다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A407td18-qFH"
      },
      "source": [
        "## 1. 규칙에 기반한 표기가 다른 단어들의 통합\n",
        "\n",
        " - US vs USA\n",
        "\n",
        "같은 의미를 가지므로 하나의 단어로 정규화 할 수 있다.\n",
        "=> 표기가 다른 단어들을 통합하는 방법인 **어간 추출(stemming)**과 **표제어 추출(lemmatizaiton)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n09x-yaD_EZy"
      },
      "source": [
        "## 2. 대, 소문자 통합\n",
        "영어권 언어에서 대, 소문자를 통합하는 것은 단어의 개수를 줄이는 정규화 방법. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xII_BcnR_Ye0"
      },
      "source": [
        "## 3. 불필요한 단어의 제거(Removing Unnecessary Words)\n",
        "\n",
        "정제 작업에서 제거해야하는 노이즈 데이터(noise data)\n",
        "- 자연어가 아니면서 아무 의미도 갖지 않는 글자들(특수 문자 등)\n",
        "- 분석하고자 하는 목적에 맞지 않는 불필요 단어\n",
        "\n",
        "불필요 단어들을 제거하는 방법\n",
        " - 불용어 제거 (다음 챕터)\n",
        " - 등장빈도가 적은 단어, 길이가 짧은 단어 제거\n",
        "\n",
        "####  (1) 등장 빈도가 적은 단어(Removing Rare words)\n",
        "e.g. 메일이 정상, 스팸을 판별하기 위해 100,000개의 메일 중 5번 밖에 등장하지 않은 단어는 분류에 도움 X\n",
        "\n",
        "#### (2) 길이가 짧은 단어(Removing words with a very short length)\n",
        "영어권에서는 길이가 짧은 단어를 삭제하는 것 만으로도 어느정도 자연어 처리에서 크게 의미가 없는 단어들을 제거하는 효과가 있다.\n",
        "\n",
        "- en : d r a g o n\n",
        "- ko : 용(龍)\n",
        "\n",
        "하지만 한국어는 한자어가 많고, 한 글자 만으로도 의미를 가진 경우가 많다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOLLub-2-pa1",
        "outputId": "99497617-2f93-459e-bceb-bcef489ed64b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " was wondering anyone out there could enlighten this car.\n"
          ]
        }
      ],
      "source": [
        "# 길이가 1~2인 단어들을 정규 표현식을 이용하여 삭제\n",
        "import re\n",
        "text = \"I was wondering if anyone out there could enlighten me on this car.\"\n",
        "shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
        "print(shortword.sub('', text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UDic0BYA-Mj"
      },
      "source": [
        "## 4. 정규 표현식(Regular Expression)\n",
        "얻어낸 코퍼스에서 노이즈 데이터의 특징을 잡아낼 수 있다면, 정규표현식을 통해서 이를 제거할 수 있는 경우가 많다.\n",
        "- e.g. HTML문서 -> HTML태그가 많이 존재"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPeuIu5gBPSy"
      },
      "source": [
        "# 03) 어간 추출(Stemming) and 표제어 추출(Lemmatization)\n",
        "\n",
        "정규화 기법 중 코퍼스에 있는 단어 개수를 줄일 수 있는 기법\n",
        "\n",
        "## 표제어 추출 (Lemmatization)\n",
        "표제어(Lemma)는 기본 사전형 단어 정도의 의미를 갖는다.\n",
        " - is, am, are은 서로 다른 스펠링이지만 표제어는 be\n",
        "\n",
        "표제어를 추출하는 가장 섬세한 방법은 단어 형태학적 파싱을 먼저 진행하는 것\n",
        "\n",
        "형태소: 의미를 가진 가장 작은 단위\n",
        "\n",
        "형태학(morphology) : 형태소로부터 단어들을 만들어가는 학문\n",
        "\n",
        "1. 어간(stem):  단어의 의미를 담고있는 핵심부분\n",
        "2. 접사(affix) : 단어에 추가적인 의미를 주는 부분\n",
        "\n",
        "형태학적 파싱이란 이를 분리하는 작업\n",
        "- e.g. cats : cat(stem) + -s(affix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzn8zZ9cDyDm",
        "outputId": "1e5f1c01-4c57-455a-9dff-f9dcd39136f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUeUTUTtA32-",
        "outputId": "481d11d7-38b6-4f14-da5a-1df2f1dbb0d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "n=WordNetLemmatizer()\n",
        "words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "print([n.lemmatize(w) for w in words])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXSnNtwUEEdT"
      },
      "source": [
        "후에 어간 추출(stemming)에 대해서 배우고, 같은 입력에 대한 결과를 비교해보시면 알겠지만 표제어 추출은 어간 추출과는 달리 **단어의 형태가 적절히 보존되는 양상**을 보이는 특징이 있습니다. \n",
        "\n",
        "하지만 그럼에도 위의 결과에서는 dy나 ha와 같이 의미를 알 수 없는 적절하지 못한 단어를 출력하고 있습니다. \n",
        "\n",
        "이는 표제어 추출기(lemmatizer)가 본래 단어의 품사 정보를 알아야만 정확한 결과를 얻을 수 있기 때문입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YMU_IljpDrLk",
        "outputId": "d9ceb19d-b10a-42a1-bcb7-26ffccae9301"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'die'"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "n.lemmatize('dies', 'v')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "rWj5kdTgEUlV",
        "outputId": "e88626a5-75af-445f-cecc-2da367160067"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'watch'"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "n.lemmatize('watched', 'v')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3pbLTU8VEVZ9",
        "outputId": "bd4a1162-b29f-4ba5-c23a-7ce5395da356"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'have'"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "n.lemmatize('has', 'v')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n.lemmatize('going', 'v')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zqIDorZCBzmj",
        "outputId": "9afac472-28d1-4b4d-8ed3-6e328d6c1f25"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'go'"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4j88rfOEi1D"
      },
      "source": [
        "#### 표제어추출 vs 어간추출\n",
        "표제어 추출은 문맥을 고려하며, 수행했을 때의 결과는 해당 단어의 품사 정보를 보존합니다. (POS 태그를 보존)\n",
        "\n",
        "하지만, 어간 추출을 수행한 결과는 품사 정보가 보존되지 않습니다. (다시 말해 POS 태그를 고려X) 더 정확히는, 어간 추출을 한 결과는 사전에 존재하지 않는 단어일 경우가 많습니다.\n",
        "\n",
        "## 2. 어간 추출(Stemming)\n",
        "어간(Stem)을 추출하는 작업을 어간 추출(Stemming)이라고 한다.\n",
        "결과 단어는 사전에 존재하지 않는 단어일 수 있다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irvkvAGGEhOV",
        "outputId": "609d5873-965b-4cac-884a-6126be2ef968"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "s = PorterStemmer()\n",
        "text=\"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n",
        "words=word_tokenize(text)\n",
        "print(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pLEwzjZFFtF",
        "outputId": "d4653cfe-ba8d-4a85-ac40-4cce43914874"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"
          ]
        }
      ],
      "source": [
        "print([s.stem(w) for w in words])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Za7mT6TjFZFw"
      },
      "source": [
        "위의 어간 추출은 단순 규칙에 기반하여 이루어지기 떄문에 사전에 없는 단어를 포함한다.\n",
        "\n",
        "예를들어, 포터 알고리즘의 어간 추출은 다음의 규칙을 가진다.\n",
        " - ALIZE → AL\n",
        " - ANCE → 제거\n",
        " - ICAL → IC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKy2RrZXFPY4",
        "outputId": "86e451a0-a445-4bc2-fd77-dab21df9797b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['formal', 'allow', 'electric']\n"
          ]
        }
      ],
      "source": [
        "words=['formalize', 'allowance', 'electricical']\n",
        "print([s.stem(w) for w in words])\n",
        "# formalize → formal\n",
        "# allowance → allow\n",
        "# electricical → electric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXkg8REPF41d"
      },
      "source": [
        "포터 어간 추출 vs 랭커스터 어간 추출\n",
        "#### Stemming\n",
        " - am → am\n",
        " - the going → the go\n",
        " - having → hav\n",
        "\n",
        "#### Lemmatization\n",
        " - am → be\n",
        " - the going → the going\n",
        " - having → have"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GejozIZ3Fsm-",
        "outputId": "e71890f6-8af3-4cd1-8485-034cd088e55e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "s=PorterStemmer()\n",
        "words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "print([s.stem(w) for w in words])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtGBpvnuF_OV",
        "outputId": "f3074a5c-a787-4aa3-c54e-ede6c6618416"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "l=LancasterStemmer()\n",
        "words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "print([l.stem(w) for w in words])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5AeT9rRGGDC"
      },
      "source": [
        "## 3. 한국어에서의 어간 추출\n",
        "아래 사진과 같이 한국어는 5언 9품사의 구조를 지닌다.\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "이 중 용언의 '동사'와 '형용사'는 어간과 어미의 결합으로 구성된다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYeLtEJGGwBg"
      },
      "source": [
        "####  (1) 활용(conjugation)\n",
        "활용이란 용언의 어간이 어미를 가지는 일\n",
        "- 어간(stem) : 용언(동사, 형용사)을 활용할 때, 원칙적으로 모양이 변하지 않는 부분. 활용에서 어미에 선행하는 부분. 때론 어간의 모양도 바뀔 수 있음(예: 긋다, 긋고, 그어서, 그어라).\n",
        "\n",
        "- 어미(ending): 용언의 어간 뒤에 붙어서 활용하면서 변하는 부분이며, 여러 문법적 기능을 수행\n",
        "\n",
        "1. 규칙 활용 : 어간의 모습이 일정\n",
        "2. 불규칙 활용 : 어간이나 어미의 모습이 변함\n",
        "\n",
        "#### (2) 규칙 활용\n",
        "규칙활용: 어간이 어미를 취할 때, 어간의 모습이 일정\n",
        "\n",
        "``` 잡/어간 + 다/어미```\n",
        "\n",
        "어간이 어미가 붙기전의 모습과 어미가 붙은 후의 모습이 같으므로, 규칙 기반으로 어미를 단순히 분리해주면 어간 추출이 된다.\n",
        "\n",
        "#### (3) 불규칙 활용\n",
        "불규칙 활용: 어간이 어미를 취할 때 어간의 모습이 바뀌거나 취하는 어미가 특수한 어미일 경우\n",
        "\n",
        "- 어간의 형식이 달라지는 경우\n",
        "   - ‘듣-, 돕-, 곱-, 잇-, 오르-, 노랗-’ -> ‘듣/들-, 돕/도우-, 곱/고우-, 잇/이-, 올/올-, 노랗/노라-’\n",
        "- 특수한 어미를 취하는 경우\n",
        "   -  ‘오르+ 아/어→올라, 하+아/어→하여, 이르+아/어→이르러, 푸르+아/어→푸르러’\n",
        "\n",
        "\n",
        "다양한 불규칙 활용의 예를 보여줍니다.\n",
        "\n",
        "링크 : https://namu.wiki/w/한국어/불규칙%20활용\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "SbCHXqSvF_b3"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9DpZuZ0H-ut"
      },
      "source": [
        "# 04) 불용어(Stopword)\n",
        "\n",
        "가지고 있는 데이터에서 유의미한 단어 토큰만을 선별하기 위해서는 큰 의미가 없는 단어 토큰을 제거하는 작업이 필요하다.\n",
        " - I, my, me, over, 조사, 접미사 같은 단어들은 문장에서는 자주 등장하지만 실제 의미 분석을 하는데는 거의 기여하는 바가 없는 경우\n",
        "\n",
        "NLTK에서는 불용어로 패키지 내에서 미리 정의하고 있다.\n",
        "## 1. NLTK에서 불용어 확인하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dT_pW6uDMMVK",
        "outputId": "f6f1e439-a596-409e-ef63-75101cc751bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPbm0gdaMEge",
        "outputId": "513e1036-1f8c-4eb4-c356-0ff7b5ea01d0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "from nltk.corpus import stopwords  \n",
        "stopwords.words('english')[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VojDEWC_MPxg"
      },
      "source": [
        "## 2. NLTK를 통해서 불용어 제거하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKGWx31UMJWV",
        "outputId": "716b5aa3-389d-463c-f948-430f24423f64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
            "['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "\n",
        "example = \"Family is not an important thing. It's everything.\"\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "word_tokens = word_tokenize(example)\n",
        "\n",
        "result = []\n",
        "for w in word_tokens: \n",
        "    if w not in stop_words: \n",
        "        result.append(w) \n",
        "\n",
        "print(word_tokens) \n",
        "print(result) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4EZSeZGMXJ0"
      },
      "source": [
        "'is', 'not', 'an'과 같은 단어들이 제거"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXfNbAnAMaKT"
      },
      "source": [
        "## 3. 한국어에서 불용어 제거하기\n",
        "사용자 가 직접 불용어 사전을 만드는 경우가 많다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHHiUEeTMRk7",
        "outputId": "558585c6-93af-4cc1-f8a8-c2fb450101c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['고기를', '아무렇게나', '구우려고', '하면', '안', '돼', '.', '고기라고', '다', '같은', '게', '아니거든', '.', '예컨대', '삼겹살을', '구울', '때는', '중요한', '게', '있지', '.']\n",
            "['고기를', '구우려고', '안', '돼', '.', '고기라고', '다', '같은', '게', '.', '삼겹살을', '구울', '때는', '중요한', '게', '있지', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "\n",
        "example = \"고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대 삼겹살을 구울 때는 중요한 게 있지.\"\n",
        "stop_words = \"아무거나 아무렇게나 어찌하든지 같다 비슷하다 예컨대 이럴정도로 하면 아니거든\"\n",
        "# 위의 불용어는 명사가 아닌 단어 중에서 저자가 임의로 선정한 것으로 실제 의미있는 선정 기준이 아님\n",
        "stop_words=stop_words.split(' ')\n",
        "word_tokens = word_tokenize(example)\n",
        "\n",
        "result = [] \n",
        "for w in word_tokens: \n",
        "    if w not in stop_words: \n",
        "        result.append(w) \n",
        "# 위의 4줄은 아래의 한 줄로 대체 가능\n",
        "# result=[word for word in word_tokens if not word in stop_words]\n",
        "\n",
        "print(word_tokens) \n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLRfCdVsMmUy"
      },
      "source": [
        "아래의 링크는 보편적으로 선택할 수 있는 한국어 불용어 리스트를 보여줍니다. (절대적인 기준 x)\n",
        "\n",
        "링크 : https://www.ranks.nl/stopwords/korean\n",
        "\n",
        "더 좋은 방법은 코드 내에서 직접 정의하지않고 txt파일이나 csv로 불용어를 정리해놓고 불러와서 사용하는 방법이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paIQh_5ZNf8r"
      },
      "source": [
        "# 05) 정규 표현식(Regular Expression)\n",
        "\n",
        "https://wikidocs.net/21703"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RDXf26eMjcT",
        "outputId": "c0de28f9-a8e8-4248-ebee-1b6867445bda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "대한민국의 국가\n",
            "애국가(愛國歌)는 대한민국의 국가이다. \n",
            "1919년 안창호에 의해 대한민국 임시 정부에서 스코틀랜드 민요인 〈작별〉에 삽입해서 부르기 시작하다가 1935년 한국의 작곡가 안익태가 지은 《한국환상곡》에 가사를 삽입해서 현재까지 부르고 있다.\n",
            " 가사의 작사자는 윤치호 설, 안창호 설, 윤치호와 최병헌 합작설 등이 있다. \n",
            "윤치호의 작사설 때문에 대한민국 임시 정부에서는 애국가를 바꾸려 하였으나 대한민국 임시 정부 주석 김구의 변호로 계속 애국가로 채택하게 되었다. \n",
            "이후 1948년의 정부 수립 이후 국가로 사용되어 왔으며, 2010년 국민의례 규정에서 국민의례시 애국가를 부르거나 연주하도록 함으로써 국가로서의 역할을 간접적으로 규정하고 있다.\n",
            " ㅈㅏㄱㅅㅏ\n",
            " 애국가의 가사는 1900년대 초에 쓰여졌다. \n",
            "작사자는 크게 윤치호라는 설과 안창호라는 설 두 가지가 있으며, 국사편찬위원회의 공식적인 입장으로는 미상이다. \n",
            "작사자 윤치호 설은 윤치호가 애국가의 가사를 1907년에 써서 후에 그 자신의 이름으로 출판했다는 것이다. \n",
            "한편 안창호가 썼다는 주장은 안창호가 애국가를 보급하는 데에 앞장섰다는 데에 중점을 두고 있다. \n",
            "1908년에 출판된 가사집 《찬미가》에 수록된 것을 비롯한 많은 일제 강점기의 애국가 출판물은 윤치호를 작사자로 돌리고 있는 등 윤치호 설에는 증거가 많은 반면 안창호 설에는 실증적인 자료가 부족하다.\n",
            " 윤치호의 사촌동생 윤치영(尹致瑛)은 윤치호가 대한민국의 애국가 가사의 일부를 썼다고 주장했다. \n",
            "애국가 링크 https://www.mois.go.kr/chd/sub/a05/story/screen.do \n",
            "작성자 abc123@uos.ac.kr\n"
          ]
        }
      ],
      "source": [
        "text = '대한민국의 국가\\n애국가(愛國歌)는 대한민국의 국가이다. \\n1919년 안창호에 의해 대한민국 임시 정부에서 스코틀랜드 민요인 〈작별〉에 삽입해서 부르기 시작하다가 1935년 한국의 작곡가 안익태가 지은 《한국환상곡》에 가사를 삽입해서 현재까지 부르고 있다.\\n 가사의 작사자는 윤치호 설, 안창호 설, 윤치호와 최병헌 합작설 등이 있다. \\n윤치호의 작사설 때문에 대한민국 임시 정부에서는 애국가를 바꾸려 하였으나 대한민국 임시 정부 주석 김구의 변호로 계속 애국가로 채택하게 되었다. \\n이후 1948년의 정부 수립 이후 국가로 사용되어 왔으며, 2010년 국민의례 규정에서 국민의례시 애국가를 부르거나 연주하도록 함으로써 국가로서의 역할을 간접적으로 규정하고 있다.\\n ㅈㅏㄱㅅㅏ\\n 애국가의 가사는 1900년대 초에 쓰여졌다. \\n작사자는 크게 윤치호라는 설과 안창호라는 설 두 가지가 있으며, 국사편찬위원회의 공식적인 입장으로는 미상이다. \\n작사자 윤치호 설은 윤치호가 애국가의 가사를 1907년에 써서 후에 그 자신의 이름으로 출판했다는 것이다. \\n한편 안창호가 썼다는 주장은 안창호가 애국가를 보급하는 데에 앞장섰다는 데에 중점을 두고 있다. \\n1908년에 출판된 가사집 《찬미가》에 수록된 것을 비롯한 많은 일제 강점기의 애국가 출판물은 윤치호를 작사자로 돌리고 있는 등 윤치호 설에는 증거가 많은 반면 안창호 설에는 실증적인 자료가 부족하다.\\n 윤치호의 사촌동생 윤치영(尹致瑛)은 윤치호가 대한민국의 애국가 가사의 일부를 썼다고 주장했다. \\n애국가 링크 https://www.mois.go.kr/chd/sub/a05/story/screen.do \\n작성자 abc123@uos.ac.kr'\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDNLkJAbVbRG"
      },
      "source": [
        "- 한자어\n",
        "- 특수기호\n",
        "- 오타\n",
        "- 링크\n",
        "- 이메일"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkBQJIXKVZpd",
        "outputId": "8d00aebf-7d2a-422f-caac-54ac08a2c717"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['(愛國歌)', '(尹致瑛)']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "re1 = re.compile('\\([一-龥]+\\)|\\([ぁ-ゔ]+\\)|\\([ァ-ヴー]+[々〆〤]+\\)')\n",
        "re1.findall(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbaTMPmkV_ZH",
        "outputId": "6d1965d9-1847-43db-c1af-a7da3e30ba99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "대한민국의 국가\n",
            "애국가는 대한민국의 국가이다. \n",
            "1919년 안창호에 의해 대한민국 임시 정부에서 스코틀랜드 민요인 〈작별〉에 삽입해서 부르기 시작하다가 1935년 한국의 작곡가 안익태가 지은 《한국환상곡》에 가사를 삽입해서 현재까지 부르고 있다.\n",
            " 가사의 작사자는 윤치호 설, 안창호 설, 윤치호와 최병헌 합작설 등이 있다. \n",
            "윤치호의 작사설 때문에 대한민국 임시 정부에서는 애국가를 바꾸려 하였으나 대한민국 임시 정부 주석 김구의 변호로 계속 애국가로 채택하게 되었다. \n",
            "이후 1948년의 정부 수립 이후 국가로 사용되어 왔으며, 2010년 국민의례 규정에서 국민의례시 애국가를 부르거나 연주하도록 함으로써 국가로서의 역할을 간접적으로 규정하고 있다.\n",
            " ㅈㅏㄱㅅㅏ\n",
            " 애국가의 가사는 1900년대 초에 쓰여졌다. \n",
            "작사자는 크게 윤치호라는 설과 안창호라는 설 두 가지가 있으며, 국사편찬위원회의 공식적인 입장으로는 미상이다. \n",
            "작사자 윤치호 설은 윤치호가 애국가의 가사를 1907년에 써서 후에 그 자신의 이름으로 출판했다는 것이다. \n",
            "한편 안창호가 썼다는 주장은 안창호가 애국가를 보급하는 데에 앞장섰다는 데에 중점을 두고 있다. \n",
            "1908년에 출판된 가사집 《찬미가》에 수록된 것을 비롯한 많은 일제 강점기의 애국가 출판물은 윤치호를 작사자로 돌리고 있는 등 윤치호 설에는 증거가 많은 반면 안창호 설에는 실증적인 자료가 부족하다.\n",
            " 윤치호의 사촌동생 윤치영은 윤치호가 대한민국의 애국가 가사의 일부를 썼다고 주장했다. \n",
            "애국가 링크 https://www.mois.go.kr/chd/sub/a05/story/screen.do \n",
            "작성자 abc123@uos.ac.kr\n"
          ]
        }
      ],
      "source": [
        "text = re1.sub('', text)\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eT04KNiiVln4",
        "outputId": "de4c0aa3-86e6-4f41-cb2a-56ca1b732cce"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['〈', '〉', '《', '》', 'ㅈㅏㄱㅅㅏ', '《', '》']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "re2 = re.compile(r'[^ .,?!/@$%~％·∼()\\x00-\\x7F가-힣]+')\n",
        "re2.findall(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWPQOa75WJ-r",
        "outputId": "a73d0f17-0c0a-45e6-d0f6-2efcf2f17b76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "대한민국의 국가\n",
            "애국가는 대한민국의 국가이다. \n",
            "1919년 안창호에 의해 대한민국 임시 정부에서 스코틀랜드 민요인 작별에 삽입해서 부르기 시작하다가 1935년 한국의 작곡가 안익태가 지은 한국환상곡에 가사를 삽입해서 현재까지 부르고 있다.\n",
            " 가사의 작사자는 윤치호 설, 안창호 설, 윤치호와 최병헌 합작설 등이 있다. \n",
            "윤치호의 작사설 때문에 대한민국 임시 정부에서는 애국가를 바꾸려 하였으나 대한민국 임시 정부 주석 김구의 변호로 계속 애국가로 채택하게 되었다. \n",
            "이후 1948년의 정부 수립 이후 국가로 사용되어 왔으며, 2010년 국민의례 규정에서 국민의례시 애국가를 부르거나 연주하도록 함으로써 국가로서의 역할을 간접적으로 규정하고 있다.\n",
            " \n",
            " 애국가의 가사는 1900년대 초에 쓰여졌다. \n",
            "작사자는 크게 윤치호라는 설과 안창호라는 설 두 가지가 있으며, 국사편찬위원회의 공식적인 입장으로는 미상이다. \n",
            "작사자 윤치호 설은 윤치호가 애국가의 가사를 1907년에 써서 후에 그 자신의 이름으로 출판했다는 것이다. \n",
            "한편 안창호가 썼다는 주장은 안창호가 애국가를 보급하는 데에 앞장섰다는 데에 중점을 두고 있다. \n",
            "1908년에 출판된 가사집 찬미가에 수록된 것을 비롯한 많은 일제 강점기의 애국가 출판물은 윤치호를 작사자로 돌리고 있는 등 윤치호 설에는 증거가 많은 반면 안창호 설에는 실증적인 자료가 부족하다.\n",
            " 윤치호의 사촌동생 윤치영은 윤치호가 대한민국의 애국가 가사의 일부를 썼다고 주장했다. \n",
            "애국가 링크 https://www.mois.go.kr/chd/sub/a05/story/screen.do \n",
            "작성자 abc123@uos.ac.kr\n"
          ]
        }
      ],
      "source": [
        "text = re2.sub('', text)\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Q1zvxMZVwZ_",
        "outputId": "db589fcd-d9c9-4f35-aca2-ed1624c69547"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['https://www.mois.go.kr/chd/sub/a05/story/screen.do']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "re3 = re.compile(r'\\w+:\\/\\/\\S+')\n",
        "re3.findall(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkXVv5WQWRiR",
        "outputId": "df4ba9dd-5f31-4cc4-af72-556271896d98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "대한민국의 국가\n",
            "애국가는 대한민국의 국가이다. \n",
            "1919년 안창호에 의해 대한민국 임시 정부에서 스코틀랜드 민요인 작별에 삽입해서 부르기 시작하다가 1935년 한국의 작곡가 안익태가 지은 한국환상곡에 가사를 삽입해서 현재까지 부르고 있다.\n",
            " 가사의 작사자는 윤치호 설, 안창호 설, 윤치호와 최병헌 합작설 등이 있다. \n",
            "윤치호의 작사설 때문에 대한민국 임시 정부에서는 애국가를 바꾸려 하였으나 대한민국 임시 정부 주석 김구의 변호로 계속 애국가로 채택하게 되었다. \n",
            "이후 1948년의 정부 수립 이후 국가로 사용되어 왔으며, 2010년 국민의례 규정에서 국민의례시 애국가를 부르거나 연주하도록 함으로써 국가로서의 역할을 간접적으로 규정하고 있다.\n",
            " \n",
            " 애국가의 가사는 1900년대 초에 쓰여졌다. \n",
            "작사자는 크게 윤치호라는 설과 안창호라는 설 두 가지가 있으며, 국사편찬위원회의 공식적인 입장으로는 미상이다. \n",
            "작사자 윤치호 설은 윤치호가 애국가의 가사를 1907년에 써서 후에 그 자신의 이름으로 출판했다는 것이다. \n",
            "한편 안창호가 썼다는 주장은 안창호가 애국가를 보급하는 데에 앞장섰다는 데에 중점을 두고 있다. \n",
            "1908년에 출판된 가사집 찬미가에 수록된 것을 비롯한 많은 일제 강점기의 애국가 출판물은 윤치호를 작사자로 돌리고 있는 등 윤치호 설에는 증거가 많은 반면 안창호 설에는 실증적인 자료가 부족하다.\n",
            " 윤치호의 사촌동생 윤치영은 윤치호가 대한민국의 애국가 가사의 일부를 썼다고 주장했다. \n",
            "애국가 링크  \n",
            "작성자 abc123@uos.ac.kr\n"
          ]
        }
      ],
      "source": [
        "text = re3.sub('', text)\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bo7Sft8XV4FI",
        "outputId": "eee9babd-320e-48c2-bc31-26c5d0aaa0cb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['abc123@uos.ac.kr']"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "re4 = re.compile(r'[a-zA-Z0-9+-_.]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+')\n",
        "re4.findall(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjT8tFOjV8dv",
        "outputId": "2bc3be99-3bed-40ff-b497-57e18f2ddc46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "대한민국의 국가\n",
            "애국가는 대한민국의 국가이다. \n",
            "1919년 안창호에 의해 대한민국 임시 정부에서 스코틀랜드 민요인 작별에 삽입해서 부르기 시작하다가 1935년 한국의 작곡가 안익태가 지은 한국환상곡에 가사를 삽입해서 현재까지 부르고 있다.\n",
            " 가사의 작사자는 윤치호 설, 안창호 설, 윤치호와 최병헌 합작설 등이 있다. \n",
            "윤치호의 작사설 때문에 대한민국 임시 정부에서는 애국가를 바꾸려 하였으나 대한민국 임시 정부 주석 김구의 변호로 계속 애국가로 채택하게 되었다. \n",
            "이후 1948년의 정부 수립 이후 국가로 사용되어 왔으며, 2010년 국민의례 규정에서 국민의례시 애국가를 부르거나 연주하도록 함으로써 국가로서의 역할을 간접적으로 규정하고 있다.\n",
            " \n",
            " 애국가의 가사는 1900년대 초에 쓰여졌다. \n",
            "작사자는 크게 윤치호라는 설과 안창호라는 설 두 가지가 있으며, 국사편찬위원회의 공식적인 입장으로는 미상이다. \n",
            "작사자 윤치호 설은 윤치호가 애국가의 가사를 1907년에 써서 후에 그 자신의 이름으로 출판했다는 것이다. \n",
            "한편 안창호가 썼다는 주장은 안창호가 애국가를 보급하는 데에 앞장섰다는 데에 중점을 두고 있다. \n",
            "1908년에 출판된 가사집 찬미가에 수록된 것을 비롯한 많은 일제 강점기의 애국가 출판물은 윤치호를 작사자로 돌리고 있는 등 윤치호 설에는 증거가 많은 반면 안창호 설에는 실증적인 자료가 부족하다.\n",
            " 윤치호의 사촌동생 윤치영은 윤치호가 대한민국의 애국가 가사의 일부를 썼다고 주장했다. \n",
            "애국가 링크  \n",
            "작성자 \n"
          ]
        }
      ],
      "source": [
        "text = re4.sub('', text)\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "OQENsxrjWW-h"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Text_Preprocessing_1.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}