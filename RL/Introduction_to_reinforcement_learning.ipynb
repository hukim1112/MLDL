{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Introduction_to_reinforcement_learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hukim1112/MLDL/blob/master/RL/Introduction_to_reinforcement_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyJqXiv99uG2"
      },
      "source": [
        "# An introduction to reinforcement learning\n",
        "© Pablo Samuel Castro, 2020. Apache License 2.0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Id90CCTiN3kI"
      },
      "source": [
        "You can also watch a video where I go through the basics [here](https://youtu.be/xMZE-9WECQE).\n",
        "\n",
        "Pueden ver un video (en español) donde presento el material [aquí](https://youtu.be/ZOaA4svJH3U).\n",
        "\n",
        "강화학습은 불확실한 환경에서 연속적인 결정을 내리기 위한 방법입니다. 전형적인 프레임에서 에이전트는 환경과 상호작용을 하고, 환경으로부터의 피드백이 에이전트의 결정에 영향을 줍니다. 에이전트는 최적의 선택의 방향으로 행동을 점차 강화해갑니다.\n",
        "\n",
        "*  에이전트는 환경의 어떤 상황에서 행동을 합니다.\n",
        "*  환경은 에이전트에게 반응을 줍니다.:\n",
        "   *  에이전트가 있는 상태와 취한 액션에 따라 보상/비용이 달라집니다.\n",
        "   *  에이전트는 선택한 액션에 따라 새로운 상태로 이동합니다.(경우에 따라 액션을 취했음에도 동일한 상태에 머무를 수 있습니다.)\n",
        "*  에이전트는 양의 보상을 증가시키는 방향으로 자신의 행동을 업데이트하고, 반대로 패널티를 줄이도록 업데이트합니다.\n",
        "\n",
        "![General RL setup](https://raw.githubusercontent.com/psc-g/intro_to_rl/master/images/generalRL.png)\n",
        "\n",
        "이 노트북에서는 구글 도파민을 사용해 강화학습 에이전트를 학습시켜봅니다.\n",
        "[dopamine framework](https://github.com/google/dopamine).\n",
        "\n",
        "노트북을 통해 다음을 배웁니다.:\n",
        "1.  강화학습의 주요한 포인트들과 도전적인 부분\n",
        "2.  MDP에 대한 수학적 표현\n",
        "3.  환경이 완벽하게 알려진 상황에서의 테이블 메소드를 보여줍니다. 이 방법들은 환경을 모르는 상황에 대한 기반이 될 것입니다.\n",
        "4.  Value 기반 학습을 소개합니다.\n",
        "5.  간단한 MDP 문제를 4의 방법으로 풉니다.\n",
        "6.  Deep RL 문제를 세부적으로 이해합니다.\n",
        "7.  우리는 강화학습을 거대한 MDP 문제에 적용해볼 것 입니다.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9BvzpGnRwH8"
      },
      "source": [
        "# A motivating example\n",
        "Consider the following simple grid where the agent (the blue circle) must navigate to the gold star by choosing from four actions at each grid cell (up, down, left, right):\n",
        "\n",
        "![Simple grid](https://raw.githubusercontent.com/psc-g/intro_to_rl/master/images/gridWorldExample.png)\n",
        "\n",
        "Being able to see the whole grid, it is trivial for us to find a shortest path to the gold star (the optimal policy for the agent):\n",
        "\n",
        "![Optimal policy](https://raw.githubusercontent.com/psc-g/intro_to_rl/master/images/gridWorldPolicy.png)\n",
        "\n",
        "In reinforcement learning problems, however, we typically do not have immediate access to the environment. In our simple example, the agent may simply know the number of cells in the grid, but may not have any information on _what_ is contained in each\n",
        "\n",
        "![Uncertain grid](https://raw.githubusercontent.com/psc-g/intro_to_rl/master/images/gridWorldUncertain.png)\n",
        "\n",
        "The agent must then _explore_ the environment by taking actions and keeping track of what is encountered at the different cells:\n",
        "\n",
        "![Partially explored](https://raw.githubusercontent.com/psc-g/intro_to_rl/master/images/gridWorldPartiallyExplored.png)\n",
        "\n",
        "In the previous image, the grey arrows represent the actions that the agent has already taken in each of those cells. It benefits the agent to keep track of these so as to remember both the actions that lead to rewards and those that lead to penalties.\n",
        "\n",
        "To anticipate some of the concepts that will be formalized below, the grid represents the _environment_. This environment consists of:\n",
        "\n",
        "*  A set of _states_ (the grid cells)\n",
        "*  A set of possible _actions_ (up, down, left, right)\n",
        "*  Transition dynamics, defining the effect of the actions (e.g. taking the _right_ action will take the agent to the cell to the right of its current state)\n",
        "*  A reward signal that indicates the benefit or cost of the action just taken (e.g. the gold star is a positive reward received upon entering the bottom-right state).\n",
        "\n",
        "The agent is typically aware of the states and actions, but not of the transition dynamics nor reward signal. Thus, the agent make use of a method that:\n",
        "\n",
        "*  Will _explore_ the environment to learn about the transition and reward dynamics.\n",
        "*  Will keep track of previously visited states and the actions performed therein, which can be used to inform future action choices.\n",
        "*  Can balance _exploring_ new experiences and _exploiting_ the knowledge it has already achieved.\n",
        "\n",
        "This last point is the \"exploit-explore dilemma\", which is a central (and open!) problem in reinforcement learning. How to best balance this tradeoff in a computationally efficient manner is the subject of active research.\n",
        "\n",
        "To illustrate its importance, assume the agent encountered a small reward in the grid. If at that point the agent were to decide to purely exploit its current knowledge, it would settle on the policy below, without ever exploring the unknown area where a larger reward awaits:\n",
        "\n",
        "![Suboptimal policy](https://raw.githubusercontent.com/psc-g/intro_to_rl/master/images/gridWorldSuboptimalPolicy.png)\n",
        "\n",
        "Having finished this motivating example, we are now ready to dive into the mathematical details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gu96w-0ejxUt"
      },
      "source": [
        "# Markov decision processes\n",
        "More formally, the problem is typically expressed as a Markov decision process\n",
        "$\\langle\\mathcal{S},\\mathcal{A},R,P,\\gamma\\rangle$, where:\n",
        "*  $\\mathcal{S}$ is a finite set of _states_ that the agent \"inhabits\". You can also consider continuous state spaces, but for this notebook we will only be considering the finite kind.\n",
        "*  $\\mathcal{A}$ is a finite set of _actions_ that the agent can perform from each state. In the vast majority of cases, it is assumed that all actions are available in all states. Action spaces can also be continuous, but we will be limiting ourselves to finite action spaces in this notebook.\n",
        "*  $R:\\mathcal{S}\\times\\mathcal{A}\\rightarrow [R_{min}, R_{max}]\\subset\\mathbb{R}$ is a bounded reward (or cost) function which provides the agent with (positive or negative) reinforcement.\n",
        "*  $P:\\mathcal{S}\\times\\mathcal{A}\\rightarrow\\Delta(\\mathcal{S})$ encodes the transition dynamics, where $P(s, a)(s')$ is the probability of ending in state $s'$ after taking action $a$ from state $s$.\n",
        "*  $\\gamma\\in[0, 1)$ is a discount factor.\n",
        "\n",
        "The notation $\\Delta(X)$ stands for the set of probability distributions over a set $X$.\n",
        "\n",
        "Since the discount factor $\\gamma$ is strictly less than $1$, it encourages the agent to accumulate rewards as quickly as possible. In the motivating example above, it means the optimal policy is also the shortest path to the big star. Having $\\gamma = 1$ would mean that the agent perceives no difference between taking a short or long path to the gold star.\n",
        "\n",
        "The agent interacts with the environment in discrete time steps. If at time step $t$ the agent is in state $s_t$ and chooses action $a_t$, the environment responds with a new state $s_{t+1}\\sim P(s_t, a_t)$ and a reward $R(s_t, a_t)$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZC1NHrej5uL"
      },
      "source": [
        "## Policies and value functions\n",
        "A policy $\\pi$ is a mapping from states to a distribution over actions: $\\pi:\\mathcal{S}\\rightarrow\\Delta(\\mathcal{A})$, and encodes the agent's _behaviour_. The _value_ of a policy $\\pi$ from an initial state $s_0$ is encoded as:\n",
        "\n",
        "$V^{\\pi}(s_0) = \\left[\\sum_{t=0}^{\\infty}\\gamma^t R(s_t, a_t) | a_t\\sim\\pi(s_t),s_{t+1}\\sim P(s_t, a_t)\\right]$\n",
        "\n",
        "This is quantifying the expected sum of discounted rewards when starting from state $s_0$ and following policy $\\pi$. It turns out we can express value functions for any state $s$ via the recurrence:\n",
        "\n",
        "$V^{\\pi}(s) = \\mathbb{E}_{a\\sim\\pi(s)}\\left[R(s, a) + \\gamma\\mathbb{E}_{s'\\sim P(s, a)}[V^{\\pi}(s')]\\right]$\n",
        "\n",
        "or equivalently, replacing the expectation over actions with a summation:\n",
        "\n",
        "$V^{\\pi}(s) = \\sum_{a\\in\\mathcal{A}}\\pi(s)(a)\\left[R(s, a) + \\gamma\\sum_{s'\\in\\mathcal{S}} P(s, a)(s')V^{\\pi}(s')\\right]$\n",
        "\n",
        "We may sometimes also be interested in the value of performing action $a$ from state $s$, and _then_ following policy $\\pi$:\n",
        "\n",
        "$Q^{\\pi}(s, a) = R(s, a) + \\gamma\\mathbb{E}_{s'\\sim P(s, a)}[V^{\\pi}(s')]$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dh51ypiZkFSA"
      },
      "source": [
        "## Optimal policies and value functions\n",
        "The goal of the agent is to find a policy $\\pi^*$ that dominates all other policies: $V^* := V^{\\pi^*} \\geq V^{\\pi}$ for all $\\pi$. It turns out that there is always a deterministic policy that achieves the optimum. The Bellman optimality equations express this via the recurrence:\n",
        "\n",
        "$V^*(s) = \\max_{a\\in\\mathcal{A}}\\left[R(s, a) + \\gamma\\sum_{s'\\in\\mathcal{S}} P(s, a)(s')V^*(s')\\right]$\n",
        "\n",
        "The state-action value function can be defined in a similar way as above:\n",
        "\n",
        "$Q^*(s, a) = R(s, a) + \\gamma\\mathbb{E}_{s'\\sim P(s, a)}[V^*(s')]$\n",
        "\n",
        "Clearly, we can compute $Q^*$ from $V^*$, and once we know $Q^*$ we can find $\\pi^*$ via:\n",
        "\n",
        "$\\pi^*(s) = \\arg\\max_{a\\in\\mathcal{A}}Q^*(s, a)$\n",
        "\n",
        "The important question, then, is _how_ do we find $\\pi^*$, $V^*$, and/or $Q^*$? Read on and find out!\n",
        "\n",
        "To begin with, run the first two cells below to install and import all necessary packages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2v7SRXLGPeb0"
      },
      "source": [
        "# Setting up the colab\n",
        "First, make sure your runtime is using a GPU accelerator. In the \"Runtime\" menu,\n",
        "select \"Change runtime type\" and select GPU as the hardware accelerator.\n",
        "\n",
        "Then, run the cell below to install all the necessary packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inwNVOucQ6gr",
        "cellView": "form"
      },
      "source": [
        "# @title Installs and imports (run me first!)\n",
        "!pip install pyglet~=1.3.2\n",
        "!apt install -y graphviz\n",
        "!pip install flax\n",
        "!pip install graphviz\n",
        "!pip install pyvirtualdisplay\n",
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y\n",
        "!pip install 'gym[atari]'\n",
        "!pip install -U dopamine-rl\n",
        "\n",
        "import flax\n",
        "from graphviz import Digraph\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as onp\n",
        "\n",
        "from IPython.display import HTML\n",
        "from pprint import pprint\n",
        "import logging\n",
        "from pyvirtualdisplay import Display\n",
        "logging.getLogger(\"pyvirtualdisplay\").setLevel(logging.ERROR)\n",
        "\n",
        "display = Display(visible=0, size=(1024, 768))\n",
        "display.start()\n",
        "import os\n",
        "os.environ[\"DISPLAY\"] = \":\" + str(display.display)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ghaDr8PPm6A"
      },
      "source": [
        "# Computing $V^*$ exactly\n",
        "If we have access to $P$ and $R$, then we can compute $V^*$ in a few ways. We will explore two common methods here:\n",
        "*  Value iteration\n",
        "*  Policy iteration\n",
        "\n",
        "When the transition and reward dynamics are known, these problems are typically referred to as _planning_ problems, which has its own field of active research. In this scenario, the \"agent\" does not need to interact with an environment to gather information for improving its policy. There is a close relationship between planning and reinforcement learning, and many lines of research sit at their intersection.\n",
        "\n",
        "For our purposes, it will be useful to review some solutions to the planning problem, as they will serve as foundations for the learning algorithms we present further down."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htXeBYYgmBw3"
      },
      "source": [
        "## Value iteration\n",
        "In Value iteration we are continuously updating an estimate $V_{t+1}$ by leveraging our previous estimate $V_t$.\n",
        "\n",
        "$V_{t+1}(s) := \\max_{a\\in\\mathcal{A}} \\left[ R(s, a) + \\gamma \\sum_{s'\\in\\mathcal{S}}P(s, a)(s') V_t(s') \\right]$\n",
        "\n",
        "This is typically referred to as the _Bellman backup_. It can be shown that, starting from an initial estimate $V_0$, $\\lim_{t\\rightarrow\\infty}V_t = V^*$.\n",
        "\n",
        "This gives us the value iteration algorithm:\n",
        "1.  Initialize $V\\equiv 0$\n",
        "1.  Loop until convergence:\n",
        "    *  For every $s\\in\\mathcal{S}$:<br>$V(s)\\leftarrow \\max_{a\\in\\mathcal{A}} \\left[ R(s, a) + \\gamma \\sum_{s'\\in\\mathcal{S}}P(s, a)(s') V(s') \\right]$\n",
        "1.  Return $V$\n",
        "\n",
        "It will be useful to think of the functions above as vectors, which then allows\n",
        "us to do the Bellman backup with matrix operations. If we assume our matrices have the following shapes:\n",
        "```\n",
        "P.shape = [num_states, num_actions, num_states]\n",
        "R.shape = [num_states, num_actions]\n",
        "V.shape = [num_states]\n",
        "Q.shape = [num_states, num_actions]\n",
        "```\n",
        "Then we can compute $V$ with two lines of `numpy` code:\n",
        "```\n",
        "import numpy as np\n",
        "Q = R + gamma * np.matmul(P, V)\n",
        "V = np.max(Q, axis=1)\n",
        "```\n",
        "\n",
        "Alternatively, we can use the [einsum](https://numpy.org/doc/stable/reference/generated/numpy.einsum.html) function, which can help clarify the dimensions along which the multiplication is happening:\n",
        "```\n",
        "Q = R + gamma * np.einsum('sat,t->sa', P, V)\n",
        "V = np.max(Q, axis=1)\n",
        "```\n",
        "\n",
        "As mentioned above, once we have $V^*$, we can compute $Q^*$ and $\\pi^*$.\n",
        "\n",
        "The code cell below is an implementation of value iteration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opTbb6yOa5PS"
      },
      "source": [
        "def value_iteration(P, R, gamma, tolerance=1e-3):\n",
        "  \"\"\"Find V* using value iteration.\n",
        "\n",
        "  Args:\n",
        "    P: numpy array defining transition dynamics. Shape: |S| x |A| x |S|.\n",
        "    R: numpy array defining rewards. Shape: |S| x |A|.\n",
        "    gamma: float, discount factor.\n",
        "    tolerance: float, tolerance level for computation.\n",
        "\n",
        "  Returns:\n",
        "    V*: numpy array of shape ns.\n",
        "    Q*: numpy array of shape ns x na.\n",
        "  \"\"\"\n",
        "  assert P.shape[0] == P.shape[2]\n",
        "  assert P.shape[0] == R.shape[0]\n",
        "  assert P.shape[1] == R.shape[1]\n",
        "  ns = P.shape[0]\n",
        "  na = P.shape[1]\n",
        "  V = onp.zeros(ns)\n",
        "  Q = onp.zeros((ns, na))\n",
        "  error = tolerance * 2\n",
        "  while error > tolerance:\n",
        "    # This is the Bellman backup (onp.einsum FTW!).\n",
        "    Q = R + gamma * onp.einsum('sat,t->sa', P, V)\n",
        "    new_V = onp.max(Q, axis=1)\n",
        "    error = onp.max(onp.abs(V - new_V))\n",
        "    V = onp.copy(new_V)\n",
        "  return V, Q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBIopRGWo_dk"
      },
      "source": [
        "## Policy iteration\n",
        "Instead of iterating over $V_t$, we can iterate over $\\pi_t$ and stop once the\n",
        "policy is no longer changing. The algorithm proceeds as follows:\n",
        "\n",
        "1.  Initialize $\\pi$ arbitrarily. For simplicity we will assume it is a matrix of shape `[num_states, num_actions]` with only one `1.0` for each row (i.e. a deterministic policy).\n",
        "1.  While $\\pi$ is changing:\n",
        "    *  $Q(s, a) = R(s, a) + \\gamma\\sum_{s'\\in\\mathcal{S}}P(s, a)(s')Q(s', \\pi(s'))$\n",
        "    *  $\\pi(s) = \\arg\\max Q(s, \\cdot)$\n",
        "1.  $Q(s, a) = R(s, a) + \\gamma\\sum_{s'\\in\\mathcal{S}}P(s, a)(s')Q(s', \\pi(s'))$\n",
        "1.  $V(s) = \\max_{a\\in\\mathcal{A}}Q(s, a)$\n",
        "1.  Return $V$\n",
        "\n",
        "Notice that, in contrast to value iteration, here we compute $Q^*$ and $V^*$ from $\\pi^*$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGva4Eljo-St"
      },
      "source": [
        "def policy_iteration(P, R, gamma):\n",
        "  \"\"\"Find V* using policy iteration.\n",
        "\n",
        "  Args:\n",
        "    P: numpy array defining transition dynamics. Shape: |S| x |A| x |S|.\n",
        "    R: numpy array defining rewards. Shape: |S| x |A|.\n",
        "    gamma: float, discount factor.\n",
        "\n",
        "  Returns:\n",
        "    V*: numpy array of shape ns.\n",
        "    Q*: numpy array of shape ns x na.\n",
        "  \"\"\"\n",
        "  assert P.shape[0] == P.shape[2]\n",
        "  assert P.shape[0] == R.shape[0]\n",
        "  assert P.shape[1] == R.shape[1]\n",
        "  ns = P.shape[0]\n",
        "  na = P.shape[1]\n",
        "  V = onp.zeros(ns)\n",
        "  Q = onp.zeros((ns, na))\n",
        "  pi = onp.zeros((ns, na))\n",
        "  for s in range(ns):\n",
        "    pi[s, onp.random.choice(na)] = 1.\n",
        "  policy_stable = False\n",
        "  while not policy_stable:\n",
        "    old_pi = onp.copy(pi)\n",
        "    # Extract V from Q using pi.\n",
        "    V = [Q[s, onp.argmax(pi[s])] for s in range(ns)]\n",
        "    Q = R + gamma * onp.einsum('sat,t->sa', P, V)\n",
        "    pi = onp.zeros((ns, na))\n",
        "    for s in range(ns):\n",
        "      pi[s, onp.argmax(Q[s])] = 1.\n",
        "    policy_stable = onp.array_equal(pi, old_pi)\n",
        "  V = [Q[s, onp.argmax(pi[s])] for s in range(ns)]\n",
        "  Q = R + gamma * onp.einsum('sat,t->sa', P, V)\n",
        "  V = [Q[s, onp.argmax(pi[s])] for s in range(ns)]\n",
        "  return V, Q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vv2TbgfwJDqw"
      },
      "source": [
        "## Learning $V^*$\n",
        "What if one does _not_ have access to $P$ and $R$? This is the most common scenario in reinforcement learning problems, and here the agents must _learn_ how to behave by interacting with the environment.\n",
        "\n",
        "The diagram below depicts this pictorially:\n",
        "*  The _agent_, in state $s$, picks an action $a$ from its policy $\\pi(s)$ and sends this action to the environment.\n",
        "*  The _environment_ returns a new state $s'\\sim P(s, a)$ and reward $R(s, a)$ to the agent.\n",
        "*  The agent can then use this new information to update its policy $\\pi$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHEkc05uLDeL",
        "cellView": "form"
      },
      "source": [
        "# @title The standard reinforcement learning scenario\n",
        "rl_styles = {\n",
        "    'agent':  {'shape': 'rectangle', 'style': 'filled', 'color': 'lightblue'},\n",
        "    'env':  {'shape': 'rectangle', 'style': 'filled', 'color': 'orange'},\n",
        "}\n",
        "rl_setup = Digraph(engine='neato')\n",
        "rl_setup.node('Agent', 'Agent', rl_styles['agent'], pos='0,0')\n",
        "rl_setup.node('Environment', 'Environment', rl_styles['env'], pos='3,0!')\n",
        "rl_setup.edge('Agent', 'Environment', 'a')\n",
        "rl_setup.edge('Environment', 'Agent', 's\\', R(s, a)')\n",
        "rl_setup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVJ6qRhELAxk"
      },
      "source": [
        "### Model-based versus model-free approaches\n",
        "Two common approaches for handling this are:\n",
        "\n",
        "1.  ***Model-based methods***: Learn approximate models $\\hat{P}$ and $\\hat{R}$ from the experience received from the environment, and solve for $\\hat{V}^*$, $\\hat{Q}^*$, and $\\hat{\\pi}^*$ using value/policy iteration.\n",
        "1.  ***Model-free methods***: Learn approximates $\\hat{V}^*$, $\\hat{Q}^*$, and/or $\\hat{\\pi}^*$ using the experience received from the environment.\n",
        "\n",
        "There are pros and cons for each of these approaches, and there is extensive (and continuing) research for both.\n",
        "\n",
        "For example, model-based methods tend to be more sample-efficient, since one can always sample from $\\hat{P}$ and $\\hat{R}$ without having to interact with the real environment.\n",
        "\n",
        "On the other hand, model-free methods are typically easier to learn and update in an online fashion (as new experience arrives), which is usually desirable.\n",
        "\n",
        "For the rest of this notebook we will focus on model-free methods.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amOY4xSQO3MK"
      },
      "source": [
        "### Value-based versus policy-based approaches\n",
        "Within model-free methods, there are two main approaches used: value-based versus policy-based.\n",
        "\n",
        "*  ***Value-based methods*** maintain and update an estimate for $Q^*$\n",
        "*  ***Policy-based methods*** maintain and update an estimate for $\\pi^*$\n",
        "\n",
        "Once again, there are pros and cons and extensive literature for both of these approaches.\n",
        "\n",
        "In this notebook, we will focus on value-based methods.\n",
        "\n",
        "You can learn more about policy-based methods in the [Sutton & Barto book](http://www.incompleteideas.net/book/the-book-2nd.html) and in the [Spinning up in Deep RL](https://spinningup.openai.com/en/latest/) post by [Open-AI](https://openai.com/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eq79nct8QwAI"
      },
      "source": [
        "### Exploration\n",
        "One of the central issues in reinforcement learning is the exploration-exploitation dilemma. This was briefly introduced in the motivating example at the top, but is critical to the eventual performance of the agent.\n",
        "\n",
        "Consider an agent that started with an initial policy $\\pi_0$, and at iteration $t$ has policy $\\pi_t$ and state-action value function $Q^{\\pi_t}\\gg Q^{\\pi_0}$. While at state $s$, the natural thing would be for the agent to pick action $a = \\pi_t(s) = \\arg\\max_{a\\in\\mathcal{A}}Q^{\\pi_t}(s, a)$. This would be a purely _exploitative_ policy.\n",
        "\n",
        "What if, had the agent picked a different action, $b\\ne a$ that resulted in a very large reward? Imagine the agent _had_ selected action $b$ and the new estimate $Q_{t+1}$ had the property that $Q_{t+1}(s, b) > Q_{t+1}(s, a)$, then the policy would be updated as $\\pi_{t+1}(s) = b$.\n",
        "\n",
        "At iteration $t$, by definition, $Q^{\\pi_t}(s, b) < Q^{\\pi_t}(s, a)$, so the agent would have never selected action $b$ with a purely exploitative policy, thereby missing out on a larger reward! Thus, it would have benefited the agent to select _sub-optimal_ action $b$ to uncover the larger reward.\n",
        "\n",
        "Selecting a sub-optimal action is what is referred to as _exploration_, since the reasoning behind choosing sub-optimally is precisely to explore the environment and potentially discover better policies.\n",
        "\n",
        "A purely exploratory policy would not be desirable either, as the agent would then always select actions randomly, which would make it unlikely to maximize the expected rewards received.\n",
        "\n",
        "As mentioned previously, balancing this exploration-exploitation tradeoff is a very active area of research.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0i_ZuXDTm_O"
      },
      "source": [
        "#### $\\epsilon$-greedy exploration\n",
        "Perhaps the simplest and best-known exploration method is $\\epsilon$-greedy exploration. At state $s$, given a policy $\\pi$, the rule for this exploration policy is simply:\n",
        "*  With probability $\\epsilon$ select an action randomly\n",
        "*  With probability $1-\\epsilon$ select action $a=\\arg\\max_{a\\in\\mathcal{A}}\\pi(s)$\n",
        "\n",
        "The code snippet below implements this exploratory policy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxJMOzjzUSHt"
      },
      "source": [
        "def epsilon_greedy(s, pi, epsilon):\n",
        "  \"\"\"A simple implementation of epsilon-greedy exploration.\n",
        "\n",
        "  Args:\n",
        "    s: int, the agent's current state.\n",
        "    pi: numpy array of shape [num_states, num_actions] encoding the agent's\n",
        "      policy.\n",
        "    epsilon: float, the epsilon value for epsilon-greedy exploration.\n",
        "\n",
        "  Returns:\n",
        "    An integer representing the action choice.\n",
        "  \"\"\"\n",
        "  na = pi.shape[1]\n",
        "  p = onp.random.rand()\n",
        "  if onp.random.rand() < epsilon:\n",
        "    return onp.random.choice(na)\n",
        "  return onp.random.choice(na, p=pi[s])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHkGV24EJGGR"
      },
      "source": [
        "### Monte Carlo methods\n",
        "Perhaps the simplest way of estimating the sum of future returns is using Monte Carlo methods. These methods execute full _trajectories_ in the environment and estimate $V^{\\pi}$ by averaging the observed trajectory returns.\n",
        "\n",
        "The algorithm can be described as follows:\n",
        "\n",
        "1.  Initialize $Q$ and pick a start state $s$.\n",
        "1.  Initialize a list $Returns$ of $|\\mathcal{S}|\\times|\\mathcal{A}|$ elements which accumulates observed returns for each state.\n",
        "1.  Initialize $\\pi$ randomly.\n",
        "1.  While learning:\n",
        "    1.  Generate a trajectory $\\langle s_0,a_0,r_0,\\cdots,s_T,a_T,r_T\\rangle$ using $\\pi$.\n",
        "    1.  $G = 0$\n",
        "    1.  For $t=T$ down to $0$:\n",
        "        1.  $G = \\gamma G + r_t$\n",
        "        1.  If $(s_t,a_t)$ does _not_ appear in $\\langle s_0,a_0,\\cdots,s_{t-1},a_{t-1}\\rangle$:\n",
        "            *  Append $G$ to $Returns(s_t, a_t)$\n",
        "            *  $Q(s_t, a_t) = average(Returns(s_t, a_t))$\n",
        "            *  $\\pi(s_t) = \\arg\\max_{a\\in\\mathcal{A}} Q(s_t, a)$\n",
        "\n",
        "The next code cell implements this approach."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-of0iY-QLit2"
      },
      "source": [
        "def monte_carlo(ns, na, step_fn, gamma, start_state, reset_state, total_episodes,\n",
        "               max_steps_per_iteration, epsilon, V):\n",
        "  \"\"\"A simple implementation of Q-learning.\n",
        "\n",
        "  Args:\n",
        "    ns: int, the number of states.\n",
        "    na: int, the number of actions.\n",
        "    step_fn: a function that receives a state and action, and returns a float\n",
        "      (reward) and next state. This represents the interaction with the\n",
        "      environment.\n",
        "    gamma: float, the discount factor.\n",
        "    start_state: int, index of starting state.\n",
        "    reset_state: int, index of state where environment resets back to start\n",
        "      state, or None if there is no reset state.\n",
        "    total_episodes: int, total number of episodes.\n",
        "    max_steps_per_iteration: int, maximum number of steps per iteration.\n",
        "    epsilon: float, exploration rate for epsilon-greedy exploration.\n",
        "    V: numpy array, true V* used for computing errors. Shape: [num_states].\n",
        "  \n",
        "  Returns:\n",
        "    V_hat: numpy array, learned value function. Shape: [num_states].\n",
        "    Q_hat: numpy array, learned Q function. Shape: [num_states, num_actions].\n",
        "    max_errors: list of floats, contains the error max_s |V*(s) - \\hat{V}*(s)|.\n",
        "    avg_errors: list of floats, contains the error avg_s |V*(s) - \\hat{V}*(s)|.\n",
        "  \"\"\"\n",
        "  # Initialize policy randomly.\n",
        "  pi_hat = onp.zeros((ns, na))\n",
        "  for s in range(ns):\n",
        "    pi_hat[s, onp.random.choice(na)] = 1.\n",
        "  # Initialize Q randomly.\n",
        "  Q_hat = onp.zeros((ns, na))\n",
        "  # Initialize the accumulated returns and number of updates.\n",
        "  returns = onp.zeros((ns, na))\n",
        "  counts = onp.zeros((ns, na))\n",
        "  # Lists to keep track of training statistics.\n",
        "  iteration_returns = []\n",
        "  max_errors = []\n",
        "  avg_errors = []\n",
        "  for episode in range(total_episodes):\n",
        "    # Each episode starts in the same start state.\n",
        "    s = start_state\n",
        "    step = 0\n",
        "    # Lists collected for each trajectory.\n",
        "    states = []\n",
        "    actions = []\n",
        "    rewards = []\n",
        "    # Generate a trajectory for a limited number of steps.\n",
        "    while step < max_steps_per_iteration:\n",
        "      step += 1\n",
        "      states.append(s)\n",
        "      a = epsilon_greedy(s, pi_hat, epsilon)  # Pick action.\n",
        "      actions.append(a)\n",
        "      r, s2 = step_fn(s, a)  # Take a step in the environment.\n",
        "      rewards.append(r)\n",
        "      if s2 == reset_state:\n",
        "        # If we've reached a reset state, the trajectory is over.\n",
        "        break\n",
        "      s = s2\n",
        "    # Update the Q-values based on the rewards received by traversing the\n",
        "    # trajectory in reverse order.\n",
        "    G = 0  # Accumulated returns.\n",
        "    step -= 1\n",
        "    while step >= 0:\n",
        "      G = gamma * G + rewards[-1]\n",
        "      rewards = rewards[:-1]\n",
        "      s = states[-1]\n",
        "      states = states[:-1]\n",
        "      a = actions[-1]\n",
        "      actions = actions[:-1]\n",
        "      # We only update Q(s, a) for the first occurence of the pair in the\n",
        "      # trajectory.\n",
        "      update_q = True\n",
        "      for i in range(len(states)):\n",
        "        if s == states[i] and a == actions[i]:\n",
        "          update_q = False\n",
        "          break\n",
        "      if update_q:\n",
        "        returns[s, a] += G\n",
        "        counts[s, a] += 1\n",
        "        Q_hat[s, a] = returns[s, a] / counts[s, a]\n",
        "        pi_hat[s] = onp.zeros(na)\n",
        "        pi_hat[s, onp.argmax(Q_hat[s])] = 1.\n",
        "      step -= 1\n",
        "    iteration_returns.append(G)\n",
        "    V_hat = onp.max(Q_hat, axis=1)\n",
        "    max_errors.append(onp.max(onp.abs(V - V_hat)))\n",
        "    avg_errors.append(onp.mean(onp.abs(V - V_hat)))\n",
        "  return V_hat, Q_hat, iteration_returns, max_errors, avg_errors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDBfq13tdI_D"
      },
      "source": [
        "### Q-learning\n",
        "Although Monte Carlo methods can update value estimates based on interactions with the environment, a more common approach in reinforcement learning is to use _Temporal-Difference_ (TD) methods. This approach combines ideas from Monte Carlo estimation and dynamic programming.\n",
        "\n",
        "Like Monte Carlo methods, Q-learning updates its estimates from sampled experiences; but like dynamic programming methods, it does so with _single-step_ transitions. In its simplest form, after performing action $a$ from state $s$ and observing reward $r$ and next state$s'$, Q-learning updates its estimate of $V^{\\pi}(s)$ as follows:\n",
        "\n",
        "$Q^{\\pi}(s, a) = V^{\\pi}(s) + \\alpha\\left[ r + \\gamma V^{\\pi}(s') - V^{\\pi}(s)\\right]$\n",
        "\n",
        "Here, $\\alpha\\in[0, 1]$ is the _step size_, and determines how aggressively we will update our estimates given new evidence from the environment.\n",
        "\n",
        "In this simplified setting we are assuming $\\alpha$ remains fixed once selected, but there are more sophisticated methods which varies it throughout training. The algorithm proceeds as follows:\n",
        "\n",
        "1.  Initialize $Q$ and $\\pi$, and pick a start state $s$.\n",
        "1.  While learning:\n",
        "   1.  Pick action $a$ according to $\\pi$ (and any exploratory policy).\n",
        "   1.  Send $a$ to the environment and receive $s'$ and $r$ in return.\n",
        "   1.  Compute the TD-error as:<br>\n",
        "   $\\delta = r + \\gamma \\max_{a'\\in\\mathcal{A}}Q(s', a') - Q(s, a)$\n",
        "   1.  Update the estimate for $Q(s, a)$ as follows:<br>\n",
        "   $Q(s, a) = Q(s, a) + \\alpha\\delta$\n",
        "   1.  $\\pi(s) = \\arg\\max_{a\\in\\mathcal{A}} Q(s, a)$\n",
        "   1.  Update $s = s'$.\n",
        "\n",
        "The next cell provides an implementation of Q-learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHMkz8wVg2pq"
      },
      "source": [
        "def q_learning(ns, na, step_fn, gamma, start_state, reset_state, total_episodes,\n",
        "               max_steps_per_iteration, epsilon, alpha, V):\n",
        "  \"\"\"A simple implementation of Q-learning.\n",
        "\n",
        "  Args:\n",
        "    ns: int, the number of states.\n",
        "    na: int, the number of actions.\n",
        "    step_fn: a function that receives a state and action, and returns a float\n",
        "      (reward) and next state. This represents the interaction with the\n",
        "      environment.\n",
        "    gamma: float, the discount factor.\n",
        "    start_state: int, index of starting state.\n",
        "    reset_state: int, index of state where environment resets back to start\n",
        "      state, or None if there is no reset state.\n",
        "    total_episodes: int, total number of episodes.\n",
        "    max_steps_per_iteration: int, maximum number of steps per iteration.\n",
        "    epsilon: float, exploration rate.\n",
        "    alpha: float, learning rate.\n",
        "    V: numpy array, true V* used for computing errors. Shape: [num_states].\n",
        "  \n",
        "  Returns:\n",
        "    V_hat: numpy array, learned value function. Shape: [num_states].\n",
        "    Q_hat: numpy array, learned Q function. Shape: [num_states, num_actions].\n",
        "    max_errors: list of floats, contains the error max_s |V*(s) - \\hat{V}*(s)|.\n",
        "    avg_errors: list of floats, contains the error avg_s |V*(s) - \\hat{V}*(s)|.\n",
        "  \"\"\"\n",
        "  # Initialize policy randomly.\n",
        "  pi_hat = onp.zeros((ns, na))\n",
        "  for s in range(ns):\n",
        "    pi_hat[s, onp.random.choice(na)] = 1.\n",
        "  # Initialize Q to zeros.\n",
        "  Q_hat = onp.zeros((ns, na))\n",
        "  # Lists collected for each trajectory.\n",
        "  iteration_returns = []\n",
        "  max_errors = []\n",
        "  avg_errors = []\n",
        "  for episode in range(total_episodes):\n",
        "    # Each episode begins in the same start state.\n",
        "    s = start_state\n",
        "    step = 0\n",
        "    num_episodes = 0\n",
        "    steps_in_episode = 0\n",
        "    cumulative_return = 0.\n",
        "    average_episode_returns = 0.\n",
        "    # Interact with the environment for a maximum number of steps\n",
        "    while step < max_steps_per_iteration:\n",
        "      a = epsilon_greedy(s, pi_hat, epsilon)  # Pick action.\n",
        "      r, s2 = step_fn(s, a)  # Take a step in the environment.\n",
        "      delta = r + gamma * max(Q_hat[s2]) - Q_hat[s, a]  # TD-error.\n",
        "      Q_hat[s, a] += alpha * delta  # Q-learning update.\n",
        "      cumulative_return += gamma**(steps_in_episode) * r\n",
        "      pi_hat[s] = onp.zeros(na)\n",
        "      pi_hat[s, onp.argmax(Q_hat[s])] = 1.\n",
        "      s = s2\n",
        "      steps_in_episode += 1\n",
        "      if s2 == reset_state:\n",
        "        s = 0\n",
        "        num_episodes += 1\n",
        "        steps_in_episode = 0\n",
        "        average_episode_returns += cumulative_return\n",
        "        cumulative_return = 0.\n",
        "      step += 1\n",
        "    average_episode_returns /= max(1, num_episodes)\n",
        "    iteration_returns.append(average_episode_returns)\n",
        "    V_hat = onp.max(Q_hat, axis=1)\n",
        "    max_errors.append(onp.max(onp.abs(V - V_hat)))\n",
        "    avg_errors.append(onp.mean(onp.abs(V - V_hat)))\n",
        "  return V_hat, Q_hat, iteration_returns, max_errors, avg_errors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVrlWe2YPpXt"
      },
      "source": [
        "# Chain MDP example\n",
        "In this section we will explore the concepts introduced above with a simple chain MDP defined as follows.\n",
        "*  The agent starts in the leftmost state in the chain and can either move \"left\" (red arrows below) into a sink state and receive a small reward, or move \"right\" (blue arrows below) into the next state and incur a penalty.\n",
        "*  At each intermediate state in the chain, moving left or right incurs the same penalty.\n",
        "*  In the rightmost state of the chain the agent can move \"right\" into the sink state and receive a large reward, or move \"left\" and incur a penalty.\n",
        "*  All transitions have a probability $\\rho$ of slipping and staying in the same state.\n",
        "\n",
        "Depending on the values of the following parameters, the resulting values for $V^*$, $Q^*$, $\\pi^*$, and how well the agent learns, will vary:\n",
        "*  Length of the chain\n",
        "*  Penalty and rewards\n",
        "*  Discount factor $\\gamma$\n",
        "*  Slippage amount $\\rho$\n",
        "*  Learning rate $\\alpha$\n",
        "*  Exploration rate $\\epsilon$\n",
        "*  Number of episodes to train, and maximum number of steps per episode.\n",
        "\n",
        "Play around with the sliders and see how things change. Does it match your intuition?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgwY6LMIPbG_",
        "cellView": "form"
      },
      "source": [
        "# @title Chain MDP dynamics\n",
        "# Set up the length of the chain\n",
        "chain_length = 6  # @param {type:'slider', min:3, max:10, step:1}\n",
        "penalty = -1  # @param {type: 'number'}\n",
        "small_reward = 0  # @param {type: 'number'}\n",
        "large_reward = 10  # @param {type: 'number'}\n",
        "\n",
        "### Draw the chain MDP ###\n",
        "styles = {\n",
        "    'state':  {'shape': 'circle', 'style': 'filled', 'color': 'lightblue'},\n",
        "    'sink':  {'shape': 'circle', 'style': 'filled', 'color': 'grey'},\n",
        "}\n",
        "chain_mdp = Digraph(engine='neato')\n",
        "for i in range(chain_length):\n",
        "  ypos = 0 if i > 0 and i < (chain_length - 1) else -0.5\n",
        "  chain_mdp.node(f's{i}', f's{i}', styles['state'], pos=f'{i*2},{ypos}!')\n",
        "chain_mdp.node('sink', 'sink', styles['sink'], pos=f'{chain_length},-5')\n",
        "for i in range(chain_length):\n",
        "  l_node = f's{i - 1}' if i > 0 else 'sink'\n",
        "  reward = small_reward if l_node == 'sink' else penalty\n",
        "  chain_mdp.edge(f's{i}', l_node, f'{reward}', color='red')\n",
        "  r_node = f's{i + 1}' if i < (chain_length - 1) else 'sink'\n",
        "  reward = large_reward if r_node == 'sink' else penalty\n",
        "  chain_mdp.edge(f's{i}', r_node, f'{reward}', color='blue')\n",
        "chain_mdp.graph_attr['splines'] = 'true'\n",
        "chain_mdp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FygFyseHhdDY",
        "cellView": "form"
      },
      "source": [
        "# @title Compute Q* and V* with value iteration\n",
        "gamma = 0.95  # @param {type:'slider', min:0, max:0.95, step:0.05}\n",
        "rho = 0.1  # @param {type:'slider', min:0, max:1.0, step:0.05}\n",
        "### Compute V* and Q* ###\n",
        "# We only have two actions (L and R). The following lines create the\n",
        "# deterministic chain dynamics for left and right actions, where the last state\n",
        "# is an absorbing state.\n",
        "P = onp.zeros((chain_length + 1, 2, chain_length + 1))\n",
        "P[:, 0] =  onp.roll(onp.eye(chain_length + 1) * (1 - rho), -1, axis=1)\n",
        "P[chain_length, 0] = onp.roll(P[chain_length, 0], 1)  # Sink loops on itself.\n",
        "P[:, 0] += onp.eye(chain_length + 1) * rho\n",
        "P[:, 1] =  onp.roll(onp.eye(chain_length + 1) * (1 - rho), 1, axis=1)\n",
        "P[chain_length, 1] = onp.roll(P[chain_length, 1], -1)  # Sink loops on itself.\n",
        "P[:, 1] += onp.eye(chain_length + 1) * rho\n",
        "# The rewards will be `penalty` everywhere, except in transitions to the sink\n",
        "# state, where they will be `small_reward` for the leftmost move and \n",
        "# `large_reward` for the rightmost move.\n",
        "# The sink state receives reward of 0 for both actions.\n",
        "R = onp.ones((chain_length + 1, 2)) * penalty\n",
        "R[0, 0] = small_reward\n",
        "R[chain_length - 1, 1] = large_reward\n",
        "R[chain_length] = 0\n",
        "\n",
        "# Now run value iteration to compute V* and Q* exactly.\n",
        "V, Q = value_iteration(P, R, gamma)\n",
        "\n",
        "solved_mdp = Digraph(engine='neato')\n",
        "for i in range(chain_length):\n",
        "  ypos = 0 if i > 0 and i < (chain_length - 1) else -0.5\n",
        "  solved_mdp.node(f's{i}', f'{V[i]:.2f}', styles['state'], pos=f'{i*2},{ypos}!')\n",
        "solved_mdp.node('sink', f'{V[chain_length]:.2f}', styles['sink'], pos=f'{chain_length},-5')\n",
        "for i in range(chain_length):\n",
        "  a = onp.argmax(Q[i])\n",
        "  l_node = f's{i - 1}' if i > 0 else 'sink'\n",
        "  r_node = f's{i + 1}' if i < (chain_length - 1) else 'sink'\n",
        "  l_reward = small_reward if l_node == 'sink' else penalty\n",
        "  r_reward = large_reward if r_node == 'sink' else penalty\n",
        "  if a == 0:\n",
        "    color = 'red'\n",
        "    next_node = l_node\n",
        "    reward = l_reward\n",
        "  else:\n",
        "    color = 'blue'\n",
        "    next_node = r_node\n",
        "    reward = r_reward\n",
        "  solved_mdp.edge(f's{i}', next_node, f'{reward}', color=color)\n",
        "solved_mdp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "zQQgqKhOF-In"
      },
      "source": [
        "# @title Compute Q* and V* with policy iteration\n",
        "### Compute V* and Q* ###\n",
        "# We only have two actions (L and R). The following lines create the\n",
        "# deterministic chain dynamics for left and right actions, where the last state\n",
        "# is an absorbing state.\n",
        "P = onp.zeros((chain_length + 1, 2, chain_length + 1))\n",
        "P[:, 0] =  onp.roll(onp.eye(chain_length + 1), -1, axis=1)\n",
        "P[chain_length, 0] = onp.roll(P[chain_length, 0], 1)  # Sink loops on itself.\n",
        "P[:, 1] =  onp.roll(onp.eye(chain_length + 1), 1, axis=1)\n",
        "P[chain_length, 1] = onp.roll(P[chain_length, 1], -1)  # Sink loops on itself.\n",
        "# The rewards will be `penalty` everywhere, except in transitions to the sink\n",
        "# state, where they will be `small_reward` for the leftmost move and \n",
        "# `large_reward` for the rightmost move.\n",
        "# The sink state receives reward of 0 for both actions.\n",
        "R = onp.ones((chain_length + 1, 2)) * penalty\n",
        "R[0, 0] = small_reward\n",
        "R[chain_length - 1, 1] = large_reward\n",
        "R[chain_length] = 0\n",
        "\n",
        "# Now run value iteration to compute V* and Q* exactly.\n",
        "V_pol, Q_pol = policy_iteration(P, R, gamma)\n",
        "\n",
        "solved_mdp = Digraph(engine='neato')\n",
        "for i in range(chain_length):\n",
        "  ypos = 0 if i > 0 and i < (chain_length - 1) else -0.5\n",
        "  solved_mdp.node(f's{i}', f'{V[i]:.2f}', styles['state'], pos=f'{i*2},{ypos}!')\n",
        "solved_mdp.node('sink', f'{V_pol[chain_length]:.2f}', styles['sink'],\n",
        "                pos=f'{chain_length},-5')\n",
        "for i in range(chain_length):\n",
        "  a = onp.argmax(Q_pol[i])\n",
        "  l_node = f's{i - 1}' if i > 0 else 'sink'\n",
        "  r_node = f's{i + 1}' if i < (chain_length - 1) else 'sink'\n",
        "  l_reward = small_reward if l_node == 'sink' else penalty\n",
        "  r_reward = large_reward if r_node == 'sink' else penalty\n",
        "  if a == 0:\n",
        "    color = 'red'\n",
        "    next_node = l_node\n",
        "    reward = l_reward\n",
        "  else:\n",
        "    color = 'blue'\n",
        "    next_node = r_node\n",
        "    reward = r_reward\n",
        "  solved_mdp.edge(f's{i}', next_node, f'{reward}', color=color)\n",
        "solved_mdp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZi6oFvcHXx6"
      },
      "source": [
        "### Interacting with the environment\n",
        "In the next code cell we define a `step` function that receives a state and action (provided by the agent), and returns a reward and a next state (provided by the environment). This `step` function will be used in the learning experiments below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8OMKkTzH6UU"
      },
      "source": [
        "ns = P.shape[0]\n",
        "na = P.shape[1]\n",
        "def step_fn(s, a):\n",
        "  \"\"\"Receives a state and action, returns reward and next state.\"\"\"\n",
        "  return R[s, a], onp.random.choice(ns, p=P[s,a])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKXIpKJ4fcC9",
        "cellView": "form"
      },
      "source": [
        "# @title Monte Carlo on the chain MDP\n",
        "epsilon = 0.1  # @param {type:'slider', min:0, max:1.0, step:0.05}\n",
        "total_trajectories = 50000 # @param {type:'slider', min:100, max:100000}\n",
        "max_steps_per_trajectory = 50 # @param {type:'slider', min:10, max:200}\n",
        "V_MC, Q_MC, iteration_returns_MC, max_errors_MC, avg_errors_MC = monte_carlo(\n",
        "    ns,\n",
        "    na,\n",
        "    step_fn,\n",
        "    gamma,\n",
        "    0,  # start state\n",
        "    chain_length,  # reset state\n",
        "    total_trajectories,\n",
        "    max_steps_per_trajectory,\n",
        "    epsilon,\n",
        "    V)  # True V*\n",
        "fig, axes = plt.subplots(1, 3, figsize=(30, 7))\n",
        "axes[0].plot(onp.arange(len(iteration_returns_MC)), iteration_returns_MC)\n",
        "axes[0].set_title('Average iteration returns', fontsize=22)\n",
        "axes[1].plot(onp.arange(len(max_errors_MC)), max_errors_MC)\n",
        "axes[1].set_title(r'Max $|V^* - \\hat{V}^*|$', fontsize=22)\n",
        "axes[2].plot(onp.arange(len(avg_errors_MC)), avg_errors_MC)\n",
        "axes[2].set_title(r'Avg $|V^* - \\hat{V}^*|$', fontsize=22)\n",
        "\n",
        "# Draw the learned value functions\n",
        "mc_mdp = Digraph(engine='neato')\n",
        "for i in range(chain_length):\n",
        "  ypos = 0 if i > 0 and i < (chain_length - 1) else -0.5\n",
        "  mc_mdp.node(f's{i}', f'{V_MC[i]:.2f}', styles['state'], pos=f'{i*2},{ypos}!')\n",
        "mc_mdp.node('sink', f'{V_MC[chain_length]:.2f}', styles['sink'], pos=f'{chain_length},-5')\n",
        "for i in range(chain_length):\n",
        "  a = onp.argmax(Q_MC[i])\n",
        "  l_node = f's{i - 1}' if i > 0 else 'sink'\n",
        "  r_node = f's{i + 1}' if i < (chain_length - 1) else 'sink'\n",
        "  l_reward = small_reward if l_node == 'sink' else penalty\n",
        "  r_reward = large_reward if r_node == 'sink' else penalty\n",
        "  if a == 0:\n",
        "    color = 'red'\n",
        "    next_node = l_node\n",
        "    reward = l_reward\n",
        "  else:\n",
        "    color = 'blue'\n",
        "    next_node = r_node\n",
        "    reward = r_reward\n",
        "  mc_mdp.edge(f's{i}', next_node, f'{reward}', color=color)\n",
        "mc_mdp\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngKodUZNkpXp",
        "cellView": "form"
      },
      "source": [
        "# @title Q-learning on the chain MDP\n",
        "alpha = 0.1  # @param {type:'slider', min:0, max:1.0, step:0.05}\n",
        "epsilon = 0.1  # @param {type:'slider', min:0, max:1.0, step:0.05}\n",
        "total_episodes = 600 # @param {type:'slider', min:100, max:10000}\n",
        "max_steps_per_iteration = 200 # @param {type:'slider', min:10, max:200}\n",
        "V_hat, Q_hat, iteration_returns, max_errors, avg_errors = q_learning(\n",
        "    ns,\n",
        "    na,\n",
        "    step_fn,\n",
        "    gamma,\n",
        "    0,  # start state\n",
        "    chain_length,  # reset state\n",
        "    total_episodes,\n",
        "    max_steps_per_iteration,\n",
        "    epsilon,\n",
        "    alpha,\n",
        "    V)  # True V*\n",
        "fig, axes = plt.subplots(1, 3, figsize=(30, 7))\n",
        "axes[0].plot(onp.arange(len(iteration_returns)), iteration_returns)\n",
        "axes[0].set_title('Average iteration returns', fontsize=22)\n",
        "axes[1].plot(onp.arange(len(max_errors)), max_errors)\n",
        "axes[1].set_title(r'Max $|V^* - \\hat{V}^*|$', fontsize=22)\n",
        "axes[2].plot(onp.arange(len(avg_errors)), avg_errors)\n",
        "axes[2].set_title(r'Avg $|V^* - \\hat{V}^*|$', fontsize=22)\n",
        "\n",
        "# Draw the learned value functions\n",
        "learned_mdp = Digraph(engine='neato')\n",
        "for i in range(chain_length):\n",
        "  ypos = 0 if i > 0 and i < (chain_length - 1) else -0.5\n",
        "  learned_mdp.node(f's{i}', f'{V_hat[i]:.2f}', styles['state'], pos=f'{i*2},{ypos}!')\n",
        "learned_mdp.node('sink', f'{V_hat[chain_length]:.2f}', styles['sink'], pos=f'{chain_length},-5')\n",
        "for i in range(chain_length):\n",
        "  a = onp.argmax(Q_hat[i])\n",
        "  l_node = f's{i - 1}' if i > 0 else 'sink'\n",
        "  r_node = f's{i + 1}' if i < (chain_length - 1) else 'sink'\n",
        "  l_reward = small_reward if l_node == 'sink' else penalty\n",
        "  r_reward = large_reward if r_node == 'sink' else penalty\n",
        "  if a == 0:\n",
        "    color = 'red'\n",
        "    next_node = l_node\n",
        "    reward = l_reward\n",
        "  else:\n",
        "    color = 'blue'\n",
        "    next_node = r_node\n",
        "    reward = r_reward\n",
        "  learned_mdp.edge(f's{i}', next_node, f'{reward}', color=color)\n",
        "learned_mdp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rAUWD2jmQxB"
      },
      "source": [
        "# Deep Reinforcement Learning\n",
        "Although useful for developing a better understanding for MDPs and reinforcement learning, the chain MDP is a tiny problem. One of the main reasons why reinforcement learning has gained much popularity is because of the impressive learning abilities when coupled with neural networks. This is what is meant by _deep reinforcement learning_: RL algorithms combined with (deep) neural networks.\n",
        "\n",
        "Mathematically, one can think of neural networks as a function approximator $\\hat{Q}_\\theta$ parameterized by a vector of weights $\\theta$. The aim of deep RL is to learn the parameters $\\theta$, via interacting with the environment, such that $\\hat{Q}_\\theta (s) \\approx Q^*(s,\\cdot)$.\n",
        "\n",
        "Note that the the learned function approximator $\\hat{Q}_{\\theta}(s)$ can provide an estimate for all states $s\\in\\mathcal{S}$, _even those it has never seen before_. If the agent learns a good function approximator, then this will be able to _generalize_ to unseen states in the environment. More specifically, the error $|Q(s, \\cdot) - \\hat{Q}_\\theta(s)|$ will not be too large.\n",
        "\n",
        "This is in stark contrast to the tabular methods we discussed above, where the agent _must_ visit all states in order to update its estimates for them. This ability to generalize is part of what has granted deep reinforcement learning the ability to tackle such challenging problems.\n",
        "\n",
        "This idea has been around for quite some time: Gerald Tesauro used it back in 1992 to train an agent to play backgammon (you can read the original paper [here](https://cling.csd.uwo.ca/cs346a/extra/tdgammon.pdf)).\n",
        "\n",
        "More recently, this idea was used on the [Arcade Learning Environment](https://jair.org/index.php/jair/article/view/10819) (a benchmark suite of 60 Atari 2600 games) in the 2015 paper [Human-level control through deep reinforcement learning](https://www.nature.com/articles/nature14236) by Mnih et al. In this paper the authors introduced the DQN algorithm. The algorithm trains a neural network $\\hat{Q}_\\theta$ that receives as input an array of pixels (a frame in an Atari game), and outputs value estimates for each possible action.\n",
        "\n",
        "The network parameters in DQN are trained using a _loss function_ that is inspired in the Q-learning update where, given a transition $(s,a)\\rightarrow (s',r')$:\n",
        "\n",
        "$\\mathcal{L}(\\theta) = \\left(R(s, a) +\\gamma\\max_{a'\\in\\mathcal{A}}\\hat{Q}_{\\theta}(s', a') - \\hat{Q}_{\\theta}(s, a)\\right)^2$\n",
        "\n",
        "Just as the temporal difference update is at the heart of Q-learning, the above loss is at the heart of DQN.\n",
        "\n",
        "There are a number of technical details that were necessary in order to properly train DQN to perform well on most of the 60 Atari 2600 games. These details are beyond the scope of this notebook, but the interested reader is referred to the paper and to some of the resources listed below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XD5HXADzzfjg"
      },
      "source": [
        "## Experience replay buffer\n",
        "One of the core elements of the DQN paper was the use of the experience replay buffer. You can think of this as a large memory where the agent stores experienced transitions, so that the agent can sample from them when learning.\n",
        "\n",
        "Specifically, after each _step_ in the environment, the agent has an experience transition of the form $\\langle s, a, r, s'\\rangle$, and it will store this in the replay buffer.\n",
        "\n",
        "When training, the agent will not use a single transition, but will sample a _batch_ of transitions from the replay buffer. The sampling in DQN is done randomly; that is, if the replay buffer is of size $N$, each stored transition has a probability $\\frac{1}{N}$ of being sampled.\n",
        "\n",
        "The loss function introduced above was defined in terms of a single transition, but it should really be defined in terms of a sampled batch of transitions:\n",
        "\n",
        "$\\mathcal{L}(\\theta) = \\mathbb{E}_{\\langle s,a,r,s'\\rangle\\sim U(D)}\\left[\\left(R(s, a) +\\gamma\\max_{a'\\in\\mathcal{A}}\\hat{Q}_{\\theta}(s', a') - \\hat{Q}_{\\theta}(s, a)\\right)^2\\right]$\n",
        "\n",
        "Where $D$ represents the replay buffer and $U(D)$ represents a uniform distribution over the elements in the replay buffer.\n",
        "\n",
        "Sampling batches in this way also allows us to take advantage of specialized hardware such as GPUs and TPUs, which can speed up training tremendously."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGF-itDf-NJR"
      },
      "source": [
        "## Final implementation details\n",
        "There are more implementation details that are beyond the scope of this notebook. The interested reader can go through the full implementations in the dopamine library:\n",
        "*  Dopamine provides both [TensorFlow](https://github.com/google/dopamine/blob/master/dopamine/discrete_domains/atari_lib.py) and [Jax](https://github.com/google/dopamine/blob/master/dopamine/jax/networks.py) implementations of the DQN network.\n",
        "*  Dopamine provides both [TensorFlow](https://github.com/google/dopamine/blob/master/dopamine/agents/dqn/dqn_agent.py) and [Jax](https://github.com/google/dopamine/blob/master/dopamine/jax/agents/dqn/dqn_agent.py) implementations of DQN. We will be using the Jax implementation in this notebook.\n",
        "*  The replay buffer is implemented [here](https://github.com/google/dopamine/blob/master/dopamine/replay_memory/circular_replay_buffer.py).\n",
        "*  The setup for running a full experiment (environment interaction, reward collection, etc.) is implemented [here](https://github.com/google/dopamine/blob/master/dopamine/discrete_domains/run_experiment.py)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hxErVdl_W95"
      },
      "source": [
        "# CartPole example\n",
        "Training an agent on an Atari game can take multiple days, even on a GPU. Thus, we will focus on a simpler problem: CartPole.\n",
        "\n",
        "In this problem there is a cart (black block below) attached to a railing (horizontal line), upon which there is a pole (brown rectangle) attached to the cart with a hinge (purple circle). The goal of the agent is to maintain the pole vertically balanced by moving the cart left or right. An episode ends as soon as the pole goes beyond a certain angle.\n",
        "\n",
        "The agent receives a reward of `1` for each step that it is able to maintain the pole balanced, and `0` otherwise.\n",
        "\n",
        "The animation below illustrates a trained agent successfully balancing the pole.\n",
        "\n",
        "![CartPole Illustration](https://user-images.githubusercontent.com/10624937/42135683-dde5c6f0-7d13-11e8-90b1-8770df3e40cf.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgRIbbu9R83W"
      },
      "source": [
        "상태(State) 설명 :\n",
        "매 스텝마다 관측할 수 있는 환경의 정보는 4개로 구성되어 있고 요소는 다음과 같습니다.\n",
        "\n",
        "- 카트의 위치\n",
        "- 카트의 속력\n",
        "- 막대기의 각도\n",
        "- 막대기의 끝부분(상단) 속도\n",
        "\n",
        "행동(Action) 설명 : \n",
        "매 스텝마다 0, 1의 값을 통해 카트를 좌, 우로 조종할 수 있습니다.\n",
        "\n",
        "보상(Reward) 설명 : \n",
        "매 스텝마다 카트가 중심을 기준으로 일정 범위 안에 있고, 막대기가 넘어지지 않으면 +1의 보상을 받습니다. 즉 막대기의 중심을 잘 잡아 넘어 뜨리지 않고 오래 버티면 성공하는 게임입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcZJug7m-bbr"
      },
      "source": [
        "Open-AI provides the [gym](https://github.com/openai/gym) library, which gives us access to a number of different environments. Creating a cartpole environment can be done with:\n",
        "\n",
        "```\n",
        "import gym\n",
        "cartpole_env = gym.make('CartPole-v1')\n",
        "```\n",
        "\n",
        "The main API methods we will need are:\n",
        "*  `step`: Receives an action (`int`) and returns 4 objects: the next state, the reward, a boolean indicating whether the environment has ended, and any extra info.\n",
        "*  `reset`: Reset the environment, returns the initial observation.\n",
        "*  `render`: Returns an object for rendering, depending on the `mode` parameter passed in. \n",
        "\n",
        "See [here](https://github.com/openai/gym/blob/3bd5ef71c2ca3766a26c3dacf87a33a9390ce1e6/gym/core.py) for more details on this API."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJleQbZ6-l7y"
      },
      "source": [
        "import gym\n",
        "cartpole_env = gym.make('CartPole-v1')\n",
        "cartpole_env.reset()\n",
        "# This will render the starting state of the environment.\n",
        "plt.imshow(cartpole_env.render(mode='rgb_array'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5JPZzTPI2oQ",
        "cellView": "form"
      },
      "source": [
        "# @title Animation utilities (run me!)\n",
        "from matplotlib import animation\n",
        "\n",
        "def animate_agent(agent, env, num_frames=100):\n",
        "  s = env.reset()\n",
        "  fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "  im = axes[0].imshow(env.render(mode='rgb_array'))\n",
        "  frames = [env.render(mode='rgb_array')]\n",
        "  returns = [0]\n",
        "  env_active = True\n",
        "  for step in range(num_frames):\n",
        "    a = agent(s)\n",
        "    s, r, done, _ = env.step(a)\n",
        "    frames.append(env.render(mode='rgb_array'))\n",
        "    returns.append(r + returns[-1])\n",
        "    if env_active and done:\n",
        "      env_active = False\n",
        "      print(f'Game over! Your agent lasted {step} steps.')\n",
        "  axes[1].set_title('Cumulative returns', fontsize=20)\n",
        "  axes[1].set_xlim(0, num_frames)\n",
        "  axes[1].set_ylim(0, max(returns) * 1.2)\n",
        "  line, = axes[1].plot([], [], lw=2)\n",
        "\n",
        "  def init():\n",
        "    line.set_data([], [])\n",
        "    im.set_data(frames[0])\n",
        "    return [im]\n",
        "  \n",
        "  def animate(i):\n",
        "    line.set_data(onp.arange(i), returns[:i])\n",
        "    im.set_data(frames[i])\n",
        "    return [im]\n",
        "  \n",
        "  anim = animation.FuncAnimation(fig, animate, init_func=init, frames=num_frames,\n",
        "                                 interval=50)\n",
        "  plt.close()\n",
        "  return HTML(anim.to_jshtml())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YOMrK_jFk9d"
      },
      "source": [
        "## Controlling cartpole with a \"blind\" policy\n",
        "We can control cartpole by encoding a \"blind\" policy that always chooses the _left_ action. This is indicated in the next code cell.\n",
        "\n",
        "These types of policies are known as _open-loop_ policies (in contrast to _closed-loop_ policies, which are the ones we've been discussing where the action choice is conditional on the agent's state).\n",
        "\n",
        "We have created a utility `animate_agent` function that receives a function which receives a state and returns an action (the agent), and a Gym environment,\n",
        "and will generate an animation of the environment as well as the cumulative returns received.\n",
        "\n",
        "Notice that with this poor open-loop policy the agent is not able to achieve a very high cumulative reward!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xS57dFVnFjCP"
      },
      "source": [
        "def only_left(unused_s):\n",
        "  # This will only return the 'left' action, ignoring state.\n",
        "  return 0\n",
        "\n",
        "animate_agent(only_left, cartpole_env, num_frames=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlXR9gkRVgZl"
      },
      "source": [
        "## Training a CartPole agent with dopamine\n",
        "We are now ready to train our first deep RL agent!\n",
        "\n",
        "Here is the CartPole DQN network as provided by dopamine.\n",
        "\n",
        "```\n",
        "class CartpoleDQNNetwork(nn.Module):\n",
        "  \"\"\"Jax DQN network for Cartpole.\"\"\"\n",
        "\n",
        "  def apply(self, x, num_actions):\n",
        "    initializer = nn.initializers.xavier_uniform()\n",
        "    # We need to add a \"batch dimension\" as nn.Conv expects it, yet vmap will\n",
        "    # have removed the true batch dimension.\n",
        "    x = x[None, ...]\n",
        "    x = x.astype(jnp.float32)\n",
        "    x = x.reshape((x.shape[0], -1))  # flatten\n",
        "    x -= gym_lib.CARTPOLE_MIN_VALS\n",
        "    x /= gym_lib.CARTPOLE_MAX_VALS - gym_lib.CARTPOLE_MIN_VALS\n",
        "    x = 2.0 * x - 1.0  # Rescale in range [-1, 1].\n",
        "    x = nn.Dense(x, features=512, kernel_init=initializer)\n",
        "    x = jax.nn.relu(x)\n",
        "    x = nn.Dense(x, features=512, kernel_init=initializer)\n",
        "    x = jax.nn.relu(x)\n",
        "    q_values = nn.Dense(x, features=num_actions, kernel_init=initializer)\n",
        "    return q_values\n",
        "```\n",
        "\n",
        "The next code cell creates a dopamine DQN agent with the network specified above.\n",
        "\n",
        "Dopamine uses a python configuration framework called [gin](https://github.com/google/gin-config) that makes it easy to specify hyperparameters for all the objects in your system (agent, replay buffer, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J48LPLdvB13Q"
      },
      "source": [
        "import dopamine"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75PxY9kdCgjd"
      },
      "source": [
        "dir(dopamine.jax.networks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pObWRplWmva6"
      },
      "source": [
        "from dopamine.jax import networks\n",
        "from dopamine.jax.agents.dqn import dqn_agent\n",
        "import gin\n",
        "\n",
        "cartpole_config = \"\"\"\n",
        "JaxDQNAgent.gamma = 0.99\n",
        "JaxDQNAgent.update_horizon = 1\n",
        "JaxDQNAgent.min_replay_history = 500\n",
        "JaxDQNAgent.update_period = 4\n",
        "JaxDQNAgent.target_update_period = 100\n",
        "JaxDQNAgent.epsilon_fn = @dqn_agent.identity_epsilon\n",
        "\n",
        "create_optimizer.name = 'adam'\n",
        "create_optimizer.learning_rate = 0.001\n",
        "create_optimizer.eps = 3.125e-4\n",
        "\n",
        "OutOfGraphReplayBuffer.replay_capacity = 50000\n",
        "OutOfGraphReplayBuffer.batch_size = 128\n",
        "\"\"\"\n",
        "gin.parse_config(cartpole_config, skip_unknown=False)\n",
        "\n",
        "dqn_agent = dqn_agent.JaxDQNAgent(num_actions=cartpole_env.action_space.n,\n",
        "                                  observation_shape=(4, 1),\n",
        "                                  observation_dtype=jnp.float64,\n",
        "                                  stack_size=1,\n",
        "                                  network=networks.ClassicControlDQNNetwork)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZTb36k60pH8"
      },
      "source": [
        "## Animate a newly initialized DQN agent\n",
        "First we will animate the agent interacting with CartPole before it has learnt anything. This should be roughly a purely random policy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QILZRoHS0mbM"
      },
      "source": [
        "def learned_policy(s):\n",
        "  return dqn_agent.step(0., s)  # We pass in a dummy reward\n",
        "\n",
        "# We set our agent in `eval_mode` to avoid it from continuing to train while\n",
        "# interacting with the environment.\n",
        "dqn_agent.eval_mode = True\n",
        "animate_agent(learned_policy, cartpole_env, num_frames=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIL6PjicqTid"
      },
      "source": [
        "Now we train the agent! This is a slimmed-down version of what is done in the dopamine [run_experiment](https://github.com/google/dopamine/blob/master/dopamine/discrete_domains/run_experiment.py) library.\n",
        "\n",
        "Training proceeds by a sequence of _iterations_, where each iteration consists of one or more _episodes_. Specifically, the parameters that control the training procedure are:\n",
        "*  `max_steps_per_episode`: this is the maximum number of allowable steps in an episode. Note that this then specifies the maximum possible cumulative reward that the agent can receive.\n",
        "*  `training_steps`: The minimum number of training steps in an iteration. The agent will execute as many episodes as necessary in order to achieve this minimum.\n",
        "*  `num_iterations`: The total number of iterations to execute during training. With the settings below, typically 30 iterations is enough to converge to a good policy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOIoJe3PqQTn"
      },
      "source": [
        "max_steps_per_episode = 200  # @param {type:'slider', min:10, max:1000}\n",
        "training_steps = 1000  # @param {type:'slider', min:10, max:5000}\n",
        "num_iterations = 30  # @param {type:'slider', min:10, max:200}\n",
        "\n",
        "# First remove eval mode!\n",
        "dqn_agent.eval_mode = False\n",
        "average_returns = []\n",
        "# Each iteration will consist of a number of episodes.\n",
        "for iteration in range(num_iterations):\n",
        "  step_count = 0\n",
        "  num_episodes = 0\n",
        "  sum_returns = 0.\n",
        "  # This while loop will continue running episodes until we've done enough\n",
        "  # training steps.\n",
        "  while step_count < training_steps:\n",
        "    episode_length = 0\n",
        "    episode_rewards = 0.\n",
        "    s = cartpole_env.reset()\n",
        "    a = dqn_agent.begin_episode(s)\n",
        "    is_terminal = False\n",
        "    # Run the episode until termination.\n",
        "    while True:\n",
        "      s, r, done, _ = cartpole_env.step(a)\n",
        "      episode_rewards += r\n",
        "      episode_length += 1\n",
        "      if done or episode_length == max_steps_per_episode:\n",
        "        # Stop the loop if the episode or ended or we've reached the max steps.\n",
        "        break\n",
        "      else:\n",
        "        a = dqn_agent.step(r, s)\n",
        "    dqn_agent.end_episode(r)\n",
        "    step_count += episode_length\n",
        "    sum_returns += episode_rewards\n",
        "    num_episodes += 1\n",
        "  average_return = sum_returns / num_episodes if num_episodes > 0 else 0.\n",
        "  print(f'Iteration {iteration}: average return = {average_return:.4f}')\n",
        "  average_returns.append(average_return)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGhkR__T1se9"
      },
      "source": [
        "## Animate trained DQN agent\n",
        "Now we can animate our trained agent. If training was successful, this should be animating a well-balanced pole!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ON-ISrk1y1T6"
      },
      "source": [
        "def learned_policy(s):\n",
        "  return dqn_agent.step(0., s)  # We pass in a dummy reward\n",
        "\n",
        "# We set our agent in `eval_mode` to avoid it from continuing to train while\n",
        "# interacting with the environment.\n",
        "dqn_agent.eval_mode = True\n",
        "animate_agent(learned_policy, cartpole_env, num_frames=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2HtXduiyoyP"
      },
      "source": [
        "# Further reading\n",
        "I hope you found this notebook useful. Here are some resources to continue your voyage into the depths of reinforcement learning!\n",
        "\n",
        "*  The canonical reference for reinforcement learning is the [Reinforcement Learning book by Sutton and Barto](http://www.incompleteideas.net/book/the-book-2nd.html).\n",
        "*  The Open-AI [spinning up in deep RL](https://spinningup.openai.com/en/latest/)\n",
        "*  The [dopamine repository](https://github.com/google/dopamine)\n",
        "*  [Temporal Difference Learning and TD-Gammon](https://cling.csd.uwo.ca/cs346a/extra/tdgammon.pdf) by Gerald Tesauro\n",
        "*  [Human-level control through deep reinforcement learning](https://www.nature.com/articles/nature14236) by Mnih et al. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXjVmeBorYuc"
      },
      "source": [
        "# Contact\n",
        "If you have any comments or suggestions on this notebook, the best way to contact me is via [twitter](https://twitter.com/pcastr)."
      ]
    }
  ]
}