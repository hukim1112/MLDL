{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Y5H90HV799Qn",
    "outputId": "350119f1-e4cc-4a6d-ee32-020712b562db"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  # Colab only\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "#import necessary libraries.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "layer = tf.keras.layers\n",
    "\n",
    "print('check tensorflow version : ', tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.data API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.data API는 데이터셋을 모델에 연결해주기 위한 복합적인 입력 파이프라인을 구축할 수 있게 도와줍니다.\n",
    "\n",
    "다수의 분산된 파일로부터 통합된 데이터를 만들어야 하는 경우나, 데이터 전처리, 미니배치, 랜덤셔플링 등의 데이터 파이프라인을 위한 복잡한 구조를 높은 추상성으로 간단하게 제어할 수 있는 인터페이스를 제공합니다.\n",
    "\n",
    "https://www.tensorflow.org/guide/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. from_tensor_slices : python으로부터 데이터 받기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(list(range(100)), np.float32) #파이썬 데이터 0,1,2,3,4,...99,100\n",
    "print(data[:10])\n",
    "\n",
    "your_first_dataset = tf.data.Dataset.from_tensor_slices(data) #텐서플로우 데이터셋 오브젝트로 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 오브젝트는 for 문에 의해 iteration 가능함.\n",
    "for elm in your_first_dataset:\n",
    "    print(elm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정상적으로 데이터를 가져오는 지 보기 위해 한 개의 데이터를 보고 싶을 수도 있습니다.\n",
    "# 첫 번째 방법(take함수를 사용해서)\n",
    "for element in your_first_dataset.take(1):\n",
    "    print(element)\n",
    "# 두 번째 방법\n",
    "print(next(iter(your_first_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 개별적으로 처리하기 위한 함수를 정의해둡니다. 이때 함수는 tensorflow의 연산자 또는 함수로 정의된 것을 사용해야 합니다.\n",
    "def multiply(element):\n",
    "    multiplied = 10*element\n",
    "    return multiplied\n",
    "\n",
    "def add(element):\n",
    "    added = element + 0.5\n",
    "    return added\n",
    "\n",
    "#데이터셋을 파이프라인으로 가져오는 중에 사전 정의한 함수에 의해 처리될 수 있습니다.\n",
    "#데이터셋 ----> map(multiply) ----> map(add) ----> batch ----> ....\n",
    "\n",
    "print(\"데이터를 map 함수를 통해 병렬적으로 처리하도록 할 수 있습니다.\")\n",
    "your_pipeline1 = your_first_dataset.map(multiply)\n",
    "for elm in your_pipeline1.take(5):\n",
    "    print(elm)\n",
    "print(\"또한 연속적으로 데이터를 처리할 수도 있습니다.\")\n",
    "your_pipeline2 = your_first_dataset.map(multiply).map(add)\n",
    "for elm in your_pipeline2.take(5):\n",
    "    print(elm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이프라인은 다음과 같이 합쳐질 수도 있습니다.\n",
    "new_pipeline = tf.data.Dataset.zip((your_pipeline1, your_pipeline2))\n",
    "#데이터셋1 ----> map(multiply) -------------------> | \n",
    "#                                                 |(zip)----> new_pipeline\n",
    "#데이터셋2 ----> map(multiply) ----> map(add) ----> |\n",
    "for elm1, elm2 in new_pipeline.take(5):\n",
    "    print(elm1, elm2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 구간을 지나서 자주 사용하는 파이프라인의 기능은 \n",
    "# cache(데이터 일부를 캐시메모리에 저장할 수 있도록 함), shuffle(데이터셋을 shuffling해서 가져옴)\n",
    "# batch(mini-batch로 데이터 가져옴), prefetch(모델이 s번째 데이터를 읽는 동안, s+1번째 데이터를 준비)\n",
    "M = 100 # space for shuffle\n",
    "N = 8 # batch size\n",
    "new_pipeline = new_pipeline.cache().shuffle(M).batch(N).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elm1, elm2 in new_pipeline.take(1):\n",
    "    print(elm1, elm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 메모리 공간 위 이미지를 가져오는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = train\n",
    "images = images/255\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. TextLineDataset : text file로부터 데이터셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_url = 'https://storage.googleapis.com/download.tensorflow.org/data/illiad/'\n",
    "file_names = ['cowper.txt', 'derby.txt', 'butler.txt']\n",
    "\n",
    "file_paths = [\n",
    "    tf.keras.utils.get_file(file_name, directory_url+file_name)\n",
    "    for file_name in file_names ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TextLineDataset(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in dataset.take(1):\n",
    "  print(line.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = dataset.shuffle(buffer_size=10000).batch(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in new_dataset.take(1):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. TextLineDataset : csv 파일로부터 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "titanic_file = tf.keras.utils.get_file(\"train.csv\", \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")\n",
    "df = pd.read_csv(titanic_file, index_col=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_slices = tf.data.Dataset.from_tensor_slices(dict(df))\n",
    "\n",
    "for feature_batch in titanic_slices.take(1):\n",
    "  for key, value in feature_batch.items():\n",
    "    print(\"  {!r:20s}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Keras_funtional_API.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
