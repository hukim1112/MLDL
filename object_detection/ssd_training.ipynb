{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NBusmtZyE1uN",
    "outputId": "a0f70153-3b8a-4461-c130-22dfea08b874"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/hukim1112/MLDL.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eDipyAkgFH8h"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import json, os, sys, time\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g182yHi0FMrB"
   },
   "outputs": [],
   "source": [
    "sys.path.append(\"/content/MLDL/object_detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aZz9RL14E2rD"
   },
   "outputs": [],
   "source": [
    "import anchor, losses, manage_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0TpMSoRkE1uU"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"exp_desc\" : \"perfection\", #실험명\n",
    "    \"sub_desc\" : \"exp1\", #하위실험명\n",
    "    \"image_dir\" : \"/content/images\", #이미지경로\n",
    "    \"coco_api\" : \"/home/dan/prj/PythonAPI\",\n",
    "    \"annotation_dir\" : \"/content/\", #어노테이션 경로\n",
    "    \"label_set\" : [\"head\",\"helmet\"], # 분류할 오브젝트 집합\n",
    "    \"input_shape\" : [160, 160, 3], #모델입력영상 크기\n",
    "    \"arch\" : \"ssd160\",\n",
    "    \"backbone\" : \"mobilenet\",\n",
    "    \"anchor_param\" : {\"ratios\": [[2], [2, 3], [2, 3], [2]],\n",
    "                           \"scales\": [0.1, 0.3, 0.6, 0.9, 1.05],\n",
    "                           \"fm_sizes\": [10, 5, 3, 1],\n",
    "                           \"image_size\": 160}, #anchor parameters\n",
    "    \"ckpt\":\n",
    "    {\n",
    "        \"save_type\" : \"best\",\n",
    "        \"max_to_keep\" : 10,\n",
    "        \"pretrained_type\" : \"init\",\n",
    "        \"model_path\" : None\n",
    "    },\n",
    "    \"inference_mode\" : \"train\", # train or mAP\n",
    "    \"train\" :\n",
    "    {\n",
    "        \"num_examples\" : -1,\n",
    "        \"batch_size\" : 16,\n",
    "        \"augmentation\" : True,\n",
    "        \"random_crop_rate\" : 0.3,\n",
    "        \"neg_ratio\" : 3,\n",
    "        \"initial_lr\" : 1e-3,\n",
    "        \"momentum\" : 0.9,\n",
    "        \"weight_decay\" : 5e-5,\n",
    "        \"num_epochs\" : 300\n",
    "    },\n",
    "    \"val\":\n",
    "    {\n",
    "        \"num_examples\" : -1,\n",
    "        \"batch_size\" : 64\n",
    "    },\n",
    "    \"test\":\n",
    "    {\n",
    "        \"num_examples\" : -1,\n",
    "        \"batch_size\" : 1\n",
    "    }\n",
    "\n",
    "}\n",
    "num_classes = len(config['label_set'])+1\n",
    "log_dir = os.path.join('logs', 'perfection', 'exp1')\n",
    "# if os.path.isdir(ckpt_dir):\n",
    "#     raise ValueError(\"checkpoint directory exists. checkout your experiment name in configure file.\")\n",
    "# if os.path.isdir(log_dir):\n",
    "#     raise ValueError(\"log directory exists. checkout your experiment name in configure file.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDoar-g5E1uV"
   },
   "source": [
    "# Dataset load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ROvJG8yqFqzh"
   },
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "# test COCO api can load my json file\n",
    "json = COCO(\"/content/GDUT_HWD.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ySIR56NFrn7",
    "outputId": "01741945-51f4-4c2e-9240-2a052929c5ca"
   },
   "outputs": [],
   "source": [
    "!ls images.zip\n",
    "!unzip images.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OVY19wH2E1uW"
   },
   "outputs": [],
   "source": [
    "from coco import Dataset\n",
    "ds_obj = Dataset(config, COCO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zWqPM4ZWE1uW",
    "outputId": "472e2c5a-8149-444d-9040-f97e9318eb9f"
   },
   "outputs": [],
   "source": [
    "batch_generator, train_length = ds_obj.load_data_generator('train', config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "151hK-4lE1uX",
    "outputId": "17fc07a5-6ea5-46e4-85ec-779c3e2c3107"
   },
   "outputs": [],
   "source": [
    "for i, (_, imgs, gt_confs, gt_locs) in enumerate(batch_generator.take(1)):\n",
    "    print(imgs.shape, gt_confs.shape, gt_locs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "as7WRt3SE1uY"
   },
   "source": [
    "# Model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YaSAOnflE1uY",
    "outputId": "4c1c7403-ca12-4af2-b405-cb512de8d31a"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"ssd160-mobilenet.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xFRRmnIiE1uY",
    "outputId": "68336eef-989c-464d-a208-44fcbf3c91a5"
   },
   "outputs": [],
   "source": [
    "model.layers[1].trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UDZLNBScPZ7S",
    "outputId": "32d2b54c-d59e-435e-cfaa-58c5c03a8838"
   },
   "outputs": [],
   "source": [
    "model(imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xE2VgOQpE1uZ"
   },
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IqW_LO49E1uZ"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(imgs, gt_confs, gt_locs, ssd, criterion, optimizer, config):\n",
    "    with tf.GradientTape() as tape:\n",
    "        confs, locs = ssd(imgs)\n",
    "\n",
    "        conf_loss, loc_loss = criterion(\n",
    "            confs, locs, gt_confs, gt_locs)\n",
    "\n",
    "        loss = conf_loss + loc_loss\n",
    "        l2_loss = [tf.nn.l2_loss(t) for t in ssd.trainable_variables]\n",
    "        l2_loss = config['train']['weight_decay'] * tf.math.reduce_sum(l2_loss)\n",
    "        loss += l2_loss\n",
    "\n",
    "    gradients = tape.gradient(loss, ssd.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, ssd.trainable_variables))\n",
    "\n",
    "    return loss, conf_loss, loc_loss, l2_loss\n",
    "\n",
    "criterion = losses.create_losses(config['train']['neg_ratio'], num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "LeedJXZ3E1uZ",
    "outputId": "dff81cd5-c4e7-43bb-c3fb-89a7b2893d0f"
   },
   "outputs": [],
   "source": [
    "summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "t_loss = tf.metrics.Mean(name='train_loss')\n",
    "t_conf_loss = tf.metrics.Mean(name='train_conf_loss')\n",
    "t_loc_loss = tf.metrics.Mean(name='train_loc_loss')\n",
    "# v_loss = tf.metrics.Mean(name='val_loss')\n",
    "# v_conf_loss = tf.metrics.Mean(name='val_conf_loss')\n",
    "# v_loc_loss = tf.metrics.Mean(name='val_loc_loss')\n",
    "\n",
    "learning_rate = 1E-3\n",
    "EPOCH = 50\n",
    "START = 0\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "for epoch in range(START+1, START+EPOCH+1):\n",
    "    start = time.time()\n",
    "    for i, (_, imgs, gt_confs, gt_locs) in enumerate(batch_generator):\n",
    "        imgs = preprocess_input(imgs)\n",
    "        loss, conf_loss, loc_loss, l2_loss = train_step(imgs, gt_confs, gt_locs, model, criterion, optimizer, config)\n",
    "        t_loss(loss)\n",
    "        t_conf_loss(conf_loss)\n",
    "        t_loc_loss(loc_loss)\n",
    "        \n",
    "        print(\"Epoch {} iteration {} loss : {}\".format(epoch, i, t_loss.result()))\n",
    "    \n",
    "    '''\n",
    "    you can add the validation part\n",
    "    \n",
    "    for i, (_, imgs, gt_confs, gt_locs) in enumerate(val_generator):\n",
    "        imgs = preprocess_input(imgs)\n",
    "        val_confs, val_locs = model(imgs)\n",
    "        val_conf_loss, val_loc_loss = criterion(val_confs, val_locs, gt_confs, gt_locs)\n",
    "        v_loss(val_conf_loss+val_loc_loss)\n",
    "        v_conf_loss(val_conf_loss)\n",
    "        v_loc_loss(val_loc_loss)\n",
    "    '''\n",
    "    # Save checkpoint with a strategy.\n",
    "    if epoch%10 == 0:\n",
    "        model.save(\"{}.h5\".format(epoch))\n",
    "\n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.scalar('train_loss', t_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('train_conf_loss', t_conf_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('train_loc_loss', t_loc_loss.result(), step=epoch)\n",
    "        #tf.summary.scalar('val_loss', v_loss.result(), step=epoch)\n",
    "        #tf.summary.scalar('val_conf_loss', v_conf_loss.result(), step=epoch)\n",
    "        #tf.summary.scalar('val_loc_loss', v_loc_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('learning_rate', learning_rate, step=epoch)\n",
    "        \n",
    "    t_loss.reset_states()\n",
    "    t_conf_loss.reset_states()\n",
    "    t_loc_loss.reset_states()\n",
    "    #v_loss.reset_states()\n",
    "    #v_conf_loss.reset_states()\n",
    "    #v_loc_loss.reset_states()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iCvZLovSPTK1"
   },
   "outputs": [],
   "source": [
    "from post_process import predict\n",
    "\n",
    "default_boxes = anchor.generate_default_boxes(config[\"anchor_param\"])\n",
    "confs, locs = model(imgs, training=False) #imgs.shape == (N,160,160,3)\n",
    "batch_boxes, batch_classes, batch_scores = predict(confs, locs, default_boxes, num_classes, conf_thresh=0.5)\n",
    "# 각 영상별 예측 bounding box(바운딩 박스)와 class(카테고리 번호), 그리고 scores(신뢰도)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "ssd-training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
