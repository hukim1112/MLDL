{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "semantic segmentation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hukim1112/MLDL/blob/master/segmentation/semantic_segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCSP-dbMw88x"
      },
      "source": [
        "# Image segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMP7mglMuGT2"
      },
      "source": [
        "이번 튜토리얼은 영상 분할(image segmentation)을 다루기 위해, <a href=\"https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/\" class=\"external\">U-Net</a>의 수정된 모델을 다뤄보겠습니다.\n",
        "\n",
        "## 영상 분할이란?\n",
        "\n",
        "영상 분류는 각 입력 영상에 레이블(또는 클래스)를 부여합니다. 하지만 만약 영상의 물체의 형상을 알기 위해, 어떤 픽셀들이 물체에 속하는 지 알기 원할 수 있습니다. 이런 경우 영상의 각 픽셀에 레이블(또는 클래스)를 부여합니다. 이런 테스크가 영상 분할이라 알려져있습니다. 분할 모델은 분류 모델보다 영상에 관한 더 상세한 정보를 반환합니다. 영상 분할은 의료용사진분석, 자율주행, 위성사진분석 등의 많은 응용에서 사용됩니다.\n",
        "\n",
        "이 튜토리얼은 [Oxford-IIIT Pet Dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/)을 사용합니다. 이 데이터는 37 종의 반려동물에 대한 사진이 종별 200장으로 구성됩니다. 그리고 학습과 테스트로 100/100장으로 나눴습니다. 각각의 이미지는 픽셀 수준 마스크(pixel-wise mask)를 상응하는 레이블로 가지고 있습니다. 마스크들은 픽셀 별로 클래스-레이블을 표현합니다. 마스크의 픽셀은 다음 카테고리 중 하나를 표현합니다.\n",
        "\n",
        "*   Class 1 : Pixel belonging to the pet.\n",
        "*   Class 2 : Pixel bordering the pet.\n",
        "*   Class 3 : None of the above/ Surrounding pixel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQX7R4bhZy5h"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental import preprocessing \n",
        "\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g87--n2AtyO_"
      },
      "source": [
        "#from tensorflow_examples.models.pix2pix import pix2pix\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWe0_rQM4JbC"
      },
      "source": [
        "## Download the Oxford-IIIT Pets dataset\n",
        "\n",
        "The dataset is [available from TensorFlow Datasets](https://www.tensorflow.org/datasets/catalog/oxford_iiit_pet). The segmentation masks are included in version 3+."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40ITeStwDwZb"
      },
      "source": [
        "dataset, info = tfds.load('oxford_iiit_pet:3.*.*', with_info=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJcVdj_U4vzf"
      },
      "source": [
        "추가적으로, 이미지의 픽셀들은 `[0,1]` 범위로 정규화합니다. 마지막으로 마스크의 픽셀값에 각각 {1, 2, 3} 중의 하나의 클래스를 부여한다. 편의를 위해 1을 차감해서 이번 학습에서는 정답 마스크로 {0, 1, 2} 중 하나의 레이블을 부여합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FD60EbcAQqov"
      },
      "source": [
        "def normalize(input_image, input_mask):\n",
        "  input_image = tf.cast(input_image, tf.float32) / 255.0\n",
        "  input_mask -= 1\n",
        "  return input_image, input_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zf0S67hJRp3D"
      },
      "source": [
        "def load_image(datapoint):\n",
        "  input_image = tf.image.resize(datapoint['image'], (128, 128))\n",
        "  input_mask = tf.image.resize(datapoint['segmentation_mask'], (128, 128))\n",
        "\n",
        "  input_image, input_mask = normalize(input_image, input_mask)\n",
        "\n",
        "  return input_image, input_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65-qHTjX5VZh"
      },
      "source": [
        "데이터셋은 이미 학습, 테스트가 나뉘어져 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHwj2-8SaQli"
      },
      "source": [
        "TRAIN_LENGTH = info.splits['train'].num_examples\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 1000\n",
        "STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39fYScNz9lmo"
      },
      "source": [
        "train_images = dataset['train'].map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test_images = dataset['test'].map(load_image, num_parallel_calls=tf.data.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9hGHyg8L3Y1"
      },
      "source": [
        "데이터셋에는 랜덤 플립을 사용하는 간단한 증강을 적용했습니다. Augment 클래스는 입력 영상과 정답 마스크를 함께 뒤집거나 뒤집지 않습니다.\n",
        "영상 증강은 다음 튜토리얼을 참고하세요. [image augmentation tutorial](https://www.tensorflow.org/tutorials/images/data_augmentation)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUWdDJRTL0PP"
      },
      "source": [
        "class Augment(tf.keras.layers.Layer):\n",
        "  def __init__(self, seed=42):\n",
        "    super().__init__()\n",
        "    # both use the same seed, so they'll make the same randomn changes.\n",
        "    self.augment_inputs = preprocessing.RandomFlip(mode=\"horizontal\", seed=seed)\n",
        "    self.augment_labels = preprocessing.RandomFlip(mode=\"horizontal\", seed=seed)\n",
        "  \n",
        "  def call(self, inputs, labels):\n",
        "    inputs = self.augment_inputs(inputs)\n",
        "    labels = self.augment_labels(labels)\n",
        "    return inputs, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTIbNIBdcgL3"
      },
      "source": [
        "입력 파이프라인을 구축하고, 배치 다음으로 증강을 적용합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPscskQcNCx4"
      },
      "source": [
        "train_batches = (\n",
        "    train_images\n",
        "    .cache()\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .repeat()\n",
        "    .map(Augment())\n",
        "    .prefetch(buffer_size=tf.data.AUTOTUNE))\n",
        "\n",
        "test_batches = test_images.batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xa3gMAE_9qNa"
      },
      "source": [
        "몇 개의 샘플을 확인해봅니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3N2RPAAW9q4W"
      },
      "source": [
        "def display(display_list):\n",
        "  plt.figure(figsize=(15, 15))\n",
        "\n",
        "  title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
        "\n",
        "  for i in range(len(display_list)):\n",
        "    plt.subplot(1, len(display_list), i+1)\n",
        "    plt.title(title[i])\n",
        "    plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n",
        "    plt.axis('off')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6u_Rblkteqb"
      },
      "source": [
        "for images, masks in train_batches.take(2):\n",
        "  sample_image, sample_mask = images[0], masks[0]\n",
        "  display([sample_image, sample_mask])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAOe93FRMk3w"
      },
      "source": [
        "## Define the model\n",
        "여기서는 수정된 [U-Net](https://arxiv.org/abs/1505.04597)을 사용합니다. U-Net은 인코더(downsampler)와 디코더(upsampler)로 구성됩니다. 더 강건한 특징들을 학습하고 학습 파라미터의 수를 줄이기 위해, 사전학습된 MobileNet-V2를 인코더로 사용합니다. 디코더는 upsample 블록을 사용할 것입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4mQle3lthit"
      },
      "source": [
        "앞서 언급한대로, 인코더로 tf.keras.applications의 MobileNetV2를 사용할 것입니다. 이때 인코더는 학습 과정에서 학습하지 않을 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzIf1SLyKXoC"
      },
      "source": [
        "def upsample(filters, size):\n",
        "  \"\"\"Upsamples an input.\n",
        "  Conv2DTranspose => Batchnorm => Dropout => Relu\n",
        "  \"\"\"\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "  result = tf.keras.Sequential()\n",
        "  result.add(\n",
        "      tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
        "                                      padding='same',\n",
        "                                      kernel_initializer=initializer,\n",
        "                                      use_bias=False))\n",
        "\n",
        "  result.add(tf.keras.layers.BatchNormalization())\n",
        "  result.add(tf.keras.layers.ReLU())\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liCeLH0ctjq7"
      },
      "source": [
        "base_model = tf.keras.applications.MobileNetV2(input_shape=[128, 128, 3], include_top=False)\n",
        "\n",
        "# Use the activations of these layers\n",
        "layer_names = [\n",
        "    'block_1_expand_relu',   # 64x64\n",
        "    'block_3_expand_relu',   # 32x32\n",
        "    'block_6_expand_relu',   # 16x16\n",
        "    'block_13_expand_relu',  # 8x8\n",
        "    'block_16_project',      # 4x4\n",
        "]\n",
        "base_model_outputs = [base_model.get_layer(name).output for name in layer_names]\n",
        "\n",
        "# Create the feature extraction model\n",
        "down_stack = tf.keras.Model(inputs=base_model.input, outputs=base_model_outputs)\n",
        "\n",
        "down_stack.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPw8Lzra5_T9"
      },
      "source": [
        "The decoder/upsampler is simply a series of upsample blocks implemented in TensorFlow examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0ZbfywEbZpJ"
      },
      "source": [
        "up_stack = [\n",
        "    upsample(512, 3),  # 4x4 -> 8x8\n",
        "    upsample(256, 3),  # 8x8 -> 16x16\n",
        "    upsample(128, 3),  # 16x16 -> 32x32\n",
        "    upsample(64, 3),   # 32x32 -> 64x64\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45HByxpVtrPF"
      },
      "source": [
        "def unet_model(output_channels:int):\n",
        "  inputs = tf.keras.layers.Input(shape=[128, 128, 3])\n",
        "\n",
        "  # Downsampling through the model\n",
        "  skips = down_stack(inputs)\n",
        "  x = skips[-1]\n",
        "  skips = reversed(skips[:-1])\n",
        "\n",
        "  # Upsampling and establishing the skip connections\n",
        "  for up, skip in zip(up_stack, skips):\n",
        "    x = up(x)\n",
        "    concat = tf.keras.layers.Concatenate()\n",
        "    x = concat([x, skip])\n",
        "\n",
        "  # This is the last layer of the model\n",
        "  last = tf.keras.layers.Conv2DTranspose(\n",
        "      filters=output_channels, kernel_size=3, strides=2,\n",
        "      padding='same')  #64x64 -> 128x128\n",
        "\n",
        "  x = last(x)\n",
        "\n",
        "  return tf.keras.Model(inputs=inputs, outputs=x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNWvC7WCLBz0"
      },
      "source": [
        "모델 구조를 빠르게 살펴보도록 하겠습니다.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkpyKo1kKw7Z"
      },
      "source": [
        "OUTPUT_CLASSES = 3\n",
        "\n",
        "model = unet_model(output_channels=OUTPUT_CLASSES)\n",
        "\n",
        "tf.keras.utils.plot_model(model, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRsjdZuEnZfA"
      },
      "source": [
        "마지막 레이어에서 몇 개의 필터를 사용하 지가 채널 수를 결정합니다. 채널은 클래스 수와 동일하도록 설계합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0DGH_4T0VYn"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "이제, 남은 것은 모델을 compile, train(fit) 하는 것입니다.\n",
        "\n",
        "픽셀을 multiclass classification을 해야하기 때문에 `CategoricalCrossentropy` with `from_logits=True`가 표준적인 손실함수입니다.\n",
        "\n",
        "`losses.SparseCategoricalCrossentropy(from_logits=True)`을 사용하면 레이블로 원핫벡터의 형태대신 스칼라로 사용할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6he36HK5uKAc"
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc3MiEO2twLS"
      },
      "source": [
        "학습전에 무엇을 예측하는 지 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwvIKLZPtxV_"
      },
      "source": [
        "def create_mask(pred_mask):\n",
        "  pred_mask = tf.argmax(pred_mask, axis=-1)\n",
        "  pred_mask = pred_mask[..., tf.newaxis]\n",
        "  return pred_mask[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLNsrynNtx4d"
      },
      "source": [
        "def show_predictions(dataset=None, num=1):\n",
        "  if dataset:\n",
        "    for image, mask in dataset.take(num):\n",
        "      pred_mask = model.predict(image)\n",
        "      display([image[0], mask[0], create_mask(pred_mask)])\n",
        "  else:\n",
        "    display([sample_image, sample_mask,\n",
        "             create_mask(model.predict(sample_image[tf.newaxis, ...]))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_1CC0T4dho3"
      },
      "source": [
        "show_predictions()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22AyVYWQdkgk"
      },
      "source": [
        "학습과정에서 모델이 향상되는 것을 확인하기 위해 callback 함수를 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHrHsqijdmL6"
      },
      "source": [
        "class DisplayCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    clear_output(wait=True)\n",
        "    show_predictions()\n",
        "    print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StKDH_B9t4SD"
      },
      "source": [
        "EPOCHS = 20\n",
        "VAL_SUBSPLITS = 5\n",
        "VALIDATION_STEPS = info.splits['test'].num_examples//BATCH_SIZE//VAL_SUBSPLITS\n",
        "\n",
        "model_history = model.fit(train_batches, epochs=EPOCHS,\n",
        "                          steps_per_epoch=STEPS_PER_EPOCH,\n",
        "                          validation_steps=VALIDATION_STEPS,\n",
        "                          validation_data=test_batches,\n",
        "                          callbacks=[DisplayCallback()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_mu0SAbt40Q"
      },
      "source": [
        "loss = model_history.history['loss']\n",
        "val_loss = model_history.history['val_loss']\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(model_history.epoch, loss, 'r', label='Training loss')\n",
        "plt.plot(model_history.epoch, val_loss, 'bo', label='Validation loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss Value')\n",
        "plt.ylim([0, 1])\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unP3cnxo_N72"
      },
      "source": [
        "## Make predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikrzoG24qwf5"
      },
      "source": [
        "show_predictions(test_batches, 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAwvlgSNoK3o"
      },
      "source": [
        "## Optional: Imbalanced classes and class weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqtFPqqu2kxP"
      },
      "source": [
        "Semantic segmentation datasets can be highly imbalanced meaning that particular class pixels can be present more inside images than that of other classes. Since segmentation problems can be treated as per-pixel classification problems we can deal with the imbalance problem by weighing the loss function to account for this. It's a simple and elegant way to deal with this problem. See the [imbalanced classes tutorial](https://www.tensorflow.org/tutorials/structured_data/imbalanced_data).\n",
        "\n",
        "To [avoid ambiguity](https://github.com/keras-team/keras/issues/3653#issuecomment-243939748), `Model.fit` does not support the `class_weight` argument for inputs with 3+ dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHt90UEQsZDn"
      },
      "source": [
        "try:\n",
        "  model_history = model.fit(train_batches, epochs=EPOCHS,\n",
        "                            steps_per_epoch=STEPS_PER_EPOCH,\n",
        "                            class_weight = {0:2.0, 1:2.0, 2:1.0})\n",
        "  assert False\n",
        "except Exception as e:\n",
        "  print(f\"{type(e).__name__}: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brbhYODCsvbe"
      },
      "source": [
        "So in this case you need to implement the weighting yourself. You'll do this using sample weights: In addition to `(data, label)` pairs, `Model.fit` also accepts `(data, label, sample_weight)` triples.\n",
        "\n",
        "`Model.fit` propagates the `sample_weight` to the losses and metrics, which also accept a `sample_weight` argument. The sample weight is multiplied by the sample's value before the reduction step. For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmHtImJn5Kk-"
      },
      "source": [
        "label = [0,0]\n",
        "prediction = [[-3., 0], [-3, 0]] \n",
        "sample_weight = [1, 10] \n",
        "\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                               reduction=tf.losses.Reduction.NONE)\n",
        "loss(label, prediction, sample_weight).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gbwo3DZ-9TxM"
      },
      "source": [
        "So to make sample weights for this tutorial you need a function that takes a `(data, label)` pair and returns a `(data, label, sample_weight)` triple. Where the `sample_weight` is a 1-channel image containing the class weight for each pixel. \n",
        "\n",
        "The simplest possible implementation is to use the label as an index into a `class_weight` list:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlG-n2Ugo8Jc"
      },
      "source": [
        "def add_sample_weights(image, label):\n",
        "  # The weights for each class, with the constraint that:\n",
        "  #     sum(class_weights) == 1.0\n",
        "  class_weights = tf.constant([2.0, 2.0, 1.0])\n",
        "  class_weights = class_weights/tf.reduce_sum(class_weights)\n",
        "\n",
        "  # Create an image of `sample_weights` by using the label at each pixel as an \n",
        "  # index into the `class weights` .\n",
        "  sample_weights = tf.gather(class_weights, indices=tf.cast(label, tf.int32))\n",
        "\n",
        "  return image, label, sample_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLH_NvH2UrXU"
      },
      "source": [
        "The resulting dataset elements contain 3 images each:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SE_ezRSFRCnE"
      },
      "source": [
        "train_batches.map(add_sample_weights).element_spec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yc-EpIzaRbSL"
      },
      "source": [
        "Now you can train a model on this weighted dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDWipedAoOQe"
      },
      "source": [
        "weighted_model = unet_model(OUTPUT_CLASSES)\n",
        "weighted_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btEFKc1xodGR"
      },
      "source": [
        "weighted_model.fit(\n",
        "    train_batches.map(add_sample_weights),\n",
        "    epochs=1,\n",
        "    steps_per_epoch=10)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}